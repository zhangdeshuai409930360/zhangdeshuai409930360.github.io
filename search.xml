<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Apache Hudi的写时复制和读时合并</title>
    <url>/2021/05/11/35/</url>
    <content><![CDATA[<h3 id="Apache-Hudi"><a href="#Apache-Hudi" class="headerlink" title="Apache Hudi"></a>Apache Hudi</h3><p>Hudi将流处理带到大数据，提供新数据，同时比传统批处理效率高一个数量级。</p>
<p>Hudi可以帮助你构建高效的数据湖，解决一些最复杂的底层存储管理问题，同时将数据更快地交给数据分析师，工程师和科学家。</p>
<h3 id="Hudi不是什么"><a href="#Hudi不是什么" class="headerlink" title="Hudi不是什么"></a>Hudi不是什么</h3><p>Hudi不是针对任何OLTP案例而设计的，在这些情况下，通常你使用的是现有的NoSQL / RDBMS数据存储。Hudi无法替代你的内存分析数据库（至少现在还没有！）。Hudi支持在几分钟内实现近乎实时的摄取，从而权衡了延迟以进行有效的批处理。</p>
<h3 id="增量处理"><a href="#增量处理" class="headerlink" title="增量处理"></a>增量处理</h3><p>增量处理仅是指以流处理方式编写微型批处理程序。典型的批处理作业每隔几个小时就会消费所有输入并重新计算所有输出。典型的流处理作业会连续/每隔几秒钟消费一些新的输入并重新计算新的/更改以输出。尽管以批处理方式重新计算所有输出可能会更简单，但这很浪费并且耗费昂贵的资源。Hudi具有以流方式编写相同批处理管道的能力，每隔几分钟运行一次。</p>
<p>虽然可将其称为流处理，但我们更愿意称其为增量处理，以区别于使用Apache Flink，Apache Apex或Apache Kafka Streams构建的纯流处理管道。</p>
<h3 id="Hudi基于MVCC设计"><a href="#Hudi基于MVCC设计" class="headerlink" title="Hudi基于MVCC设计"></a>Hudi基于MVCC设计</h3><p><img src="/images/56.png" alt="alt"></p>
<h3 id="存储类型和视图"><a href="#存储类型和视图" class="headerlink" title="存储类型和视图"></a>存储类型和视图</h3><p>Hudi存储类型定义了如何在DFS上对数据进行索引和布局以及如何在这种组织之上实现上述原语和时间轴活动（即如何写入数据）。<br>反过来，视图定义了基础数据如何暴露给查询（即如何读取数据）。<br>存储类型    支持的视图<br>写时复制    读优化 + 增量<br>读时合并    读优化 + 增量 + 近实时</p>
<h3 id="两种存储类型"><a href="#两种存储类型" class="headerlink" title="两种存储类型"></a>两种存储类型</h3><p>写时复制（copy on write）：仅使用列式文件（parquet）存储数据。在写入/更新数据时，直接同步合并原文件，生成新版本的基文件（需要重写整个列数据文件，即使只有一个字节的新数据被提交）。此存储类型下，写入数据非常昂贵，而读取的成本没有增加，所以适合频繁读的工作负载，因为数据集的最新版本在列式文件中始终可用，以进行高效的查询。</p>
<p>读时合并（merge on read）：使用列式（parquet）与行式（avro）文件组合，进行数据存储。在更新记录时，更新到增量文件中（avro），然后进行异步（或同步）的compaction，创建列式文件（parquet）的新版本。此存储类型适合频繁写的工作负载，因为新记录是以appending 的模式写入增量文件中。但是在读取数据集时，需要将增量文件与旧文件进行合并，生成列式文件。</p>
<h3 id="存储数据的视图（查询模式）"><a href="#存储数据的视图（查询模式）" class="headerlink" title="存储数据的视图（查询模式）"></a>存储数据的视图（查询模式）</h3><p>读优化视图（Read Optimized view）：直接query 基文件（数据集的最新快照），也就是列式文件（如parquet）。相较于非Hudi列式数据集，有相同的列式查询性能</p>
<p>增量视图（Incremental View）：仅query新写入数据集的文件，也就是指定一个commit/compaction，query此之后的新数据。</p>
<p>实时视图（Real-time View）：query最新基文件与增量文件。此视图通过将最新的基文件（parquet）与增量文件（avro）进行动态合并，然后进行query。可以提供近实时的数据（会有几分钟的延迟）</p>
<h3 id="写时复制存储"><a href="#写时复制存储" class="headerlink" title="写时复制存储"></a>写时复制存储</h3><p>以下内容说明了将数据写入写时复制存储并在其上运行两个查询时，它是如何工作的：<br><img src="/images/57.png" alt="alt"></p>
<h3 id="读时合并存储"><a href="#读时合并存储" class="headerlink" title="读时合并存储"></a>读时合并存储</h3><p>以下内容说明了存储的工作方式，并显示了对近实时表和读优化表的查询：<br><img src="/images/58.png" alt="alt"></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Hudi</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive严格模式</title>
    <url>/2021/03/06/34/</url>
    <content><![CDATA[<h3 id="Hive严格模式"><a href="#Hive严格模式" class="headerlink" title="Hive严格模式"></a>Hive严格模式</h3><p>Hive提供了一个严格模式，可以防止用户执行那些可能产生意向不到的不好的效果的查询。说通俗一点就是这种模式可以阻止某些查询的执行。上次面试问的我一脸懵逼</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; <span class="built_in">set</span> hive.mapred.mode=strict;</span><br></pre></td></tr></table></figure>
<h3 id="严格模式限制条件"><a href="#严格模式限制条件" class="headerlink" title="严格模式限制条件"></a>严格模式限制条件</h3><p>1.带有分区的表的查询</p>
<p>如果在一个分区表执行hive，除非where语句中包含分区字段过滤条件来显示数据范围，否则不允许执行。换句话说就是在严格模式下不允许用户扫描所有的分区。</p>
<p>进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。如果不进行分区限制的查询会消耗巨大的资源来处理，如下不带分区的查询语句：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id=5;</span><br><span class="line">FAILED: Error <span class="keyword">in</span> semantic analysis: No Partition Predicate Found <span class="keyword">for</span> Alias <span class="string">"fracture_ins"</span> Table <span class="string">"fracture_ins</span></span><br></pre></td></tr></table></figure>
<p>解决在where后面必须带分区</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; SELECT DISTINCT(planner_id) FROM fracture_ins  WHERE planner_id=5 AND hit_date=20120101;</span><br></pre></td></tr></table></figure>

<p>2.带有orderby的查询<br>对于使用了orderby的查询，要求必须有limit语句。因为orderby为了执行排序过程会讲所有的结果分发到同一个reducer中<br>进行处理，强烈要求用户增加这个limit语句可以防止reducer额外执行很长一段时间：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; SELECT * FROM fracture_ins WHERE hit_date&gt;2012 ORDER BY planner_id;</span><br><span class="line">FAILED: Error <span class="keyword">in</span> semantic analysis: line 1:56 In strict mode,<span class="built_in">limit</span> must be specified <span class="keyword">if</span> ORDER BY is present planner_id</span><br></pre></td></tr></table></figure>

<p>解决方案就是增加一个limit关键字：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; SELECT * FROM fracture_ins WHERE hit_date&gt;2012 ORDER BY planner_id LIMIT 100000;</span><br></pre></td></tr></table></figure>

<p>3.限制笛卡尔积的查询<br>对关系型数据库非常了解的用户可能期望在执行join查询的时候不适用on语句而是使用where语句，这样关系型数据库的执行优化器就可以高效的将where语句转换成那个on语句了。不幸的是，Hive并不支持这样的优化，因为如果表非常大的话，就会出现不可控的情况，如下是不带on的语句：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; SELECT * FROM fracture_act JOIN fracture_ads WHERE fracture_act.planner_id = fracture_ads.planner_id;</span><br><span class="line">FAILED: Error <span class="keyword">in</span> semantic analysis: In strict mode, cartesian product is not allowed. If you really want to perform the operation, +<span class="built_in">set</span> hive.mapred.mode=nonstrict+</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Atlas安装数据治理</title>
    <url>/2020/09/22/33/</url>
    <content><![CDATA[<h3 id="Atlas概述"><a href="#Atlas概述" class="headerlink" title="Atlas概述"></a>Atlas概述</h3><p>Apache Atlas为组织提供开放式元数据管理和治理功能，用以构建其数据资产目录，对这些资产进行分类和管理，并为数据分析师和数据治理团队，提供围绕这些数据资产的协作功能。</p>
<h3 id="Atlas架构原理"><a href="#Atlas架构原理" class="headerlink" title="Atlas架构原理"></a>Atlas架构原理</h3><p><img src="/images/52.png" alt="alt"></p>
<h3 id="Atlas安装及使用"><a href="#Atlas安装及使用" class="headerlink" title="Atlas安装及使用"></a>Atlas安装及使用</h3><p>1）Atlas官网地址：<a href="https://atlas.apache.org/" target="_blank" rel="noopener">https://atlas.apache.org/</a></p>
<p>2）文档查看地址：<a href="https://atlas.apache.org/0.8.4/index.html" target="_blank" rel="noopener">https://atlas.apache.org/0.8.4/index.html</a></p>
<p>3）下载地址：<a href="https://www.apache.org/dyn/closer.cgi/atlas/0.8.4/apache-atlas-0.8.4-sources.tar.gz" target="_blank" rel="noopener">https://www.apache.org/dyn/closer.cgi/atlas/0.8.4/apache-atlas-0.8.4-sources.tar.gz</a></p>
<h3 id="HDP安装Solr5-2-1"><a href="#HDP安装Solr5-2-1" class="headerlink" title="HDP安装Solr5.2.1"></a>HDP安装Solr5.2.1</h3><h3 id="HDP安装Atlas0-8-2"><a href="#HDP安装Atlas0-8-2" class="headerlink" title="HDP安装Atlas0.8.2"></a>HDP安装Atlas0.8.2</h3><pre><code class="bash">[root@hadoop101 atlas]$ bin/import-hive.sh
Using Hive configuration directory [/opt/module/hive/conf]
Log file <span class="keyword">for</span> import is /opt/module/atlas/logs/import-hive.log
log4j:WARN No such property [maxFileSize] <span class="keyword">in</span> org.apache.log4j.PatternLayout.
log4j:WARN No such property [maxBackupIndex] <span class="keyword">in</span> org.apache.log4j.PatternLayout.

输入用户名：admin；输入密码：admin
Enter username <span class="keyword">for</span> atlas :- admin
Enter password <span class="keyword">for</span> atlas :-
Hive Meta Data import was successful!!!</code></pre>
<h3 id="显示所有hive-tables"><a href="#显示所有hive-tables" class="headerlink" title="显示所有hive tables"></a>显示所有hive tables</h3><p><img src="/images/53.png" alt="alt"></p>
<h3 id="选择数据库中的表可以看到之间的血缘关系图"><a href="#选择数据库中的表可以看到之间的血缘关系图" class="headerlink" title="选择数据库中的表可以看到之间的血缘关系图"></a>选择数据库中的表可以看到之间的血缘关系图</h3><p><img src="/images/54.png" alt="alt"></p>
<h3 id="字段的血缘关系"><a href="#字段的血缘关系" class="headerlink" title="字段的血缘关系"></a>字段的血缘关系</h3><p><img src="/images/55.png" alt="alt"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Apache Atlas为Hadoop集群提供了包括数据分类、集中策略引擎、数据血缘、安全和生命周期管理在内的元数据治理核心能力，其与Apache Falcon，Apache Ranger相互整合可以形成完整的数据治理解决方案。但是Atlas目前还是Apache孵化项目，尚未成熟，有待发展。</p>
<p>Atlas目前还存在以下一些需要改进之处：</p>
<p>缺乏对元数据的全局视图，对元数据的血缘追溯只能够展示具体某张表或某个SQL的生命周期(其前提是用户必须对Hadoop的元数据结构十分清楚，才能够通过Atlas的查询语句去定位自己需要了解的表)</p>
<p>0.8以前的版本，对元数据只能进行只读操作，例如只能展示Hive的表但是不能创建新表</p>
<p>与Hadoop各组件的集成尚待完善，例如Atlas对Hive的元数据变更操作的捕获只支持hive CLI，不支持beeline/JDBC</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Atlas</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL优化技巧</title>
    <url>/2020/08/27/32/</url>
    <content><![CDATA[<p>###<br>首先，对于MySQL层优化我一般遵从五个原则：</p>
<p>减少数据访问：设置合理的字段类型，启用压缩，通过索引访问等减少磁盘 IO。</p>
<p>返回更少的数据：只返回需要的字段和数据分页处理，减少磁盘 IO 及网络 IO。</p>
<p>减少交互次数：批量 DML 操作，函数存储等减少数据连接次数。</p>
<p>减少服务器 CPU 开销：尽量减少数据库排序操作以及全表查询，减少 CPU 内存占用。</p>
<p>利用更多资源：使用表分区，可以增加并行操作，更大限度利用 CPU 资源。</p>
<p>总结到 SQL 优化中，就如下三点：</p>
<p>最大化利用索引。</p>
<p>尽可能避免全表扫描。</p>
<p>减少无效数据的查询。</p>
<p>理解 SQL 优化原理 ，首先要搞清楚 SQL 执行顺序</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">DISTINCT &lt;select_list&gt;</span><br><span class="line">FROM &lt;left_table&gt;</span><br><span class="line">&lt;join_type&gt; JOIN &lt;right_table&gt;</span><br><span class="line">ON &lt;join_condition&gt;</span><br><span class="line">WHERE &lt;where_condition&gt;</span><br><span class="line">GROUP BY &lt;group_by_list&gt;</span><br><span class="line">HAVING &lt;having_condition&gt;</span><br><span class="line">ORDER BY &lt;order_by_condition&gt;</span><br><span class="line">LIMIT &lt;limit_number&gt;</span><br></pre></td></tr></table></figure>
<p>SELECT 语句，执行顺序如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM</span><br><span class="line">&lt;表名&gt; <span class="comment"># 选取表，将多个表数据通过笛卡尔积变成一个表。</span></span><br><span class="line">ON</span><br><span class="line">&lt;筛选条件&gt; <span class="comment"># 对笛卡尔积的虚表进行筛选</span></span><br><span class="line">JOIN &lt;join, left join, right join...&gt; </span><br><span class="line">&lt;join表&gt; <span class="comment"># 指定join，用于添加数据到on之后的虚表中，例如left join会将左表的剩余数据添加到虚表中</span></span><br><span class="line">WHERE</span><br><span class="line">&lt;<span class="built_in">where</span>条件&gt; <span class="comment"># 对上述虚表进行筛选</span></span><br><span class="line">GROUP BY</span><br><span class="line">&lt;分组条件&gt; <span class="comment"># 分组</span></span><br><span class="line">&lt;SUM()等聚合函数&gt; <span class="comment"># 用于having子句进行判断，在书写上这类聚合函数是写在having判断里面的</span></span><br><span class="line">HAVING</span><br><span class="line">&lt;分组筛选&gt; <span class="comment"># 对分组后的结果进行聚合筛选</span></span><br><span class="line">SELECT</span><br><span class="line">&lt;返回数据列表&gt; <span class="comment"># 返回的单列必须在group by子句中，聚合函数除外</span></span><br><span class="line">DISTINCT</span><br><span class="line"><span class="comment"># 数据除重</span></span><br><span class="line">ORDER BY</span><br><span class="line">&lt;排序条件&gt; <span class="comment"># 排序</span></span><br><span class="line">LIMIT</span><br><span class="line">&lt;行数限制&gt;</span><br></pre></td></tr></table></figure>

<h3 id="避免不走索引的场景"><a href="#避免不走索引的场景" class="headerlink" title="避免不走索引的场景"></a>避免不走索引的场景</h3><p>①尽量避免在字段开头模糊查询，会导致数据库引擎放弃索引进行全表扫描</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE username LIKE <span class="string">'%陈%'</span></span><br></pre></td></tr></table></figure>
<p>优化方式：尽量在字段后面使用模糊查询。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE username LIKE <span class="string">'陈%'</span></span><br></pre></td></tr></table></figure>
<p>如果需求是要在前面使用模糊查询：</p>
<p>使用 MySQL 内置函数 INSTR（str，substr）来匹配，作用类似于 Java 中的 indexOf()，查询字符串出现的角标位置。</p>
<p>使用 FullText 全文索引，用 match against 检索。</p>
<p>数据量较大的情况，建议引用 ElasticSearch、Solr，亿级数据量检索速度秒级。</p>
<p>当表数据量较少（几千条儿那种），别整花里胡哨的，直接用 like ‘%xx%’。</p>
<h3 id="尽量避免使用-in-和-not-in，会导致引擎走全表扫描"><a href="#尽量避免使用-in-和-not-in，会导致引擎走全表扫描" class="headerlink" title="尽量避免使用 in 和 not in，会导致引擎走全表扫描"></a>尽量避免使用 in 和 not in，会导致引擎走全表扫描</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE id IN (2,3)</span><br></pre></td></tr></table></figure>
<p>优化方式：如果是连续数值，可以用 between 代替</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE id BETWEEN 2 AND 3</span><br></pre></td></tr></table></figure>
<p>如果是子查询，可以用 exists 代替。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- 不走索引</span><br><span class="line">select * from A <span class="built_in">where</span> A.id <span class="keyword">in</span> (select id from B);</span><br><span class="line">-- 走索引</span><br><span class="line">select * from A <span class="built_in">where</span> exists (select * from B <span class="built_in">where</span> B.id = A.id);</span><br></pre></td></tr></table></figure>

<h3 id="尽量避免使用-or，会导致数据库引擎放弃索引进行全表扫描"><a href="#尽量避免使用-or，会导致数据库引擎放弃索引进行全表扫描" class="headerlink" title="尽量避免使用 or，会导致数据库引擎放弃索引进行全表扫描"></a>尽量避免使用 or，会导致数据库引擎放弃索引进行全表扫描</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE id = 1 OR id = 3</span><br></pre></td></tr></table></figure>
<p>优化方式：可以用 union 代替 or。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE id = 1</span><br><span class="line">UNION</span><br><span class="line">SELECT * FROM t WHERE id = 3</span><br></pre></td></tr></table></figure>

<h3 id="尽量避免进行-null-值的判断，会导致数据库引擎放弃索引进行全表扫描"><a href="#尽量避免进行-null-值的判断，会导致数据库引擎放弃索引进行全表扫描" class="headerlink" title="尽量避免进行 null 值的判断，会导致数据库引擎放弃索引进行全表扫描"></a>尽量避免进行 null 值的判断，会导致数据库引擎放弃索引进行全表扫描</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE score IS NULL</span><br></pre></td></tr></table></figure>
<p>优化方式：可以给字段添加默认值 0，对 0 值进行判断。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM t WHERE score = 0</span><br></pre></td></tr></table></figure>

<h3 id="尽量避免在-where-条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描"><a href="#尽量避免在-where-条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描" class="headerlink" title="尽量避免在 where 条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描"></a>尽量避免在 where 条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描</h3><p>可以将表达式、函数操作移动到等号右侧，如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- 全表扫描</span><br><span class="line">SELECT * FROM T WHERE score/10 = 9</span><br><span class="line">-- 走索引</span><br><span class="line">SELECT * FROM T WHERE score = 10*9</span><br></pre></td></tr></table></figure>

<h3 id="当数据量大时，避免使用-where-1-1-的条件"><a href="#当数据量大时，避免使用-where-1-1-的条件" class="headerlink" title="当数据量大时，避免使用 where 1=1 的条件"></a>当数据量大时，避免使用 where 1=1 的条件</h3><p>通常为了方便拼装查询条件，我们会默认使用该条件，数据库引擎会放弃索引进行全表扫描</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- 全表扫描</span><br><span class="line">SELECT username, age, sex FROM T WHERE 1=1</span><br></pre></td></tr></table></figure>
<p>优化方式：用代码拼装 SQL 时进行判断，没 where 条件就去掉 where，有 where 条件就加 and。</p>
<h3 id="查询条件不能用-lt-gt-或者"><a href="#查询条件不能用-lt-gt-或者" class="headerlink" title="查询条件不能用 &lt;&gt; 或者 !="></a>查询条件不能用 &lt;&gt; 或者 !=</h3><p>使用索引列作为条件进行查询时，需要避免使用&lt;&gt;或者!=等判断条件。</p>
<p>如确实业务需要，使用到不等于符号，需要在重新评估索引建立，避免在此字段上建立索引，改由查询条件中其他索引字段代替。</p>
<h3 id="where-条件仅包含复合索引非前置列"><a href="#where-条件仅包含复合索引非前置列" class="headerlink" title="where 条件仅包含复合索引非前置列"></a>where 条件仅包含复合索引非前置列</h3><p>如下：复合（联合）索引包含 key_part1，key_part2，key_part3 三列，但 SQL 语句没有包含索引前置列”key_part1”，按照 MySQL 联合索引的最左匹配原则，不会走联合索引</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">select col1 from table <span class="built_in">where</span> key_part2=1 and key_part3=2</span><br></pre></td></tr></table></figure>

<h3 id="隐式类型转换造成不使用索引"><a href="#隐式类型转换造成不使用索引" class="headerlink" title="隐式类型转换造成不使用索引"></a>隐式类型转换造成不使用索引</h3><p>如下 SQL 语句由于索引对列类型为 varchar，但给定的值为数值，涉及隐式类型转换，造成不能正确走索引。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">select col1 from table <span class="built_in">where</span> col_varchar=123;</span><br></pre></td></tr></table></figure>

<h3 id="order-by-条件要与-where-中条件一致，否则-order-by-不会利用索引进行排序"><a href="#order-by-条件要与-where-中条件一致，否则-order-by-不会利用索引进行排序" class="headerlink" title="order by 条件要与 where 中条件一致，否则 order by 不会利用索引进行排序"></a>order by 条件要与 where 中条件一致，否则 order by 不会利用索引进行排序</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- 不走age索引</span><br><span class="line">SELECT * FROM t order by age;</span><br><span class="line"></span><br><span class="line">-- 走age索引</span><br><span class="line">SELECT * FROM t <span class="built_in">where</span> age &gt; 0 order by age;</span><br></pre></td></tr></table></figure>

<p>对于上面的语句，数据库的处理顺序是：</p>
<p>第一步：根据 where 条件和统计信息生成执行计划，得到数据。</p>
<p>第二步：将得到的数据排序。当执行处理数据（order by）时，数据库会先查看第一步的执行计划，看 order by 的字段是否在执行计划中利用了索引。如果是，则可以利用索引顺序而直接取得已经排好序的数据。如果不是，则重新进行排序操作。</p>
<p>第三步：返回排序后的数据。</p>
<p>当 order by 中的字段出现在 where 条件中时，才会利用索引而不再二次排序，更准确的说，order by 中的字段在执行计划中利用了索引时，不用排序操作。</p>
<p>这个结论不仅对 order by 有效，对其他需要排序的操作也有效。比如 group by 、union 、distinct 等。</p>
<h3 id="正确使用-hint-优化语句"><a href="#正确使用-hint-优化语句" class="headerlink" title="正确使用 hint 优化语句"></a>正确使用 hint 优化语句</h3><p>MySQL 中可以使用 hint 指定优化器在执行时选择或忽略特定的索引。</p>
<p>一般而言，处于版本变更带来的表结构索引变化，更建议避免使用 hint，而是通过 Analyze table 多收集统计信息。</p>
<p>但在特定场合下，指定 hint 可以排除其他索引干扰而指定更优的执行计划：</p>
<p>USE INDEX 在你查询语句中表名的后面，添加 USE INDEX 来提供希望 MySQL 去参考的索引列表，就可以让 MySQL 不再考虑其他可用的索引。</p>
<p>例子: SELECT col1 FROM table USE INDEX (mod_time, name)…</p>
<p>IGNORE INDEX 如果只是单纯的想让 MySQL 忽略一个或者多个索引，可以使用 IGNORE INDEX 作为 Hint。</p>
<p>例子: SELECT col1 FROM table IGNORE INDEX (priority) …</p>
<p>FORCE INDEX 为强制 MySQL 使用一个特定的索引，可在查询中使用FORCE INDEX 作为 Hint。</p>
<p>例子: SELECT col1 FROM table FORCE INDEX (mod_time) …</p>
<p>在查询的时候，数据库系统会自动分析查询语句，并选择一个最合适的索引。但是很多时候，数据库系统的查询优化器并不一定总是能使用最优索引。</p>
<p>如果我们知道如何选择索引，可以使用 FORCE INDEX 强制查询使用指定的索引。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM students FORCE INDEX (idx_class_id) WHERE class_id = 1 ORDER BY id DESC;</span><br></pre></td></tr></table></figure>
<h3 id="SELECT-语句其他优化"><a href="#SELECT-语句其他优化" class="headerlink" title="SELECT 语句其他优化"></a>SELECT 语句其他优化</h3><p>①避免出现 select *</p>
<p>首先，select * 操作在任何类型数据库中都不是一个好的 SQL 编写习惯。</p>
<p>使用 select * 取出全部列，会让优化器无法完成索引覆盖扫描这类优化，会影响优化器对执行计划的选择，也会增加网络带宽消耗，更会带来额外的 I/O，内存和 CPU 消耗。</p>
<p>建议提出业务实际需要的列数，将指定列名以取代 select *。具体详情见《为什么大家都说SELECT * 效率低》</p>
<p>②避免出现不确定结果的函数</p>
<p>特定针对主从复制这类业务场景。由于原理上从库复制的是主库执行的语句，使用如 now()、rand()、sysdate()、current_user() 等不确定结果的函数很容易导致主库与从库相应的数据不一致。</p>
<p>另外不确定值的函数，产生的 SQL 语句无法利用 query cache。</p>
<p>③多表关联查询时，小表在前，大表在后</p>
<p>在 MySQL 中，执行 from 后的表关联查询是从左往右执行的（Oracle 相反），第一张表会涉及到全表扫描。</p>
<p>所以将小表放在前面，先扫小表，扫描快效率较高，在扫描后面的大表，或许只扫描大表的前 100 行就符合返回条件并 return 了。</p>
<p>例如：表 1 有 50 条数据，表 2 有 30 亿条数据；如果全表扫描表 2，你品，那就先去吃个饭再说吧是吧。</p>
<p>④使用表的别名</p>
<p>当在 SQL 语句中连接多个表时，请使用表的别名并把别名前缀于每个列名上。这样就可以减少解析的时间并减少哪些友列名歧义引起的语法错误。</p>
<p>⑤用 where 字句替换 HAVING 字句</p>
<p>避免使用 HAVING 字句，因为 HAVING 只会在检索出所有记录之后才对结果集进行过滤，而 where 则是在聚合前刷选记录，如果能通过 where 字句限制记录的数目，那就能减少这方面的开销。</p>
<p>HAVING 中的条件一般用于聚合函数的过滤，除此之外，应该将条件写在 where 字句中。</p>
<p>where 和 having 的区别：where 后面不能使用组函数。</p>
<p>⑥调整 Where 字句中的连接顺序</p>
<p>MySQL 采用从左往右，自上而下的顺序解析 where 子句。根据这个原理，应将过滤数据多的条件往前放，最快速度缩小结果集。</p>
<h3 id="增删改-DML-语句优化"><a href="#增删改-DML-语句优化" class="headerlink" title="增删改 DML 语句优化"></a>增删改 DML 语句优化</h3><p>①大批量插入数据<br>如果同时执行大量的插入，建议使用多个值的 INSERT 语句（方法二）。这比使用分开 INSERT 语句快（方法一），一般情况下批量插入效率有几倍的差别。<br>方法一：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">insert into T values(1,2); </span><br><span class="line"></span><br><span class="line">insert into T values(1,3); </span><br><span class="line"></span><br><span class="line">insert into T values(1,4);</span><br></pre></td></tr></table></figure>
<p>方法二：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Insert into T values(1,2),(1,3),(1,4);</span><br></pre></td></tr></table></figure>

<p>选择后一种方法的原因有三：<br>减少 SQL 语句解析的操作，MySQL 没有类似 Oracle 的 share pool，采用方法二，只需要解析一次就能进行数据的插入操作。</p>
<p>在特定场景可以减少对 DB 连接次数。</p>
<p>SQL 语句较短，可以减少网络传输的 IO。</p>
<p>②适当使用 commit<br>适当使用 commit 可以释放事务占用的资源而减少消耗，commit 后能释放的资源如下：</p>
<p>事务占用的 undo 数据块。</p>
<p>事务在 redo log 中记录的数据块。</p>
<p>释放事务施加的，减少锁争用影响性能。特别是在需要使用 delete 删除大量数据的时候，必须分解删除量并定期 commit。</p>
<p>③避免重复查询更新的数据<br>针对业务中经常出现的更新行同时又希望获得改行信息的需求，MySQL 并不支持 PostgreSQL 那样的 UPDATE RETURNING 语法，在 MySQL 中可以通过变量实现。</p>
<p>例如，更新一行记录的时间戳，同时希望查询当前记录中存放的时间戳是什么？<br>简单方法实现：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Update t1 <span class="built_in">set</span> time=now() <span class="built_in">where</span> col1=1; </span><br><span class="line"></span><br><span class="line">Select time from t1 <span class="built_in">where</span> id =1;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Update t1 <span class="built_in">set</span> time=now () <span class="built_in">where</span> col1=1 and @now: = now (); </span><br><span class="line"></span><br><span class="line">Select @now;</span><br></pre></td></tr></table></figure>
<p>前后二者都需要两次网络来回，但使用变量避免了再次访问数据表，特别是当 t1 表数据量较大时，后者比前者快很多。</p>
<p>④查询优先还是更新（insert、update、delete）优先<br>MySQL 还允许改变语句调度的优先级，它可以使来自多个客户端的查询更好地协作，这样单个客户端就不会由于锁定而等待很长时间。改变优先级还可以确保特定类型的查询被处理得更快</p>
<p>我们首先应该确定应用的类型，判断应用是以查询为主还是以更新为主的，是确保查询效率还是确保更新的效率，决定是查询优先还是更新优先</p>
<p>下面我们提到的改变调度策略的方法主要是针对只存在表锁的存储引擎，比如  MyISAM 、MEMROY、MERGE，对于 Innodb 存储引擎，语句的执行是由获得行锁的顺序决定的。</p>
<p>MySQL 的默认的调度策略可用总结如下：<br>写入操作优先于读取操作。</p>
<p>对某张数据表的写入操作某一时刻只能发生一次，写入请求按照它们到达的次序来处理。</p>
<p>对某张数据表的多个读取操作可以同时地进行。</p>
<p>MySQL 提供了几个语句调节符，允许你修改它的调度策略:<br>LOW_PRIORITY 关键字应用于 DELETE、INSERT、LOAD DATA、REPLACE 和 UPDATE。</p>
<p>HIGH_PRIORITY 关键字应用于 SELECT 和 INSERT 语句。</p>
<p>DELAYED 关键字应用于 INSERT 和 REPLACE 语句</p>
<p>如果写入操作是一个 LOW_PRIORITY（低优先级）请求，那么系统就不会认为它的优先级高于读取操作。</p>
<p>在这种情况下，如果写入者在等待的时候，第二个读取者到达了，那么就允许第二个读取者插到写入者之前。</p>
<p>只有在没有其它的读取者的时候，才允许写入者开始操作。这种调度修改可能存在 LOW_PRIORITY 写入操作永远被阻塞的情况。</p>
<p>SELECT 查询的 HIGH_PRIORITY（高优先级）关键字也类似。它允许 SELECT 插入正在等待的写入操作之前，即使在正常情况下写入操作的优先级更高。</p>
<p>另外一种影响是，高优先级的 SELECT 在正常的 SELECT 语句之前执行，因为这些语句会被写入操作阻塞。</p>
<p>如果希望所有支持 LOW_PRIORITY 选项的语句都默认地按照低优先级来处理，那么请使用–low-priority-updates 选项来启动服务器。</p>
<p>通过使用 INSERTHIGH_PRIORITY 来把 INSERT 语句提高到正常的写入优先级，可以消除该选项对单个 INSERT 语句的影响。</p>
<h3 id="查询条件优化"><a href="#查询条件优化" class="headerlink" title="查询条件优化"></a>查询条件优化</h3><p>①对于复杂的查询，可以使用中间临时表暂存数据</p>
<p>②优化 group by 语句</p>
<p>默认情况下，MySQL 会对 GROUP BY 分组的所有值进行排序，如 “GROUP BY col1，col2，….;” 查询的方法如同在查询中指定 “ORDER BY col1，col2，…;” 。</p>
<p>如果显式包括一个包含相同的列的 ORDER BY 子句，MySQL 可以毫不减速地对它进行优化，尽管仍然进行排序。</p>
<p>因此，如果查询包括 GROUP BY 但你并不想对分组的值进行排序，你可以指定 ORDER BY NULL 禁止排序。</p>
<p>例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT col1, col2, COUNT(*) FROM table GROUP BY col1, col2 ORDER BY NULL ;</span><br></pre></td></tr></table></figure>

<p>③优化 join 语句<br>MySQL 中可以通过子查询来使用 SELECT 语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。</p>
<p>使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的 SQL 操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询可以被更有效率的连接（JOIN）..替代。</p>
<p>例子：假设要将所有没有订单记录的用户取出来，可以用下面这个查询完成：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT col1 FROM customerinfo WHERE CustomerID NOT <span class="keyword">in</span> (SELECT CustomerID FROM salesinfo )</span><br></pre></td></tr></table></figure>

<p>如果使用连接（JOIN）..来完成这个查询工作，速度将会有所提升。</p>
<p>尤其是当 salesinfo 表中对 CustomerID 建有索引的话，性能将会更好，查询如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT col1 FROM customerinfo </span><br><span class="line">   LEFT JOIN salesinfoON customerinfo.CustomerID=salesinfo.CustomerID </span><br><span class="line">      WHERE salesinfo.CustomerID IS NULL</span><br></pre></td></tr></table></figure>

<p>连接（JOIN）..之所以更有效率一些，是因为 MySQL 不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作。</p>
<p>④优化 union 查询</p>
<p>MySQL 通过创建并填充临时表的方式来执行 union 查询。除非确实要消除重复的行，否则建议使用 union all。</p>
<p>原因在于如果没有 all 这个关键词，MySQL 会给临时表加上 distinct 选项，这会导致对整个临时表的数据做唯一性校验，这样做的消耗相当高。</p>
<p>高效：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 </span><br><span class="line"></span><br><span class="line">UNION ALL </span><br><span class="line"></span><br><span class="line">SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= <span class="string">'TEST'</span>;</span><br></pre></td></tr></table></figure>
<p>低效：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 </span><br><span class="line"></span><br><span class="line">UNION </span><br><span class="line"></span><br><span class="line">SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= <span class="string">'TEST'</span>;</span><br></pre></td></tr></table></figure>

<p>⑤拆分复杂 SQL 为多个小 SQL，避免大事务<br>如下：<br>简单的 SQL 容易使用到 MySQL 的 QUERY CACHE。</p>
<p>减少锁表时间特别是使用 MyISAM 存储引擎的表。</p>
<p>可以使用多核 CPU。</p>
<p>⑥使用 truncate 代替 delete<br>当删除全表中记录时，使用 delete 语句的操作会被记录到 undo 块中，删除记录也记录 binlog。</p>
<p>当确认需要删除全表时，会产生很大量的 binlog 并占用大量的 undo 数据块，此时既没有很好的效率也占用了大量的资源.</p>
<p>使用 truncate 替代，不会记录可恢复的信息，数据不能被恢复。也因此使用 truncate 操作有其极少的资源占用与极快的时间。另外，使用 truncate 可以回收表的水位，使自增字段值归零。</p>
<p>⑦使用合理的分页方式以提高分页效率</p>
<p>使用合理的分页方式以提高分页效率 针对展现等分页需求，合适的分页方式能够提高分页的效率。<br>案例 1：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">select * from t <span class="built_in">where</span> thread_id = 10000 and deleted = 0 </span><br><span class="line">   order by gmt_create asc <span class="built_in">limit</span> 0, 15;</span><br></pre></td></tr></table></figure>

<p>上述例子通过一次性根据过滤条件取出所有字段进行排序返回。数据访问开销=索引 IO+索引全部记录结果对应的表数据 IO。</p>
<p>因此，该种写法越翻到后面执行效率越差，时间越长，尤其表数据量很大的时候。</p>
<p>适用场景：当中间结果集很小（10000 行以下）或者查询条件复杂（指涉及多个不同查询字段或者多表连接）时适用。</p>
<p>案例 2：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">select t.* from (select id from t <span class="built_in">where</span> thread_id = 10000 and deleted = 0</span><br><span class="line">   order by gmt_create asc <span class="built_in">limit</span> 0, 15) a, t </span><br><span class="line">      <span class="built_in">where</span> a.id = t.id;</span><br></pre></td></tr></table></figure>

<p>上述例子必须满足 t 表主键是 id 列，且有覆盖索引 secondary key：（thread_id, deleted, gmt_create）。</p>
<p>通过先根据过滤条件利用覆盖索引取出主键 id 进行排序，再进行 join 操作取出其他字段。</p>
<p>数据访问开销=索引 IO+索引分页后结果（例子中是 15 行）对应的表数据 IO。因此，该写法每次翻页消耗的资源和时间都基本相同，就像翻第一页一样。</p>
<p>适用场景：当查询和排序字段（即 where 子句和 order by 子句涉及的字段）有对应覆盖索引时，且中间结果集很大的情况时适用。</p>
<h3 id="建表优化"><a href="#建表优化" class="headerlink" title="建表优化"></a>建表优化</h3><p>①在表中建立索引，优先考虑 where、order by 使用到的字段。</p>
<p>②尽量使用数字型字段（如性别，男：1 女：2），若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。</p>
<p>这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。</p>
<p>③查询数据量大的表 会造成查询缓慢。主要的原因是扫描行数过多。这个时候可以通过程序，分段分页进行查询，循环遍历，将结果合并处理进行展示。</p>
<p>要查询 100000 到 100050 的数据，如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM (SELECT ROW_NUMBER() OVER(ORDER BY ID ASC) AS rowid,* </span><br><span class="line">   FROM infoTab)t WHERE t.rowid &gt; 100000 AND t.rowid &lt;= 100050</span><br></pre></td></tr></table></figure>
<p>④用 varchar/nvarchar 代替 char/nchar。</p>
<p>尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。</p>
<p>不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL 也包含在内），都是占用 100 个字符的空间的，如果是 varchar 这样的变长字段， null 不占用空间。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/08/25/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Datax3.0简介</title>
    <url>/2020/08/04/31/</url>
    <content><![CDATA[<h3 id="Datex3-0概览"><a href="#Datex3-0概览" class="headerlink" title="Datex3.0概览"></a>Datex3.0概览</h3><p>DataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。<br>（这是一个单机多任务的ETL工具）</p>
<p><img src="/images/48.png" alt="alt"></p>
<h3 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a>设计理念</h3><p>为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。</p>
<h3 id="当前使用现状"><a href="#当前使用现状" class="headerlink" title="当前使用现状"></a>当前使用现状</h3><p>此前已经开源DataX1.0版本，此次介绍为阿里云开源全新版本DataX3.0，有了更多更强大的功能和更好的使用体验。Github主页地址：<a href="https://github.com/alibaba/DataX">https://github.com/alibaba/DataX</a></p>
<h3 id="DataX3-0框架设计"><a href="#DataX3-0框架设计" class="headerlink" title="DataX3.0框架设计"></a>DataX3.0框架设计</h3><p>DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。</p>
<p><img src="/images/49.png" alt="alt"><br>Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。<br>Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。<br>Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。</p>
<h3 id="DataX3-0插件体系"><a href="#DataX3-0插件体系" class="headerlink" title="DataX3.0插件体系"></a>DataX3.0插件体系</h3><p>DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图</p>
<p><img src="/images/50.png" alt="alt"></p>
<h3 id="DataX3-0核心架构"><a href="#DataX3-0核心架构" class="headerlink" title="DataX3.0核心架构"></a>DataX3.0核心架构</h3><p>DataX 3.0 开源版本支持单机多线程模式完成同步作业运行，本小节按一个DataX作业生命周期的时序图，从整体架构设计非常简要说明DataX各个模块相互关系。</p>
<p><img src="/images/51.png" alt="alt"></p>
<p>核心模块介绍：<br>DataX完成单个数据同步的作业，我们称之为Job，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。</p>
<p>DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。Task便是DataX作业的最小单元，每一个Task都会负责一部分数据的同步工作。</p>
<p>切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。</p>
<p>每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader—&gt;Channel—&gt;Writer的线程来完成任务同步工作。</p>
<p>DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0</p>
<h3 id="DataX调度流程："><a href="#DataX调度流程：" class="headerlink" title="DataX调度流程："></a>DataX调度流程：</h3><p>举例来说，用户提交了一个DataX作业，并且配置了20个并发，目的是将一个100张分表的mysql数据同步到odps里面。 DataX的调度决策思路是：</p>
<p>DataXJob根据分库分表切分成了100个Task。</p>
<p>根据20个并发，DataX计算共需要分配4个TaskGroup。</p>
<p>4个TaskGroup平分切分好的100个Task，每一个TaskGroup负责以5个并发共计运行25个Task。</p>
<h3 id="五、DataX-3-0六大核心优势"><a href="#五、DataX-3-0六大核心优势" class="headerlink" title="五、DataX 3.0六大核心优势"></a>五、DataX 3.0六大核心优势</h3><h3 id="可靠的数据质量监控"><a href="#可靠的数据质量监控" class="headerlink" title="可靠的数据质量监控"></a>可靠的数据质量监控</h3><p>完美解决数据传输个别类型失真问题<br>DataX旧版对于部分数据类型(比如时间戳)传输一直存在毫秒阶段等数据失真情况，新版本DataX3.0已经做到支持所有的强数据类型，每一种插件都有自己的数据类型转换策略，让数据可以完整无损的传输到目的端。</p>
<p>提供作业全链路的流量、数据量运行时监控<br>DataX3.0运行过程中可以将作业本身状态、数据流量、数据速度、执行进度等信息进行全面的展示，让用户可以实时了解作业状态。并可在作业执行过程中智能判断源端和目的端的速度对比情况，给予用户更多性能排查信息。</p>
<p>提供脏数据探测<br>在大量数据的传输过程中，必定会由于各种原因导致很多数据传输报错(比如类型转换错误)，这种数据DataX认为就是脏数据。DataX目前可以实现脏数据精确过滤、识别、采集、展示，为用户提供多种的脏数据处理模式，让用户准确把控数据质量大关！</p>
<h3 id="丰富的数据转换功能"><a href="#丰富的数据转换功能" class="headerlink" title="丰富的数据转换功能"></a>丰富的数据转换功能</h3><p>DataX作为一个服务于大数据的ETL工具，除了提供数据快照搬迁功能之外，还提供了丰富数据转换的功能，让数据在传输过程中可以轻松完成数据脱敏，补全，过滤等数据转换功能，另外还提供了自动groovy函数，让用户自定义转换函数。详情请看DataX3的transformer详细介绍。</p>
<h3 id="精准的速度控制"><a href="#精准的速度控制" class="headerlink" title="精准的速度控制"></a>精准的速度控制</h3><p>还在为同步过程对在线存储压力影响而担心吗？新版本DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在库可以承受的范围内达到最佳的同步速度。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;speed&quot;</span>: &#123;</span><br><span class="line">   <span class="string">&quot;channel&quot;</span>: 8,    ----并发数限速（根据自己CPU合理控制并发数）</span><br><span class="line">   <span class="string">&quot;byte&quot;</span>: 524288,  ----字节流限速（根据自己的磁盘和网络合理控制字节数）</span><br><span class="line">   <span class="string">&quot;record&quot;</span>: 10000  ----记录流限速（根据数据合理空行数）</span><br></pre></td></tr></table></figure>

<h3 id="强劲的同步性能"><a href="#强劲的同步性能" class="headerlink" title="强劲的同步性能"></a>强劲的同步性能</h3><p>DataX3.0每一种读插件都有一种或多种切分策略，都能将作业合理切分成多个Task并行执行，单机多线程执行模型可以让DataX速度随并发成线性增长。在源端和目的端性能都足够的情况下，单个作业一定可以打满网卡。另外，DataX团队对所有的已经接入的插件都做了极致的性能优化，并且做了完整的性能测试</p>
<h3 id="健壮的容错机制"><a href="#健壮的容错机制" class="headerlink" title="健壮的容错机制"></a>健壮的容错机制</h3><p>DataX作业是极易受外部因素的干扰，网络闪断、数据源不稳定等因素很容易让同步到一半的作业报错停止。因此稳定性是DataX的基本要求，在DataX 3.0的设计中，重点完善了框架和插件的稳定性。目前DataX3.0可以做到线程级别、进程级别(暂时未开放)、作业级别多层次局部/全局的重试，保证用户的作业稳定运行。<br>线程内部重试</p>
<p>DataX的核心插件都经过团队的全盘review，不同的网络交互方式都有不同的重试策略。</p>
<h3 id="线程级别重试"><a href="#线程级别重试" class="headerlink" title="线程级别重试"></a>线程级别重试</h3><p>目前DataX已经可以实现TaskFailover，针对于中间失败的Task，DataX框架可以做到整个Task级别的重新调度。</p>
<p>作者：香山上的麻雀<br>链接：<a href="https://www.jianshu.com/p/f5f0dc99d5ab">https://www.jianshu.com/p/f5f0dc99d5ab</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Datax</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据架构师毕生所学</title>
    <url>/2020/06/05/30/</url>
    <content><![CDATA[<p><a href="/download/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E5%B8%88%E8%AF%A5%E5%81%9A%E5%88%B0%E7%9A%84.pdf%22">点击下载</a></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive内置函数速查表</title>
    <url>/2020/06/05/29/</url>
    <content><![CDATA[<p><a href="/download/Hive%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%E9%80%9F%E6%9F%A5%E8%A1%A8.pdf%22">点击下载</a></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink与Spark多方面区别对比</title>
    <url>/2020/05/21/27/</url>
    <content><![CDATA[<p><a href="https://mp.weixin.qq.com/s?__biz=MzI0NTIxNzE1Ng==&mid=2651218385&idx=1&sn=a885d40c4418d5e26df131fdda847b74&utm_source=tuicool&utm_medium=referral" title="面试注意点">本文转载于</a><br>场景描述：Flink是标准的实时处理引擎，而且Spark的两个模块Spark Streaming和Structured Streaming都是基于微批处理的，不过现在Spark Streaming已经非常稳定基本都没有更新了，然后重点移到spark sql和structured Streaming了</p>
<p>Flink和Spark的区别在编程模型、任务调度、时间机制、Kafka 动态分区的感知、容错及处理语义、背压等几个方面存在不同。</p>
<h3 id="维表join和异步IO"><a href="#维表join和异步IO" class="headerlink" title="维表join和异步IO"></a>维表join和异步IO</h3><p>Structured Streaming不直接支持与维表的join操作，但是可以使用map、flatmap及udf等来实现该功能，所有的这些都是同步算子，不支持异步IO操作。但是Structured Streaming直接与静态数据集的join，可以也可以帮助实现维表的join功能，当然维表要不可变。</p>
<p>Flink支持与维表进行join操作，除了map，flatmap这些算子之外，flink还有异步IO算子，可以用来实现维表，提升性能。</p>
<h3 id="状态管理"><a href="#状态管理" class="headerlink" title="状态管理"></a>状态管理</h3><p>状态维护应该是流处理非常核心的概念了，比如join，分组，聚合等操作都需要维护历史状态。那么flink在这方面很好，structured Streaming也是可以，但是spark Streaming就比较弱了，只有个别状态维护算子upstatebykye等，大部分状态需要用户自己维护，虽然这个对用户来说有更大的可操作性和可以更精细控制但是带来了编程的麻烦。flink和Structured Streaming都支持自己完成了join及聚合的状态维护。</p>
<p>Structured Streaming有高级的算子，用户可以完成自定义的mapGroupsWithState和flatMapGroupsWithState，可以理解类似Spark Streaming 的upstatebykey等状态算子。</p>
<p>就拿mapGroupsWithState为例：<br>由于Flink与Structured Streaming的架构的不同，task是常驻运行的，flink不需要状态算子，只需要状态类型的数据结构。</p>
<p>首先看一下Keyed State下，我们可以用哪些原子状态：<br>ValueState：即类型为T的单值状态。这个状态与对应的key绑定，是最简单的状态了。它可以通过update方法更新状态值，通过value()方法获取状态值。</p>
<p>ListState：即key上的状态值为一个列表。可以通过add方法往列表中附加值；也可以通过get()方法返回一个Iterable来遍历状态值。</p>
<p>ReducingState：这种状态通过用户传入的reduceFunction，每次调用add方法添加值的时候，会调用</p>
<p>reduceFunction，最后合并到一个单一的状态值。</p>
<p>FoldingState：跟ReducingState有点类似，不过它的状态值类型可以与add方法中传入的元素类型不同（这种状态将会在Flink未来版本中被删除）。</p>
<p>MapState：即状态值为一个map。用户通过put或putAll方法添加元素。</p>
<h3 id="Join操作"><a href="#Join操作" class="headerlink" title="Join操作"></a>Join操作</h3><p>Flink的join操作</p>
<p>flink的join操作没有大的限制，支持种类丰富，比如：</p>
<p>Inner Equi-join</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM Orders INNER JOIN Product ONOrders.productId = Product.id</span><br></pre></td></tr></table></figure>

<p>Outer Equi-join</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM Orders LEFT JOIN Product ON Orders.productId =Product.id</span><br><span class="line"></span><br><span class="line">SELECT * FROM Orders RIGHT JOIN Product ON Orders.productId =Product.id</span><br><span class="line"></span><br><span class="line">SELECT * FROM Orders FULL OUTER JOIN Product ONOrders.productId = Product.id</span><br></pre></td></tr></table></figure>

<p>Time-windowed Join</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT * FROM Oderso,Shipmentss WHEREo.id=s.orderIdAND o.ordertimeBETWEENs.shiptime INTERVAL<span class="string">&#x27;4&#x27;</span>HOURANDs.shiptime</span><br></pre></td></tr></table></figure>

<p>Expanding arrays into a relation</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT users, tag FROM Orders CROSS JOIN UNNEST(tags) AS t (tag)</span><br></pre></td></tr></table></figure>

<p>Join with Table Function</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Inner Join</span><br><span class="line"></span><br><span class="line">A row of the left (outer) table is dropped, <span class="keyword">if</span> its table <span class="keyword">function</span> call returns an empty result.</span><br><span class="line">SELECT users, tag</span><br><span class="line">FROM Orders, LATERAL TABLE(unnest_udtf(tags)) t AS tag</span><br><span class="line"></span><br><span class="line">Left Outer Join</span><br><span class="line">If a table <span class="keyword">function</span> call returns an empty result, the corresponding outer row is preserved and the result padded with null values.</span><br><span class="line"></span><br><span class="line">SELECT users, tag</span><br><span class="line">FROM Orders LEFT JOIN LATERAL TABLE(unnest_udtf(tags)) t AS tag ON TRUE</span><br></pre></td></tr></table></figure>

<p>Join with Temporal Table</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">SELECT</span><br><span class="line"> o_amount, r_rate</span><br><span class="line">FROM</span><br><span class="line"> Orders,</span><br><span class="line">LATERAL TABLE (Rates(o_proctime))</span><br><span class="line">WHERE</span><br><span class="line"> r_currency = o_currency</span><br></pre></td></tr></table></figure>

<p>Structured Streaming的join操作<br>Structured Streaming的join限制颇多了，限于篇幅问题在这里只讲一下join的限制<br><img src="/images/42.png" alt="alt"></p>
<h3 id="容错机制及一致性语义"><a href="#容错机制及一致性语义" class="headerlink" title="容错机制及一致性语义"></a>容错机制及一致性语义</h3><p>Spark Streaming 保证仅一次处理</p>
<p>对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。</p>
<p>对于 Spark Streaming 与 kafka 结合的 direct Stream 可以自己维护 offset 到 zookeeper、kafka 或任何其它外部系统，每次提交完结果之后再提交 offset，这样故障恢复重启可以利用上次提交的 offset 恢复，保证数据不丢失。但是假如故障发生在提交结果之后、提交 offset 之前会导致数据多次处理，这个时候我们需要保证处理结果多次输出不影响正常的业务。</p>
<p>由此可以分析，假设要保证数据恰一次处理语义，那么结果输出和 offset 提交必须在一个事务内完成。在这里有以下两种做法：<br>repartition(1) Spark Streaming 输出的 action 变成仅一个 partition，这样可以利用事务去做：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Dstream.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">   rdd.repartition(1).foreachPartition(partition=&gt;&#123;    //    开启事务</span><br><span class="line">       partition.foreach(each=&gt;&#123;//        提交数据</span><br><span class="line">       &#125;)    //  提交事务</span><br><span class="line">   &#125;)</span><br><span class="line"> &#125;)</span><br></pre></td></tr></table></figure>
<p>将结果和 offset 一起提交</p>
<p>也就是结果数据包含 offset。这样提交结果和提交 offset 就是一个操作完成，不会数据丢失，也不会重复处理。故障恢复的时候可以利用上次提交结果带的 offset。</p>
<p>Flink 与 kafka 0.11 保证仅一次处理</p>
<p>若要 sink 支持仅一次语义，必须以事务的方式写数据到 Kafka，这样当提交事务时两次 checkpoint 间的所有写入操作作为一个事务被提交。这确保了出现故障或崩溃时这些写入操作能够被回滚。</p>
<p>在一个分布式且含有多个并发执行 sink 的应用中，仅仅执行单次提交或回滚是不够的，因为所有组件都必须对这些提交或回滚达成共识，这样才能保证得到一致性的结果。Flink 使用两阶段提交协议以及预提交(pre-commit)阶段来解决这个问题。</p>
<p>本例中的 Flink 应用如图所示包含以下组件：</p>
<p>一个source，从Kafka中读取数据（即KafkaConsumer）</p>
<p>一个时间窗口化的聚会操作</p>
<p>一个sink，将结果写回到Kafka（即KafkaProducer）</p>
<p><img src="/images/35.png" alt="alt"><br>下面详细讲解 flink 的两段提交思路</p>
<p><img src="/images/36.png" alt="alt"></p>
<p>Flink checkpointing 开始时便进入到 pre-commit 阶段。具体来说，一旦 checkpoint 开始，Flink 的 JobManager 向输入流中写入一个 checkpoint barrier ，将流中所有消息分割成属于本次 checkpoint 的消息以及属于下次 checkpoint 的，barrier 也会在操作算子间流转。对于每个 operator 来说，该 barrier 会触发 operator 状态后端为该 operator 状态打快照。data source 保存了 Kafka 的 offset，之后把 checkpoint barrier 传递到后续的 operator。</p>
<p>这种方式仅适用于 operator 仅有它的内部状态。内部状态是指 Flink state backends 保存和管理的内容（如第二个 operator 中 window 聚合算出来的 sum）。</p>
<p>当一个进程仅有它的内部状态的时候，除了在 checkpoint 之前将需要将数据更改写入到 state backend，不需要在预提交阶段做其他的动作。在 checkpoint 成功的时候，Flink 会正确的提交这些写入，在 checkpoint 失败的时候会终止提交</p>
<p><img src="/images/37.png" alt="alt"></p>
<p>当结合外部系统的时候，外部系统必须要支持可与两阶段提交协议捆绑使用的事务。显然本例中的 sink 由于引入了 kafka sink，因此在预提交阶段 data sink 必须预提交外部事务。如下图<br><img src="/images/38.png" alt="alt"><br>当 barrier 在所有的算子中传递一遍，并且触发的快照写入完成，预提交阶段完成。所有的触发状态快照都被视为 checkpoint 的一部分，也可以说 checkpoint 是整个应用程序的状态快照，包括预提交外部状态。出现故障可以从 checkpoint 恢复。下一步就是通知所有的操作算子 checkpoint 成功。该阶段 jobmanager 会为每个 operator 发起 checkpoint 已完成的回调逻辑。</p>
<p>本例中 data source 和窗口操作无外部状态，因此该阶段，这两个算子无需执行任何逻辑，但是 data sink 是有外部状态的，因此，此时我们必须提交外部事务，如下图：</p>
<p><img src="/images/39.png" alt="alt"><br>以上就是 flink 实现恰一次处理的基本逻辑。</p>
<h3 id="背压"><a href="#背压" class="headerlink" title="背压"></a>背压</h3><p>消费者消费的速度低于生产者生产的速度，为了使应用正常，消费者会反馈给生产者来调节生产者生产的速度，以使得消费者需要多少，生产者生产多少。</p>
<p>Spark Streaming 的背压<br>Spark Streaming 跟 kafka 结合是存在背压机制的，目标是根据当前 job 的处理情况来调节后续批次的获取 kafka 消息的条数。为了达到这个目的，Spark Streaming 在原有的架构上加入了一个 RateController，利用的算法是 PID，需要的反馈数据是任务处理的结束时间、调度时间、处理时间、消息条数，这些数据是通过 SparkListener 体系获得，然后通过 PIDRateEsimator 的 compute 计算得到一个速率，进而可以计算得到一个 offset，然后跟限速设置最大消费条数比较得到一个最终要消费的消息最大 offset</p>
<p>PIDRateEsimator 的 compute 方法如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">def compute(       </span><br><span class="line">      time: Long, </span><br><span class="line">      // <span class="keyword">in</span> milliseconds  </span><br><span class="line">      numElements: Long,        </span><br><span class="line">      processingDelay: Long, </span><br><span class="line">      // <span class="keyword">in</span> milliseconds  schedulingDelay: Long </span><br><span class="line">      // <span class="keyword">in</span> milliseconds  </span><br><span class="line">      ): Option[Double] = &#123;logTrace(s<span class="string">&quot;\ntime = <span class="variable">$time</span>, # records = <span class="variable">$numElements</span>, &quot;</span> + s<span class="string">&quot;processing time = <span class="variable">$processingDelay</span>, scheduling delay = <span class="variable">$schedulingDelay</span>&quot;</span>)   </span><br><span class="line">      this.synchronized &#123;<span class="keyword">if</span> (time &gt; latestTime &amp;&amp; numElements &gt; 0 &amp;&amp; processingDelay &gt; 0) &#123;</span><br><span class="line">        val delaySinceUpdate = (time - latestTime).toDouble / 1000          </span><br><span class="line">        val processingRate = numElements.toDouble / processingDelay * 1000         </span><br><span class="line">        val error = latestRate - processingRate                 </span><br><span class="line">        val historicalError = schedulingDelay.toDouble * processingRate/ batchIntervalMillis</span><br><span class="line">        // <span class="keyword">in</span> elements/(second ^ 2)        </span><br><span class="line">        val dError = (error - latestError) / delaySinceUpdate        </span><br><span class="line">        val newRate = (latestRate - proportional * error - integral * historicalError - derivative * dError).max(minRate)        </span><br><span class="line">        logTrace(s<span class="string">&quot;&quot;</span><span class="string">&quot; | latestRate = <span class="variable">$latestRate</span>, error = <span class="variable">$error</span> | latestError = <span class="variable">$latestError</span>, historicalError = <span class="variable">$historicalError</span> | delaySinceUpdate = <span class="variable">$delaySinceUpdate</span>, dError = <span class="variable">$dError</span> &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)        </span><br><span class="line">        latestTime = time                 </span><br><span class="line">        <span class="keyword">if</span> (firstRun) &#123; latestRate = processingRate latestError = 0D firstRun = <span class="literal">false</span>  logTrace(<span class="string">&quot;First run, rate estimation skipped&quot;</span>) None  &#125; </span><br><span class="line">        <span class="keyword">else</span> &#123; latestRate = newRate  latestError = error logTrace(s<span class="string">&quot;New rate = <span class="variable">$newRate</span>&quot;</span>)  Some(newRate)       &#125;     &#125; </span><br><span class="line">      <span class="keyword">else</span> &#123;       logTrace(<span class="string">&quot;Rate estimation skipped&quot;</span>)               None     &#125;   &#125; &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>Flink 的背压<br>与 Spark Streaming 的背压不同的是，Flink 背压是 jobmanager 针对每一个 task 每 50ms 触发 100 次 Thread.getStackTrace() 调用，求出阻塞的占比。过程如图 16 所示：</p>
<p><img src="/images/43.png" alt="alt"><br>阻塞占比在 web 上划分了三个等级：</p>
<p>OK: 0 &lt;= Ratio &lt;= 0.10，表示状态良好；<br>LOW: 0.10 &lt; Ratio &lt;= 0.5，表示有待观察；<br>HIGH: 0.5 &lt; Ratio &lt;= 1，表示要处理了。</p>
<h3 id="表管理"><a href="#表管理" class="headerlink" title="表管理"></a>表管理</h3><p>flink和structured streaming都可以讲流注册成一张表，然后使用sql进行分析，不过两者之间区别还是有些的。<br>Structured Streaming将流注册成临时表，然后用sql进行查询，操作也是很简单跟静态的dataset/dataframe一样。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">&quot;updates&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select count(*) from updates&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>其实，此处回想Spark Streaming 如何注册临时表呢？在foreachRDD里，讲rdd转换为dataset/dataframe，然后将其注册成临时表，该临时表特点是代表当前批次的数据，而不是全量数据。Structured Streaming注册的临时表就是流表，针对整个实时流的。Sparksession.sql执行结束后，返回的是一个流dataset/dataframe,当然这个很像spark sql的sql文本执行，所以为了区别一个dataframe/dataset是否是流式数据，可以df.isStreaming来判断。</p>
<p>当然，flink也支持直接注册流表，然后写sql分析，sql文本在flink中使用有两种形式：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">1). tableEnv.sqlQuery(<span class="string">&quot;SELECT product,amount FROM Orders WHERE product LIKE &#x27;%Rubber%&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">2). tableEnv.sqlUpdate(</span><br><span class="line"><span class="string">&quot;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHEREproduct LIKE &#x27;%Rubber%&#x27;&quot;</span>);</span><br></pre></td></tr></table></figure>


<p>对于第一种形式，sqlQuery执行结束之后会返回一张表也即是Table对象,然后可以进行后续操作或者直接输出，如：result.writeAsCsv(“”);。<br>而sqlUpdate是直接将结果输出到了tablesink，所以要首先注册tablesink，方式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">TableSink csvSink = newCsvTableSink(<span class="string">&quot;/path/to/file&quot;</span>, ...);</span><br><span class="line"></span><br><span class="line">String[] fieldNames = &#123;<span class="string">&quot;product&quot;</span>,<span class="string">&quot;amount&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">TypeInformation[] fieldTypes =&#123;Types.STRING, Types.INT&#125;;</span><br><span class="line"></span><br><span class="line">tableEnv.registerTableSink(<span class="string">&quot;RubberOrders&quot;</span>,fieldNames, fieldTypes, csvSink);</span><br></pre></td></tr></table></figure>

<p>flink注册表的形式比较多，直接用数据源注册表，如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tableEnv.registerExternalCatalog();</span><br><span class="line">tableEnv.registerTableSource();</span><br></pre></td></tr></table></figure>
<p>也可以从datastream转换成表，如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tableEnv.registerDataStream(<span class="string">&quot;Orders&quot;</span>,ds, <span class="string">&quot;user, product, amount&quot;</span>);</span><br><span class="line">Table table = tableEnv.fromDataStream(ds,<span class="string">&quot;user, product, amount&quot;</span>);</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Flink,Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink端到端状态一致性EXACTLY_ONCE实现</title>
    <url>/2020/05/20/26/</url>
    <content><![CDATA[<h3 id="状态一致性"><a href="#状态一致性" class="headerlink" title="状态一致性:"></a>状态一致性:</h3><p><img src="/images/28.png" alt="alt"><br>有状态的流处理，内部每个算子任务都可以有自己的状态；</p>
<p>对于流处理器内部（没有接入sink）来说，所谓的状态一致性，其实就是我们所说的计算结果要保证准确；</p>
<p>一条数据不应该丢失，也不应该重复计算；</p>
<p>在遇到故障时可以恢复状态，恢复以后的重新计算，结果应该也是完全正常的</p>
<h3 id="状态一致性分类："><a href="#状态一致性分类：" class="headerlink" title="状态一致性分类："></a>状态一致性分类：</h3><p>AT_MOST_ONCE（最多一次），当任务故障时最简单做法是什么都不干，既不恢复丢失状态，也不重播丢失数据。At-most-once语义的含义是最多处理一次事件。</p>
<p>AT_LEAST_ONCE（至少一次），在大多数真实应用场景，我们希望不丢失数据。这种类型的保障称为at-least-once，意思是所有的事件都得到了处理，而一些事件还可能被处理多次。</p>
<p>EXACTLY_ONCE（精确一次），恰好处理一次是最严格的的保证，也是最难实现的。恰好处理一次语义不仅仅意味着没有事件丢失，还意味着针对每一个数据，内部状态仅仅更新一次。</p>
<h3 id="一致性检查点（Checkpoints）"><a href="#一致性检查点（Checkpoints）" class="headerlink" title="一致性检查点（Checkpoints）"></a>一致性检查点（Checkpoints）</h3><p>Flink使用了一种轻量级快照机制 — 检查点（Checkpoint）来保证exactly-one语义；</p>
<p>有状态流应用的一致检查点，其实就是：所有任务的状态，在某个时间点的一份拷贝（一份快照）。而这个时间点，应该是所有任务都恰好处理完一个相同的输入数据的时候。</p>
<p>应用状态的一致性检查点，是Flink故障恢复机制的核心。</p>
<p><img src="/images/29.png" alt="alt"></p>
<h3 id="端到端（end-to-end）状态一致性"><a href="#端到端（end-to-end）状态一致性" class="headerlink" title="端到端（end-to-end）状态一致性"></a>端到端（end-to-end）状态一致性</h3><p>目前我们看到的一致性保证都是由流处理器实现的，也就是说都是在Flink流处理内部保证的；而在真实应用中，流处理应用除了流处理器以外还包含了数据源（例如kafka）和输出到持久化系统；</p>
<p>端到端的一致性保证，意味着结果的正确性贯穿了整个流处理应用的始终，每个组件都保证了它自己的一致性；</p>
<p>整个端到端的一致性级别取决于所有组件中一致性最弱的组件；</p>
<h3 id="端到端exactly-once"><a href="#端到端exactly-once" class="headerlink" title="端到端exactly-once"></a>端到端exactly-once</h3><p>内部保证 — checkpoint</p>
<p>source端 — 可重设数据的读取位置；可重新读取偏移量</p>
<p>sink端 – 从故障恢复时，数据不会重复写入外部系统：幂等写入和事务写入；</p>
<h3 id="幂等写入（Idempotent-Writes）："><a href="#幂等写入（Idempotent-Writes）：" class="headerlink" title="幂等写入（Idempotent Writes）："></a>幂等写入（Idempotent Writes）：</h3><p>幂等操作即一个操作可以重复执行很多次，但只导致一次结果更改，也就是说，后面再重复执行就不起作用了；</p>
<p><img src="/images/30.png" alt="alt"></p>
<p> 它的原理不是不重复写入而是重复写完之后结果还是一样；它的瑕疵是不能做到完全意义上exactly-once（在故障恢复时，突然把外部系统写入操作跳到之前的某个状态然后继续往里边写，故障之前发生的那一段的状态变化又重演了直到最后发生故障那一刻追上就正常了；假如中间这段又被读取了就可能会有些问题）；</p>
<h3 id="事务写入（Transactional-Writes）"><a href="#事务写入（Transactional-Writes）" class="headerlink" title="事务写入（Transactional Writes）:"></a>事务写入（Transactional Writes）:</h3><p> 事务Transactional Writes</p>
<p>应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤销；<br>具有原子性，一个事务中的一系列的操作要么全部成功，要么一个都不做。<br>实现思想：构建的事务对应着checkpoint，等到checkpoint真正完成的时候，才把所有对应的结果写入sink系统中。</p>
<p>实现方式：预习日志和两阶段提交</p>
<h3 id="预习日志（Write-Ahead-Log，WAL）"><a href="#预习日志（Write-Ahead-Log，WAL）" class="headerlink" title="预习日志（Write-Ahead-Log，WAL）"></a>预习日志（Write-Ahead-Log，WAL）</h3><p>把结果数据先当成状态保存，然后在收到checkpoint完成的通知时，一次性写入sink系统；</p>
<p>简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定；</p>
<p>DataStream API提供了一个模板类：GenericWriteAheadSink，来实现这种事务性sink；</p>
<p>瑕疵：<br>A. sink系统没说它支持事务，有可能出现一部分写进去了一部分没写进去（如果算失败，再写一次就写了两次了）；</p>
<p>B. checkpoint做完了sink才去真正写入（但其实得等sink都写完checkpoint才能生效，所以WAL这个机制jobmanager确定它写完还不算真正写完，还得有一个外部系统已经确认完成的checkpoint）</p>
<h3 id="两阶段提交（Two–Phase–Commit，2PC）–-真正能够实现exactly-once"><a href="#两阶段提交（Two–Phase–Commit，2PC）–-真正能够实现exactly-once" class="headerlink" title="两阶段提交（Two–Phase–Commit，2PC）– 真正能够实现exactly-once"></a>两阶段提交（Two–Phase–Commit，2PC）– 真正能够实现exactly-once</h3><p>对于每个checkpoint，sink任务会启动一个事务，并将接下来所有接收的数据添加到事务里；</p>
<p>然后将这些数据写入外部sink系统，但不提交他们 – 这时只是预提交；</p>
<p>当它收到checkpoint完成时的通知，它才正式提交事务，实现结果的真正写入；</p>
<p>这种方式真正实现了exactly-once，它需要一个提供事务支持的外部sink系统，Flink提供了TwoPhaseCommitSinkFunction接口</p>
<h3 id="2PC对外部sink的要求"><a href="#2PC对外部sink的要求" class="headerlink" title="2PC对外部sink的要求"></a>2PC对外部sink的要求</h3><p>外部sink系统必须事务支持，或者sink任务必须能够模拟外部系统上的事务；</p>
<p>在checkpoint的间隔期间里，必须能够开启一个事务，并接受数据写入；</p>
<p>在收到checkpoint完成的通知之前，事务必须是“等待提交”的状态，在故障恢复的情况下，这可能需要一些时间。如果这个时候sink系统关闭事务（例如超时了），那么未提交的数据就会丢失；</p>
<p>sink任务必须能够在进程失败后恢复事务；</p>
<p>提交事务必须是幂等操作；</p>
<h3 id="不同Source和sink的一致性保证："><a href="#不同Source和sink的一致性保证：" class="headerlink" title="不同Source和sink的一致性保证："></a>不同Source和sink的一致性保证：</h3><p><img src="/images/31.png" alt="alt"></p>
<p>Flink+kafka端到端状态一致性的保证<br>Flink和kafka天生就是一对，用kafka做为source，用kafka做完sink  &lt;===&gt;  实现端到端的一致性</p>
<p>内部 – 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性；</p>
<p>source – kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性；</p>
<p>sink – kafka producer作为sink，采用两阶段提交sink，需要实现一个TwoPhaseCommitSinkFunction</p>
<p><img src="/images/32.png" alt="alt"></p>
<p><img src="/images/33.png" alt="alt"></p>
<p><img src="/images/34.png" alt="alt"></p>
<p> 默认是AT_LEAST_ONCE</p>
<p>Exactly-once两阶段提交</p>
<p><img src="/images/35.png" alt="alt"><br>JobManager协调各个TaskManager进行checkpoint存储；</p>
<p>checkpoint保存在StateBackend中，默认StateBackend是内存级的，也可以改为文件级的进行持久化保存；</p>
<p><img src="/images/36.png" alt="alt"><br>当checkpoint启动时，JobManager会将检查点分界线（barrier）注入数据流；</p>
<p>barrier会在算子间传递下去；</p>
<p><img src="/images/37.png" alt="alt"></p>
<p>每个算子会对当前的状态做个快照，保存到状态后端；</p>
<p>checkpoint机制可以保证内部的状态一致性；</p>
<p><img src="/images/38.png" alt="alt"><br>每个内部的transform任务遇到barrier时，都会把状态存到checkpoint里；</p>
<p>sink任务首先把数据写入外部kafka，这些数据都属于预提交的事务；遇到barrier时，把状态保存到状态后端，并开启新的预提交事务（以barrier为界之前的数据属于上一个事务，之后的数据属于下一个新的事务）；</p>
<p><img src="/images/39.png" alt="alt"><br>当所有算子任务的快照完成，也就是这次的checkpoint完成时，JobManager会向所有任务发通知，确认这次checkpoint完成；</p>
<p>sink任务收到确认通知，正式提交之前的事务，kafka中未确认数据改完“已确认”；</p>
<h3 id="Exactly-once两阶段提交步骤："><a href="#Exactly-once两阶段提交步骤：" class="headerlink" title="Exactly-once两阶段提交步骤："></a>Exactly-once两阶段提交步骤：</h3><p> 第一条数据来在之后，开启一个kafka的事务（transaction），正常写入kafka分区日志但标记为未提交，这就是“预提交”；</p>
<p> JobManagere触发checkpoint操作，barrier从source开始向下传递，遇到barrier的算子将状态存入状态后端，并通知JobManagere；</p>
<p> sink连接器接收到barrier，保存当前状态，存入checkpoint，通知JobManager并开启下一阶段的事务，用于提交下个检查点的数据；</p>
<p> JobManager收到所有任务的通知，发出确认信息，表示checkpoint完成；</p>
<p> sink任务收到JobManager的确认信息，正式提交这段时间的数据；</p>
<p> 外部kafka关闭事务，提交的数据可以正常消费了。</p>
<p>在代码中真正实现flink和kafak的端到端exactly-once语义：</p>
<p><img src="/images/40.png" alt="alt"></p>
<p><img src="/images/41.png" alt="alt"></p>
<p>A. 这里需要配置下，因为它默认的是AT_LEAST_ONCE；</p>
<p>B. 对于外部kafka读取的消费者的隔离级别，默认是read_on_commited，如果默认是可以读未提交的数据，就相当于整个一致性还没得到保证（未提交的数据没有最终确认那边就可以读了，相当于那边已经消费数据了，事务就是假的了）  所以需要修改kafka的隔离级别；</p>
<p>C. timeout超时问题，flink和kafka 默认sink是超时1h，而kafak集群中配置的tranctraction事务的默认超时时间是15min，flink-kafak这边的连接器的时间长，这边还在等着做操作 ，kafak那边等checkpoint等的时间太长直接关闭了。所以两边的超时时间最起码前边要比后边的小</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink使用assignAscendingTimestamps 生成水印的三个重载方法</title>
    <url>/2020/05/13/25/</url>
    <content><![CDATA[<h3 id="先简单介绍一下Timestamp-和Watermark-的概念"><a href="#先简单介绍一下Timestamp-和Watermark-的概念" class="headerlink" title="先简单介绍一下Timestamp 和Watermark 的概念:"></a>先简单介绍一下Timestamp 和Watermark 的概念:</h3><ol>
<li>Timestamp和Watermark都是基于事件的时间字段生成的</li>
<li>Timestamp和Watermark是两个不同的东西，并且一旦生成都跟事件数据没有关系了（所有即使事件中不再包含生成Timestamp和Watermark的字段也没关系）</li>
<li>事件数据和 Timestamp 一一对应（事件在流中传递以StreamRecord对象表示，value 和 timestamp 是它的两个成员变量)</li>
<li>Watermark 在生成之后与事件数据没有直接关系，Watermark 作为一个消息，和事件数据一样在流中传递（Watermark 和StreamRecord 具有相同的父类：StreamElement）</li>
<li>Timestamp 与 Watermark 在生成之后，会在下游window算子中做比较，判断事件数据是否是过期数据</li>
<li>只有window算子才会用Watermark判断事件数据是否过期</li>
</ol>
<h3 id="assignTimestamps-extractor-TimestampExtractor-T-DataStream-T"><a href="#assignTimestamps-extractor-TimestampExtractor-T-DataStream-T" class="headerlink" title="assignTimestamps(extractor: TimestampExtractor[T]): DataStream[T]"></a>assignTimestamps(extractor: TimestampExtractor[T]): DataStream[T]</h3><p>此方法是数据流的快捷方式，其中已知元素时间戳在每个并行流中单调递增。在这种情况下，系统可以通过跟踪上升时间戳自动且完美地生成水印。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">val input = env.addSource(<span class="built_in">source</span>)</span><br><span class="line">.map(json =&gt; &#123;</span><br><span class="line">        val id = json.get(<span class="string">&quot;id&quot;</span>).asText()</span><br><span class="line">        val createTime = json.get(<span class="string">&quot;createTime&quot;</span>).asText()</span><br><span class="line">        val amt = json.get(<span class="string">&quot;amt&quot;</span>).asText()</span><br><span class="line">        LateDataEvent(<span class="string">&quot;key&quot;</span>, id, createTime, amt)</span><br><span class="line">      &#125;)</span><br><span class="line">      // flink auto create timestamp &amp; watermark</span><br><span class="line">      .assignAscendingTimestamps(element =&gt; sdf.parse(element.createTime).getTime)</span><br></pre></td></tr></table></figure>
<p>注：这种方法创建时间戳与水印最简单，返回一个long类型的数字就可以了</p>
<h3 id="assignTimestampsAndWatermarks-assigner-AssignerWithPeriodicWatermarks-T-DataStream-T"><a href="#assignTimestampsAndWatermarks-assigner-AssignerWithPeriodicWatermarks-T-DataStream-T" class="headerlink" title="assignTimestampsAndWatermarks(assigner: AssignerWithPeriodicWatermarks[T]): DataStream[T]"></a>assignTimestampsAndWatermarks(assigner: AssignerWithPeriodicWatermarks[T]): DataStream[T]</h3><p>基于给定的水印生成器生成水印，即使没有新元素到达也会定期检查给定水印生成器的新水印，以指定允许延迟时间</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">val input = env.addSource(<span class="built_in">source</span>)</span><br><span class="line">      .map(json =&gt; &#123;</span><br><span class="line">        val id = json.get(<span class="string">&quot;id&quot;</span>).asText()</span><br><span class="line">        val createTime = json.get(<span class="string">&quot;createTime&quot;</span>).asText()</span><br><span class="line">        val amt = json.get(<span class="string">&quot;amt&quot;</span>).asText()</span><br><span class="line">        LateDataEvent(<span class="string">&quot;key&quot;</span>, id, createTime, amt)</span><br><span class="line">      &#125;)</span><br><span class="line">      // assign timestamp &amp; watermarks periodically(定期生成水印)</span><br><span class="line">      .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[LateDataEvent](Time.milliseconds(50)) &#123;</span><br><span class="line">       override def extractTimestamp(element: LateDataEvent): Long = &#123;</span><br><span class="line">         println(<span class="string">&quot;want watermark : &quot;</span> + sdf.parse(element.createTime).getTime)</span><br><span class="line">         sdf.parse(element.createTime).getTime</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;)</span><br></pre></td></tr></table></figure>

<h3 id="assignTimestampsAndWatermarks-assigner-AssignerWithPeriodicWatermarks-T-DataStream-T-1"><a href="#assignTimestampsAndWatermarks-assigner-AssignerWithPeriodicWatermarks-T-DataStream-T-1" class="headerlink" title="assignTimestampsAndWatermarks(assigner: AssignerWithPeriodicWatermarks[T]): DataStream[T]"></a>assignTimestampsAndWatermarks(assigner: AssignerWithPeriodicWatermarks[T]): DataStream[T]</h3><p>此方法仅基于流元素创建水印，对于通过[[AssignerWithPunctuatedWatermarks＃extractTimestamp（Object，long）]]处理的每个元素，<br>调用[[AssignerWithPunctuatedWatermarks＃checkAndGetNextWatermark（）]]方法，如果返回的水印值大于以前的水印，会发出新的水印，<br>此方法可以完全控制水印的生成，但是要注意，每秒生成数百个水印会影响性能</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">val input = env.addSource(<span class="built_in">source</span>)</span><br><span class="line">      .map(json =&gt; &#123;</span><br><span class="line">        val id = json.get(<span class="string">&quot;id&quot;</span>).asText()</span><br><span class="line">        val createTime = json.get(<span class="string">&quot;createTime&quot;</span>).asText()</span><br><span class="line">        val amt = json.get(<span class="string">&quot;amt&quot;</span>).asText()</span><br><span class="line">        LateDataEvent(<span class="string">&quot;key&quot;</span>, id, createTime, amt)</span><br><span class="line">      &#125;)</span><br><span class="line">      // assign timestamp &amp; watermarks every event</span><br><span class="line">      .assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks[LateDataEvent]() &#123;</span><br><span class="line">      // check extractTimestamp emitted watermark is non-null and large than previously</span><br><span class="line">      override def checkAndGetNextWatermark(lastElement: LateDataEvent, extractedTimestamp: Long): Watermark = &#123;</span><br><span class="line">        new Watermark(extractedTimestamp)</span><br><span class="line">      &#125;</span><br><span class="line">      // generate next watermark</span><br><span class="line">      override def extractTimestamp(element: LateDataEvent, previousElementTimestamp: Long): Long = &#123;</span><br><span class="line">        val eventTime = sdf.parse(element.createTime).getTime</span><br><span class="line">        eventTime</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<p><a href="https://www.cnblogs.com/Springmoon-venn/p/11403665.html">转载于</a></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink使用SQL操作几种类型的window</title>
    <url>/2020/05/11/24/</url>
    <content><![CDATA[<p>Flink SQL 支持三种窗口类型, 分别为 Tumble Windows / HOP Windows 和 Session Windows. 其中 HOP windows 对应 Table API 中的 Sliding Window, 同时每种窗口分别有相应的使用场景和方法</p>
<p>Tumble Window（翻转窗口）<br>Hop Window（滑动窗口）<br>Session Window（会话窗口）</p>
<h3 id="HOPWindowExample"><a href="#HOPWindowExample" class="headerlink" title="HOPWindowExample"></a>HOPWindowExample</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package sql.window;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeHint;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;</span><br><span class="line">import org.apache.flink.table.api.Table;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">import java.sql.Timestamp;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class HOPWindowExample &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        // 获取 environment</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        // 指定系统时间概念为 event time</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 初始数据</span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; <span class="built_in">log</span> = env.fromCollection(Arrays.asList(</span><br><span class="line">                //时间 14:53:00</span><br><span class="line">                new Tuple3&lt;&gt;(1572591180_000L,<span class="string">&quot;xiao_ming&quot;</span>,300),</span><br><span class="line">                //时间 14:53:09</span><br><span class="line">                new Tuple3&lt;&gt;(1572591189_000L,<span class="string">&quot;zhang_san&quot;</span>,303),</span><br><span class="line">                //时间 14:53:12</span><br><span class="line">                new Tuple3&lt;&gt;(1572591192_000L, <span class="string">&quot;xiao_li&quot;</span>,204),</span><br><span class="line">                //时间 14:53:21</span><br><span class="line">                new Tuple3&lt;&gt;(1572591201_000L,<span class="string">&quot;li_si&quot;</span>, 208)</span><br><span class="line">        ));</span><br><span class="line"></span><br><span class="line">        // 指定时间戳</span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;Long, String, Integer&gt;&gt; logWithTime = log.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public long extractAscendingTimestamp(Tuple3&lt;Long, String, Integer&gt; element) &#123;</span><br><span class="line">                <span class="built_in">return</span> element.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        // 转换为 Table</span><br><span class="line">        Table logT = tEnv.fromDataStream(logWithTime, <span class="string">&quot;t.rowtime, name, v&quot;</span>);</span><br><span class="line"></span><br><span class="line">        // HOP(time_attr, interval1, interval2)</span><br><span class="line">        // interval1 滑动长度</span><br><span class="line">        // interval2 窗口长度</span><br><span class="line">        Table result = tEnv.sqlQuery(<span class="string">&quot;SELECT HOP_START(t, INTERVAL &#x27;5&#x27; SECOND, INTERVAL &#x27;10&#x27; SECOND) AS window_start,&quot;</span> +</span><br><span class="line">                <span class="string">&quot;HOP_END(t, INTERVAL &#x27;5&#x27; SECOND, INTERVAL &#x27;10&#x27; SECOND) AS window_end, SUM(v) FROM &quot;</span></span><br><span class="line">                + logT + <span class="string">&quot; GROUP BY HOP(t, INTERVAL &#x27;5&#x27; SECOND, INTERVAL &#x27;10&#x27; SECOND)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        TypeInformation&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt; tpinf = new TypeHint&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt;()&#123;&#125;.getTypeInfo();</span><br><span class="line">        tEnv.toAppendStream(result, tpinf).<span class="built_in">print</span>();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="TumbleWindowExample"><a href="#TumbleWindowExample" class="headerlink" title="TumbleWindowExample"></a>TumbleWindowExample</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package sql.window;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeHint;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;</span><br><span class="line">import org.apache.flink.table.api.Table;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">import java.sql.Timestamp;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class TumbleWindowExample &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        // 获取 environment</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        // 指定系统时间概念为 event time</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 初始数据</span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; <span class="built_in">log</span> = env.fromCollection(Arrays.asList(</span><br><span class="line">                //时间 14:53:00</span><br><span class="line">                new Tuple3&lt;&gt;(1572591180_000L,<span class="string">&quot;xiao_ming&quot;</span>,300),</span><br><span class="line">                //时间 14:53:09</span><br><span class="line">                new Tuple3&lt;&gt;(1572591189_000L,<span class="string">&quot;zhang_san&quot;</span>,303),</span><br><span class="line">                //时间 14:53:12</span><br><span class="line">                new Tuple3&lt;&gt;(1572591192_000L, <span class="string">&quot;xiao_li&quot;</span>,204),</span><br><span class="line">                //时间 14:53:21</span><br><span class="line">                new Tuple3&lt;&gt;(1572591201_000L,<span class="string">&quot;li_si&quot;</span>, 208)</span><br><span class="line">                ));</span><br><span class="line"></span><br><span class="line">        // 指定时间戳</span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;Long, String, Integer&gt;&gt; logWithTime = log.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public long extractAscendingTimestamp(Tuple3&lt;Long, String, Integer&gt; element) &#123;</span><br><span class="line">                <span class="built_in">return</span> element.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        // 转换为 Table</span><br><span class="line">        Table logT = tEnv.fromDataStream(logWithTime, <span class="string">&quot;t.rowtime, name, v&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Table result = tEnv.sqlQuery(<span class="string">&quot;SELECT TUMBLE_START(t, INTERVAL &#x27;10&#x27; SECOND) AS window_start,&quot;</span> +</span><br><span class="line">                <span class="string">&quot;TUMBLE_END(t, INTERVAL &#x27;10&#x27; SECOND) AS window_end, SUM(v) FROM &quot;</span></span><br><span class="line">                + logT + <span class="string">&quot; GROUP BY TUMBLE(t, INTERVAL &#x27;10&#x27; SECOND)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        TypeInformation&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt; tpinf = new TypeHint&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt;()&#123;&#125;.getTypeInfo();</span><br><span class="line">        tEnv.toAppendStream(result, tpinf).<span class="built_in">print</span>();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="SessionWindowExample"><a href="#SessionWindowExample" class="headerlink" title="SessionWindowExample"></a>SessionWindowExample</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package sql.window;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeHint;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;</span><br><span class="line">import org.apache.flink.table.api.Table;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">import java.sql.Timestamp;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class SessionWindowExample &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        // 获取 environment</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        // 指定系统时间概念为 event time</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 初始数据</span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; <span class="built_in">log</span> = env.fromCollection(Arrays.asList(</span><br><span class="line">                //时间 14:53:00</span><br><span class="line">                new Tuple3&lt;&gt;(1572591180_000L,<span class="string">&quot;xiao_ming&quot;</span>,300),</span><br><span class="line"></span><br><span class="line">                /*    Start Session   */</span><br><span class="line">                //时间 14:53:09</span><br><span class="line">                new Tuple3&lt;&gt;(1572591189_000L,<span class="string">&quot;zhang_san&quot;</span>,303),</span><br><span class="line">                //时间 14:53:12</span><br><span class="line">                new Tuple3&lt;&gt;(1572591192_000L, <span class="string">&quot;xiao_li&quot;</span>,204),</span><br><span class="line"></span><br><span class="line">                /*    Start Session   */</span><br><span class="line">                //时间 14:53:21</span><br><span class="line">                new Tuple3&lt;&gt;(1572591201_000L,<span class="string">&quot;li_si&quot;</span>, 208)</span><br><span class="line">        ));</span><br><span class="line"></span><br><span class="line">        // 指定时间戳</span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;Long, String, Integer&gt;&gt; logWithTime = log.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public long extractAscendingTimestamp(Tuple3&lt;Long, String, Integer&gt; element) &#123;</span><br><span class="line">                <span class="built_in">return</span> element.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        // 转换为 Table</span><br><span class="line">        Table logT = tEnv.fromDataStream(logWithTime, <span class="string">&quot;t.rowtime, name, v&quot;</span>);</span><br><span class="line"></span><br><span class="line">        // SESSION(time_attr, interval)</span><br><span class="line">        // interval 表示两条数据触发session的最大间隔</span><br><span class="line">        Table result = tEnv.sqlQuery(<span class="string">&quot;SELECT SESSION_START(t, INTERVAL &#x27;5&#x27; SECOND) AS window_start,&quot;</span> +</span><br><span class="line">                <span class="string">&quot;SESSION_END(t, INTERVAL &#x27;5&#x27; SECOND) AS window_end, SUM(v) FROM &quot;</span></span><br><span class="line">                + logT + <span class="string">&quot; GROUP BY SESSION(t, INTERVAL &#x27;5&#x27; SECOND)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        TypeInformation&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt; tpinf = new TypeHint&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt;()&#123;&#125;.getTypeInfo();</span><br><span class="line">        tEnv.toAppendStream(result, tpinf).<span class="built_in">print</span>();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>AppendStreamTableSink、RetractStreamTableSink、UpsertStreamTableSink</title>
    <url>/2020/05/08/23/</url>
    <content><![CDATA[<h3 id="Flink-Table-amp-SQL-StreamTableSink有三类接口-AppendStreamTableSink、UpsertStreamTableSink、RetractStreamTableSink。"><a href="#Flink-Table-amp-SQL-StreamTableSink有三类接口-AppendStreamTableSink、UpsertStreamTableSink、RetractStreamTableSink。" class="headerlink" title="Flink Table &amp; SQL StreamTableSink有三类接口: AppendStreamTableSink、UpsertStreamTableSink、RetractStreamTableSink。"></a>Flink Table &amp; SQL StreamTableSink有三类接口: AppendStreamTableSink、UpsertStreamTableSink、RetractStreamTableSink。</h3><p>AppendStreamTableSink: 可将动态表转换为Append流。适用于动态表只有Insert的场景。</p>
<p>RetractStreamTableSink: 可将动态表转换为Retract流。适用于动态表有Insert、Delete、Update的场景。</p>
<p>UpsertStreamTableSink: 可将动态表转换为Upsert流。适用于动态表有Insert、Delete、Update的场景。</p>
<p>注意:</p>
<p>RetractStreamTableSink中: Insert被编码成一条Add消息; Delete被编码成一条Retract消息;Update被编码成两条消息(先是一条Retract消息，再是一条Add消息)，即先删除再增加。</p>
<p>UpsertStreamTableSink: Insert和Update均被编码成一条消息(Upsert消息); Delete被编码成一条Delete消息。</p>
<p>UpsertStreamTableSink和RetractStreamTableSink最大的不同在于Update编码成一条消息，效率上比RetractStreamTableSink高。</p>
<p>上述说的编码指的是动态表转换为DataStream时，表的增删改如何体现到DataStream上。</p>
<h3 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:00&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:02&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:06&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:10&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:06&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_3&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:06&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_3&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:12&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_4&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:06&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_5&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:06&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:15&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:02:16&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br><span class="line">&#123;<span class="string">&quot;userID&quot;</span>: <span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;eventTime&quot;</span>: <span class="string">&quot;2016-01-01 10:03:16&quot;</span>, <span class="string">&quot;eventType&quot;</span>: <span class="string">&quot;browse&quot;</span>, <span class="string">&quot;productID&quot;</span>: <span class="string">&quot;product_5&quot;</span>, <span class="string">&quot;productPrice&quot;</span>: 20&#125;</span><br></pre></td></tr></table></figure>

<h3 id="AppendStreamTableSink-示例"><a href="#AppendStreamTableSink-示例" class="headerlink" title="AppendStreamTableSink 示例"></a>AppendStreamTableSink 示例</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.shuai.test;</span><br><span class="line"></span><br><span class="line">import java.time.LocalDateTime;</span><br><span class="line">import java.time.OffsetDateTime;</span><br><span class="line">import java.time.ZoneOffset;</span><br><span class="line">import java.time.format.DateTimeFormatter;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSink;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.table.api.DataTypes;</span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.TableSchema;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">import org.apache.flink.table.sinks.TableSink;</span><br><span class="line">import org.apache.flink.table.types.DataType;</span><br><span class="line">import org.apache.flink.types.Row;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON;</span><br><span class="line">import com.shuai.test.model.UserBrowseLog;</span><br><span class="line"></span><br><span class="line">public class AppendStreamTableSink &#123;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">		// 设置运行环境</span><br><span class="line">		EnvironmentSettings environmentSettings = EnvironmentSettings.newInstance().useBlinkPlanner().build();</span><br><span class="line"></span><br><span class="line">		StreamExecutionEnvironment streamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">		StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(streamExecutionEnvironment,environmentSettings);</span><br><span class="line"></span><br><span class="line">		Properties props = new Properties();</span><br><span class="line">		props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">//		props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.3.122:9092&quot;</span>);</span><br><span class="line">		props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">//		props.setProperty(<span class="string">&quot;connector.version&quot;</span>, <span class="string">&quot;universal&quot;</span>);</span><br><span class="line">//		DataStream&lt;UserBrowseLog&gt; browseStream = streamExecutionEnvironment</span><br><span class="line">//				.addSource(new FlinkKafkaConsumer011&lt;&gt;(<span class="string">&quot;demo&quot;</span>, new SimpleStringSchema(), props))</span><br><span class="line">//				.process(new BrowseKafkaProcessFunction());</span><br><span class="line"></span><br><span class="line">//	       DataStream&lt;UserBrowseLog&gt; browseStream=streamExecutionEnvironment</span><br><span class="line">//	                .addSource(new FlinkKafkaConsumer010&lt;&gt;(<span class="string">&quot;demo&quot;</span>, new SimpleStringSchema(), props))</span><br><span class="line">//	                .process(new BrowseKafkaProcessFunction());</span><br><span class="line"></span><br><span class="line">		FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(<span class="string">&quot;demo&quot;</span>, new SimpleStringSchema(), props);</span><br><span class="line"></span><br><span class="line">		DataStream&lt;UserBrowseLog&gt; browseStream = streamExecutionEnvironment.addSource(consumer)</span><br><span class="line">				.process(new BrowseKafkaProcessFunction()).setParallelism(1);</span><br><span class="line"></span><br><span class="line">		tableEnvironment.registerDataStream(<span class="string">&quot;source_kafka_browse_log&quot;</span>, browseStream,</span><br><span class="line">				<span class="string">&quot;userID,eventTime,eventType,productID,productPrice,eventTimeTimestamp&quot;</span>);</span><br><span class="line"></span><br><span class="line">		// 4、注册AppendStreamTableSink</span><br><span class="line">		String[] sinkFieldNames = &#123; <span class="string">&quot;userID&quot;</span>, <span class="string">&quot;eventTime&quot;</span>, <span class="string">&quot;eventType&quot;</span>, <span class="string">&quot;productID&quot;</span>, <span class="string">&quot;productPrice&quot;</span>,</span><br><span class="line">				<span class="string">&quot;eventTimeTimestamp&quot;</span> &#125;;</span><br><span class="line">		DataType[] sinkFieldTypes = &#123; DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),</span><br><span class="line">				DataTypes.INT(), DataTypes.BIGINT() &#125;;</span><br><span class="line">		TableSink&lt;Row&gt; myAppendStreamTableSink = new MyAppendStreamTableSink(sinkFieldNames, sinkFieldTypes);</span><br><span class="line">		tableEnvironment.registerTableSink(<span class="string">&quot;sink_stdout&quot;</span>, myAppendStreamTableSink);</span><br><span class="line"></span><br><span class="line">		// 5、连续查询</span><br><span class="line"></span><br><span class="line">		String sql = <span class="string">&quot;insert into sink_stdout select userID,eventTime,eventType,productID,productPrice,eventTimeTimestamp from source_kafka_browse_log where userID=&#x27;user_1&#x27;&quot;</span>;</span><br><span class="line">		tableEnvironment.sqlUpdate(sql);</span><br><span class="line"></span><br><span class="line">		tableEnvironment.execute(AppendStreamTableSink.class.getSimpleName());</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static class BrowseKafkaProcessFunction extends ProcessFunction&lt;String, UserBrowseLog&gt; &#123;</span><br><span class="line"></span><br><span class="line">		@Override</span><br><span class="line">		public void processElement(String value, ProcessFunction&lt;String, UserBrowseLog&gt;.Context ctx,</span><br><span class="line">				Collector&lt;UserBrowseLog&gt; out) throws Exception &#123;</span><br><span class="line">			UserBrowseLog browseLog = JSON.parseObject(value, UserBrowseLog.class);</span><br><span class="line">			// 增加一个long类型的时间戳</span><br><span class="line">			// 指定eventTime为yyyy-MM-dd HH:mm:ss格式的北京时间</span><br><span class="line">			java.time.format.DateTimeFormatter format = DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">			OffsetDateTime eventTime = LocalDateTime.parse(browseLog.getEventTime(), format)</span><br><span class="line">					.atOffset(ZoneOffset.of(<span class="string">&quot;+08:00&quot;</span>));</span><br><span class="line">			// 转换成毫秒时间戳</span><br><span class="line">			long eventTimeTimestamp = eventTime.toInstant().toEpochMilli();</span><br><span class="line">			browseLog.setEventTimeTimestamp(eventTimeTimestamp);</span><br><span class="line"></span><br><span class="line">			out.collect(browseLog);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	private static class MyAppendStreamTableSink implements org.apache.flink.table.sinks.AppendStreamTableSink&lt;Row&gt; &#123;</span><br><span class="line"></span><br><span class="line">		private TableSchema tableSchema;</span><br><span class="line"></span><br><span class="line">		public MyAppendStreamTableSink(String[] fieldNames, DataType[] fieldTypes) &#123;</span><br><span class="line">			this.tableSchema = TableSchema.builder().fields(fieldNames, fieldTypes).build();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		@Override</span><br><span class="line">		public TableSchema <span class="function"><span class="title">getTableSchema</span></span>() &#123;</span><br><span class="line">			<span class="built_in">return</span> tableSchema;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		@Override</span><br><span class="line">		public DataType <span class="function"><span class="title">getConsumedDataType</span></span>() &#123;</span><br><span class="line">			<span class="built_in">return</span> tableSchema.toRowDataType();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// 已过时</span><br><span class="line">		@Override</span><br><span class="line">		public TableSink&lt;Row&gt; configure(String[] fieldNames, TypeInformation&lt;?&gt;[] fieldTypes) &#123;</span><br><span class="line">			<span class="built_in">return</span> null;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// 已过时</span><br><span class="line">		@Override</span><br><span class="line">		public void emitDataStream(DataStream&lt;Row&gt; dataStream) &#123;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		@Override</span><br><span class="line">		public DataStreamSink&lt;Row&gt; consumeDataStream(DataStream&lt;Row&gt; dataStream) &#123;</span><br><span class="line">			<span class="built_in">return</span> dataStream.addSink(new SinkFunction());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		private static class SinkFunction extends RichSinkFunction&lt;Row&gt; &#123;</span><br><span class="line">			public <span class="function"><span class="title">SinkFunction</span></span>() &#123;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			@Override</span><br><span class="line">			public void invoke(Row value, Context context) throws Exception &#123;</span><br><span class="line">				System.out.println(value);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">user_1,2016-01-01 10:02:00,browse,product_5,20,1451613720000</span><br><span class="line">user_1,2016-01-01 10:02:16,browse,product_5,20,1451613736000</span><br><span class="line">user_1,2016-01-01 10:02:15,browse,product_5,20,1451613735000</span><br><span class="line">user_1,2016-01-01 10:02:02,browse,product_5,20,1451613722000</span><br><span class="line">user_1,2016-01-01 10:02:06,browse,product_5,20,1451613726000</span><br><span class="line">user_1,2016-01-01 10:03:16,browse,product_5,20,1451613796000</span><br></pre></td></tr></table></figure>

<h3 id="RetractStreamTableSink-示例"><a href="#RetractStreamTableSink-示例" class="headerlink" title="RetractStreamTableSink 示例"></a>RetractStreamTableSink 示例</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.shuai.test;</span><br><span class="line"></span><br><span class="line">import java.time.LocalDateTime;</span><br><span class="line">import java.time.OffsetDateTime;</span><br><span class="line">import java.time.ZoneOffset;</span><br><span class="line">import java.time.format.DateTimeFormatter;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line">import org.apache.flink.api.java.typeutils.RowTypeInfo;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSink;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line">import org.apache.flink.table.api.DataTypes;</span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.TableSchema;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">import org.apache.flink.table.sinks.TableSink;</span><br><span class="line">import org.apache.flink.table.types.DataType;</span><br><span class="line">import org.apache.flink.types.Row;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON;</span><br><span class="line">import com.shuai.test.model.UserBrowseLog;</span><br><span class="line"></span><br><span class="line">public class RetractStreamTableSink &#123;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) throws Exception &#123;</span><br><span class="line">		// 设置运行环境</span><br><span class="line">		EnvironmentSettings environmentSettings = EnvironmentSettings.newInstance().useBlinkPlanner().build();</span><br><span class="line"></span><br><span class="line">		StreamExecutionEnvironment streamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">		StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(streamExecutionEnvironment,</span><br><span class="line">				environmentSettings);</span><br><span class="line"></span><br><span class="line">		Properties props = new Properties();</span><br><span class="line">		props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">		props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">		FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(<span class="string">&quot;demo&quot;</span>, new SimpleStringSchema(), props);</span><br><span class="line"></span><br><span class="line">		DataStream&lt;UserBrowseLog&gt; browseStream = streamExecutionEnvironment.addSource(consumer)</span><br><span class="line">				.process(new BrowseKafkaProcessFunction()).setParallelism(1);</span><br><span class="line"></span><br><span class="line">		tableEnvironment.registerDataStream(<span class="string">&quot;source_kafka_browse_log&quot;</span>, browseStream,</span><br><span class="line">				<span class="string">&quot;userID,eventTime,eventType,productID,productPrice,eventTimeTimestamp&quot;</span>);</span><br><span class="line"></span><br><span class="line">		// 4、注册AppendStreamTableSink</span><br><span class="line">		String[] sinkFieldNames = &#123; <span class="string">&quot;userID&quot;</span>, <span class="string">&quot;browseNumber&quot;</span> &#125;;</span><br><span class="line">		DataType[] sinkFieldTypes = &#123; DataTypes.STRING(), DataTypes.BIGINT() &#125;;</span><br><span class="line">		org.apache.flink.table.sinks.RetractStreamTableSink&lt;Row&gt; retractStreamTableSink = new MyRetractStreamTableSink(</span><br><span class="line">				sinkFieldNames, sinkFieldTypes);</span><br><span class="line">		tableEnvironment.registerTableSink(<span class="string">&quot;sink_stdout&quot;</span>,retractStreamTableSink);</span><br><span class="line">		</span><br><span class="line">        //5、连续查询</span><br><span class="line">        //统计每个Uid的浏览次数</span><br><span class="line">        String sql=<span class="string">&quot;insert into sink_stdout select userID,count(1) as browseNumber from source_kafka_browse_log where userID in (&#x27;user_1&#x27;,&#x27;user_2&#x27;) group by userID &quot;</span>;</span><br><span class="line">        tableEnvironment.sqlUpdate(sql);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //6、开始执行</span><br><span class="line">        tableEnvironment.execute(RetractStreamTableSink.class.getSimpleName());</span><br><span class="line"></span><br><span class="line">		</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static class BrowseKafkaProcessFunction extends ProcessFunction&lt;String, UserBrowseLog&gt; &#123;</span><br><span class="line"></span><br><span class="line">		@Override</span><br><span class="line">		public void processElement(String value, ProcessFunction&lt;String, UserBrowseLog&gt;.Context ctx,</span><br><span class="line">				Collector&lt;UserBrowseLog&gt; out) throws Exception &#123;</span><br><span class="line">			UserBrowseLog browseLog = JSON.parseObject(value, UserBrowseLog.class);</span><br><span class="line">			// 增加一个long类型的时间戳</span><br><span class="line">			// 指定eventTime为yyyy-MM-dd HH:mm:ss格式的北京时间</span><br><span class="line">			java.time.format.DateTimeFormatter format = DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">			OffsetDateTime eventTime = LocalDateTime.parse(browseLog.getEventTime(), format)</span><br><span class="line">					.atOffset(ZoneOffset.of(<span class="string">&quot;+08:00&quot;</span>));</span><br><span class="line">			// 转换成毫秒时间戳</span><br><span class="line">			long eventTimeTimestamp = eventTime.toInstant().toEpochMilli();</span><br><span class="line">			browseLog.setEventTimeTimestamp(eventTimeTimestamp);</span><br><span class="line"></span><br><span class="line">			out.collect(browseLog);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/**</span><br><span class="line">	 * 自定义RetractStreamTableSink</span><br><span class="line">	 *</span><br><span class="line">	 * Table在内部被转换成具有Add(增加)和Retract(撤消/删除)的消息流，最终交由DataStream的SinkFunction处理。</span><br><span class="line">	 * DataStream里的数据格式是Tuple2类型,如Tuple2&lt;Boolean, Row&gt;。</span><br><span class="line">	 * Boolean是Add(增加)或Retract(删除)的flag(标识)。Row是真正的数据类型。</span><br><span class="line">	 * Table中的Insert被编码成一条Add消息。如Tuple2&lt;True, Row&gt;。</span><br><span class="line">	 * Table中的Update被编码成两条消息。一条删除消息Tuple2&lt;False, Row&gt;，一条增加消息Tuple2&lt;True, Row&gt;。</span><br><span class="line">	 */</span><br><span class="line">	private static class MyRetractStreamTableSink implements org.apache.flink.table.sinks.RetractStreamTableSink&lt;Row&gt; &#123;</span><br><span class="line"></span><br><span class="line">		private TableSchema tableSchema;</span><br><span class="line"></span><br><span class="line">		public MyRetractStreamTableSink(String[] fieldNames, DataType[] fieldTypes) &#123;</span><br><span class="line">			this.tableSchema = TableSchema.builder().fields(fieldNames, fieldTypes).build();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		public TableSchema <span class="function"><span class="title">getTableSchema</span></span>() &#123;</span><br><span class="line">			<span class="built_in">return</span> tableSchema;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// 已过时</span><br><span class="line">		public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; configure(String[] fieldNames, TypeInformation&lt;?&gt;[] fieldTypes) &#123;</span><br><span class="line">			<span class="built_in">return</span> null;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// 已过时</span><br><span class="line">		public void emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; dataStream) &#123;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// 最终会转换成DataStream处理</span><br><span class="line">		public DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; dataStream) &#123;</span><br><span class="line">			<span class="built_in">return</span> dataStream.addSink(new SinkFunction());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		public TypeInformation&lt;Row&gt; <span class="function"><span class="title">getRecordType</span></span>() &#123;</span><br><span class="line">			<span class="built_in">return</span> new RowTypeInfo(tableSchema.getFieldTypes(), tableSchema.getFieldNames());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		private static class SinkFunction extends RichSinkFunction&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#123;</span><br><span class="line">			public <span class="function"><span class="title">SinkFunction</span></span>() &#123;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			@Override</span><br><span class="line">			public void invoke(Tuple2&lt;Boolean, Row&gt; value, Context context) throws Exception &#123;</span><br><span class="line">				Boolean flag = value.f0;</span><br><span class="line">				<span class="keyword">if</span> (flag) &#123;</span><br><span class="line">					System.out.println(<span class="string">&quot;增加... &quot;</span> + value);</span><br><span class="line">				&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">					System.out.println(<span class="string">&quot;删除... &quot;</span> + value);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">增加... (<span class="literal">true</span>,user_2,1)</span><br><span class="line">删除... (<span class="literal">false</span>,user_2,1)</span><br><span class="line">增加... (<span class="literal">true</span>,user_2,2)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,1)</span><br><span class="line">删除... (<span class="literal">false</span>,user_1,1)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,2)</span><br><span class="line">删除... (<span class="literal">false</span>,user_1,2)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,3)</span><br><span class="line">删除... (<span class="literal">false</span>,user_1,3)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,4)</span><br><span class="line">删除... (<span class="literal">false</span>,user_1,4)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,5)</span><br><span class="line">删除... (<span class="literal">false</span>,user_1,5)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,6)</span><br></pre></td></tr></table></figure>

<h3 id="UpsertStreamTableSink示例"><a href="#UpsertStreamTableSink示例" class="headerlink" title="UpsertStreamTableSink示例"></a>UpsertStreamTableSink示例</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.shuai.test;</span><br><span class="line"></span><br><span class="line">import java.time.LocalDateTime;</span><br><span class="line">import java.time.OffsetDateTime;</span><br><span class="line">import java.time.ZoneOffset;</span><br><span class="line">import java.time.format.DateTimeFormatter;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line">import org.apache.flink.api.java.typeutils.RowTypeInfo;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSink;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line">import org.apache.flink.table.api.DataTypes;</span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.TableSchema;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">import org.apache.flink.table.sinks.TableSink;</span><br><span class="line">import org.apache.flink.table.types.DataType;</span><br><span class="line">import org.apache.flink.types.Row;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON;</span><br><span class="line">import com.shuai.test.RetractStreamTableSink.BrowseKafkaProcessFunction;</span><br><span class="line">import com.shuai.test.model.UserBrowseLog;</span><br><span class="line"></span><br><span class="line">public class UpsertStreamTableSink &#123;</span><br><span class="line">	public static void main(String[] args) throws Exception &#123;</span><br><span class="line">		// 设置运行环境</span><br><span class="line">		EnvironmentSettings environmentSettings = EnvironmentSettings.newInstance().useBlinkPlanner().build();</span><br><span class="line"></span><br><span class="line">		StreamExecutionEnvironment streamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">		StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(streamExecutionEnvironment,</span><br><span class="line">				environmentSettings);</span><br><span class="line"></span><br><span class="line">		Properties props = new Properties();</span><br><span class="line">		props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">		props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">		FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(<span class="string">&quot;demo&quot;</span>, new SimpleStringSchema(), props);</span><br><span class="line"></span><br><span class="line">		DataStream&lt;UserBrowseLog&gt; browseStream = streamExecutionEnvironment.addSource(consumer)</span><br><span class="line">				.process(new BrowseKafkaProcessFunction()).setParallelism(1);</span><br><span class="line"></span><br><span class="line">		tableEnvironment.registerDataStream(<span class="string">&quot;source_kafka_browse_log&quot;</span>, browseStream,</span><br><span class="line">				<span class="string">&quot;userID,eventTime,eventType,productID,productPrice,eventTimeTimestamp&quot;</span>);</span><br><span class="line"></span><br><span class="line">		// 4、注册AppendStreamTableSink</span><br><span class="line">		String[] sinkFieldNames = &#123; <span class="string">&quot;userID&quot;</span>, <span class="string">&quot;browseNumber&quot;</span> &#125;;</span><br><span class="line">		DataType[] sinkFieldTypes = &#123; DataTypes.STRING(), DataTypes.BIGINT() &#125;;</span><br><span class="line">		org.apache.flink.table.sinks.UpsertStreamTableSink&lt;Row&gt; retractStreamTableSink = new MyUpsertStreamTableSink(</span><br><span class="line">				sinkFieldNames, sinkFieldTypes);</span><br><span class="line">		tableEnvironment.registerTableSink(<span class="string">&quot;sink_stdout&quot;</span>,retractStreamTableSink);</span><br><span class="line">		</span><br><span class="line">        //5、连续查询</span><br><span class="line">        //统计每个Uid的浏览次数</span><br><span class="line">        String sql=<span class="string">&quot;insert into sink_stdout select userID,count(1) as browseNumber from source_kafka_browse_log where userID in (&#x27;user_1&#x27;,&#x27;user_2&#x27;) group by userID &quot;</span>;</span><br><span class="line">        tableEnvironment.sqlUpdate(sql);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //6、开始执行</span><br><span class="line">        tableEnvironment.execute(RetractStreamTableSink.class.getSimpleName());</span><br><span class="line"></span><br><span class="line">		</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static class BrowseKafkaProcessFunction extends ProcessFunction&lt;String, UserBrowseLog&gt; &#123;</span><br><span class="line"></span><br><span class="line">		@Override</span><br><span class="line">		public void processElement(String value, ProcessFunction&lt;String, UserBrowseLog&gt;.Context ctx,</span><br><span class="line">				Collector&lt;UserBrowseLog&gt; out) throws Exception &#123;</span><br><span class="line">			UserBrowseLog browseLog = JSON.parseObject(value, UserBrowseLog.class);</span><br><span class="line">			// 增加一个long类型的时间戳</span><br><span class="line">			// 指定eventTime为yyyy-MM-dd HH:mm:ss格式的北京时间</span><br><span class="line">			java.time.format.DateTimeFormatter format = DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">			OffsetDateTime eventTime = LocalDateTime.parse(browseLog.getEventTime(), format)</span><br><span class="line">					.atOffset(ZoneOffset.of(<span class="string">&quot;+08:00&quot;</span>));</span><br><span class="line">			// 转换成毫秒时间戳</span><br><span class="line">			long eventTimeTimestamp = eventTime.toInstant().toEpochMilli();</span><br><span class="line">			browseLog.setEventTimeTimestamp(eventTimeTimestamp);</span><br><span class="line"></span><br><span class="line">			out.collect(browseLog);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	/**</span><br><span class="line">     * 自定义UpsertStreamTableSink</span><br><span class="line">     * Table在内部被转换成具有Add(增加)和Retract(撤消/删除)的消息流，最终交由DataStream的SinkFunction处理。</span><br><span class="line">     * Boolean是Add(增加)或Retract(删除)的flag(标识)。Row是真正的数据类型。</span><br><span class="line">     * Table中的Insert被编码成一条Add消息。如Tuple2&lt;True, Row&gt;。</span><br><span class="line">     * Table中的Update被编码成一条Add消息。如Tuple2&lt;True, Row&gt;。</span><br><span class="line">     * 在SortLimit(即order by ... <span class="built_in">limit</span> ...)的场景下，被编码成两条消息。一条删除消息Tuple2&lt;False, Row&gt;，一条增加消息Tuple2&lt;True, Row&gt;。</span><br><span class="line">     */</span><br><span class="line">    @SuppressWarnings(<span class="string">&quot;unused&quot;</span>)</span><br><span class="line">	private static class MyUpsertStreamTableSink implements org.apache.flink.table.sinks.UpsertStreamTableSink&lt;Row&gt; &#123;</span><br><span class="line"></span><br><span class="line">        private TableSchema tableSchema;</span><br><span class="line"></span><br><span class="line">        @SuppressWarnings(<span class="string">&quot;unused&quot;</span>)</span><br><span class="line">		public MyUpsertStreamTableSink(String[] fieldNames, DataType[] fieldTypes) &#123;</span><br><span class="line">            this.tableSchema = TableSchema.builder().fields(fieldNames,fieldTypes).build();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public TableSchema <span class="function"><span class="title">getTableSchema</span></span>() &#123;</span><br><span class="line">            <span class="built_in">return</span> tableSchema;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 设置Unique Key</span><br><span class="line">        // 如上SQL中有GroupBy，则这里的唯一键会自动被推导为GroupBy的字段</span><br><span class="line">        @Override</span><br><span class="line">        public void setKeyFields(String[] keys) &#123;&#125;</span><br><span class="line"></span><br><span class="line">        // 是否只有Insert</span><br><span class="line">        // 如上SQL场景，需要Update，则这里被推导为isAppendOnly=<span class="literal">false</span></span><br><span class="line">        @Override</span><br><span class="line">        public void setIsAppendOnly(Boolean isAppendOnly) &#123;&#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public TypeInformation&lt;Row&gt; <span class="function"><span class="title">getRecordType</span></span>() &#123;</span><br><span class="line">            <span class="built_in">return</span> new RowTypeInfo(tableSchema.getFieldTypes(),tableSchema.getFieldNames());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 已过时</span><br><span class="line">        public void emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; dataStream) &#123;&#125;</span><br><span class="line"></span><br><span class="line">        // 最终会转换成DataStream处理</span><br><span class="line"> </span><br><span class="line">        public DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; dataStream) &#123;</span><br><span class="line">            <span class="built_in">return</span> dataStream.addSink(new SinkFunction());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">        public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; configure(String[] fieldNames, TypeInformation&lt;?&gt;[] fieldTypes) &#123;</span><br><span class="line">            <span class="built_in">return</span> null;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        private static class SinkFunction extends RichSinkFunction&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#123;</span><br><span class="line">            /**</span><br><span class="line">			 * </span><br><span class="line">			 */</span><br><span class="line">			private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">			public <span class="function"><span class="title">SinkFunction</span></span>() &#123;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public void invoke(Tuple2&lt;Boolean, Row&gt; value, Context context) throws Exception &#123;</span><br><span class="line">                Boolean flag = value.f0;</span><br><span class="line">                <span class="keyword">if</span>(flag)&#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;增加... &quot;</span>+value);</span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;删除... &quot;</span>+value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">增加... (<span class="literal">true</span>,user_1,1)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,2)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,3)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,4)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,5)</span><br><span class="line">增加... (<span class="literal">true</span>,user_1,6)</span><br><span class="line">增加... (<span class="literal">true</span>,user_2,1)</span><br><span class="line">增加... (<span class="literal">true</span>,user_2,2)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka to Flink - HDFS</title>
    <url>/2020/04/27/22/</url>
    <content><![CDATA[<h3 id="kafka制造数据"><a href="#kafka制造数据" class="headerlink" title="kafka制造数据"></a>kafka制造数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.flink.etl;</span><br><span class="line"></span><br><span class="line">import lombok.AllArgsConstructor;</span><br><span class="line">import lombok.ToString;</span><br><span class="line"></span><br><span class="line">@lombok.Data</span><br><span class="line">@ToString</span><br><span class="line"></span><br><span class="line">public class Data &#123;</span><br><span class="line">	</span><br><span class="line">	public long <span class="function"><span class="title">getTimestamp</span></span>() &#123;</span><br><span class="line">		<span class="built_in">return</span> timestamp;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public void setTimestamp(long timestamp) &#123;</span><br><span class="line">		this.timestamp = timestamp;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public String <span class="function"><span class="title">getEvent</span></span>() &#123;</span><br><span class="line">		<span class="built_in">return</span> event;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public void setEvent(String event) &#123;</span><br><span class="line">		this.event = event;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public String <span class="function"><span class="title">getUuid</span></span>() &#123;</span><br><span class="line">		<span class="built_in">return</span> uuid;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public void setUuid(String uuid) &#123;</span><br><span class="line">		this.uuid = uuid;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	private long timestamp;</span><br><span class="line">	private String event;</span><br><span class="line">	private String uuid;</span><br><span class="line"> </span><br><span class="line">	public Data(long timestamp, String event, String uuid) &#123;</span><br><span class="line">		</span><br><span class="line">		this.timestamp=timestamp;</span><br><span class="line">		this.event=event;</span><br><span class="line">		this.uuid=uuid;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.flink.etl;</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.Random;</span><br><span class="line">import java.util.UUID;</span><br><span class="line"></span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class DataGenerator &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.3.122:9092&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;key.serializer&quot;</span>, StringSerializer.class.getName());</span><br><span class="line">        properties.put(<span class="string">&quot;value.serializer&quot;</span>, StringSerializer.class.getName());</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line">        List&lt;String&gt; events = Arrays.asList(<span class="string">&quot;page_view&quot;</span>, <span class="string">&quot;adv_click&quot;</span>, <span class="string">&quot;thumbs_up&quot;</span>);</span><br><span class="line">        Random random = new Random();</span><br><span class="line">        Data data = null;</span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = null;</span><br><span class="line">        try &#123;</span><br><span class="line">            <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">                long timestamp = System.currentTimeMillis();</span><br><span class="line">                String event = events.get(random.nextInt(events.size()));</span><br><span class="line">                String uuid = UUID.randomUUID().toString();</span><br><span class="line">                data = new Data(timestamp, event, uuid);</span><br><span class="line">                record = new ProducerRecord&lt;&gt;(<span class="string">&quot;data-collection-topic&quot;</span>, JSON.toJSONString(data));</span><br><span class="line">                producer.send(record);</span><br><span class="line">                Thread.sleep(3000);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            producer.flush();</span><br><span class="line">            producer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="StreamingJob"><a href="#StreamingJob" class="headerlink" title="StreamingJob"></a>StreamingJob</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.flink.etl;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringEncoder;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.core.fs.Path;</span><br><span class="line">import org.apache.flink.runtime.state.StateBackend;</span><br><span class="line">import org.apache.flink.runtime.state.filesystem.FsStateBackend;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.CheckpointConfig;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line">public class StreamingJob &#123;</span><br><span class="line">	public static void main(String[] args) throws Exception &#123;</span><br><span class="line">		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">		Properties props = new Properties();</span><br><span class="line">		props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.3.122:9092&quot;</span>);</span><br><span class="line">		props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">		FlinkKafkaConsumer010&lt;String&gt; consumer = new FlinkKafkaConsumer010&lt;&gt;(</span><br><span class="line">				<span class="string">&quot;data-collection-topic&quot;</span>, new SimpleStringSchema(), props);</span><br><span class="line">		DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br><span class="line">		DefaultRollingPolicy rollingPolicy = DefaultRollingPolicy</span><br><span class="line">				.create()</span><br><span class="line">				.withMaxPartSize(1024*1024*128) // 设置每个文件的最大大小 ,默认是128M。这里设置为128M</span><br><span class="line">				.withRolloverInterval(TimeUnit.MINUTES.toMillis(86400)) // 滚动写入新文件的时间，默认60s。这里设置为无限大</span><br><span class="line">				.withInactivityInterval(TimeUnit.MINUTES.toMillis(60)) // 60s空闲，就滚动写入新的文件</span><br><span class="line">				.build();</span><br><span class="line">		StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">				.forRowFormat(new Path(<span class="string">&quot;hdfs://DATASEA/home/dmp_operator1/bpointdata&quot;</span>), new SimpleStringEncoder&lt;String&gt;())</span><br><span class="line">				.withBucketAssigner(new EventTimeBucketAssigner()).withRollingPolicy(rollingPolicy).build();</span><br><span class="line">		stream.addSink(sink);</span><br><span class="line">		env.enableCheckpointing(10_000);</span><br><span class="line">		env.setParallelism(1);</span><br><span class="line">		env.setStateBackend((StateBackend) new FsStateBackend(<span class="string">&quot;hdfs://DATASEA/home/dmp_operator1/flink/checkpoints&quot;</span>));</span><br><span class="line">		env.getCheckpointConfig().enableExternalizedCheckpoints(</span><br><span class="line">		CheckpointConfig.ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br><span class="line">		// execute program</span><br><span class="line">		env.execute(<span class="string">&quot;Flink Streaming Java API Skeleton&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="EventTimeBucketAssigner"><a href="#EventTimeBucketAssigner" class="headerlink" title="EventTimeBucketAssigner"></a>EventTimeBucketAssigner</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.flink.etl;</span><br><span class="line"></span><br><span class="line">import org.apache.flink.core.io.SimpleVersionedSerializer;</span><br><span class="line">import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;</span><br><span class="line">import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.text.SimpleDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line"></span><br><span class="line">public class EventTimeBucketAssigner implements BucketAssigner&lt;String, String&gt; &#123;</span><br><span class="line">	private ObjectMapper mapper = new ObjectMapper();</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public String getBucketId(String element, Context context) &#123;</span><br><span class="line">		JsonNode node = null;</span><br><span class="line">		long date = System.currentTimeMillis();</span><br><span class="line">		try &#123;</span><br><span class="line">			node = mapper.readTree(element);</span><br><span class="line">//			date = (long) (node.path(<span class="string">&quot;timestamp&quot;</span>).floatValue() * 1000);</span><br><span class="line">			date = (long) (node.path(<span class="string">&quot;timestamp&quot;</span>).floatValue());</span><br><span class="line">		&#125; catch (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">//		String partitionValue = new SimpleDateFormat(<span class="string">&quot;yyyyMMddHHmm&quot;</span>).format(new Date(date)); </span><br><span class="line">		String partitionValue = new SimpleDateFormat(<span class="string">&quot;yyyyMMdd&quot;</span>).format(new Date(date));</span><br><span class="line">		<span class="built_in">return</span> <span class="string">&quot;dt=&quot;</span> + partitionValue;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public SimpleVersionedSerializer&lt;String&gt; <span class="function"><span class="title">getSerializer</span></span>() &#123;</span><br><span class="line">		<span class="built_in">return</span> SimpleVersionedStringSerializer.INSTANCE;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Druid原理和架构</title>
    <url>/2020/04/20/21/</url>
    <content><![CDATA[<h3 id="Druid简介"><a href="#Druid简介" class="headerlink" title="Druid简介"></a>Druid简介</h3><p>概念：主要是解决低延迟下实时数据摄入与查询的平台，本质是一个数据存储，但是数据仍然是保存在（hdfs、文件系统等）中。<br>特点：<br>① 列式存储格式：<br>可以将列作为索引，为仅查看几列的查询提供了巨大的速度提升<br>② 高可用、高并发：<br>① 集群扩展、缩小、删除、宕机都不会停止服务，全天候运行<br>② HA、sql的并行化执行、可扩展、容灾等<br>③ 支持1000+的并发用户，并提供隔离机制支持多租户模式（多租户就是并发互不影响）<br>④ 低延迟<br>Druid采用了列式存储、倒排索引、位图索引等关键技术，能够在亚秒级别内完成海量数据的过滤、聚合以及多维分析等操作。<br>⑤ 存储时候聚合：<br>无论是实时数据消费还是批量数据处理，Druid在基于DataSource结构存储数据时即可选择对任意的指标列进行聚合操作<br>聚合：提前做好sum，count等操作</p>
<h3 id="Druid架构"><a href="#Druid架构" class="headerlink" title="Druid架构"></a>Druid架构</h3><p><img src="/images/26.png" alt="alt"><br>总体可以分为：四个节点+三个依赖</p>
<h3 id="四个节点："><a href="#四个节点：" class="headerlink" title="四个节点："></a>四个节点：</h3><h4 id="实时节点（RealtimeNode）-新版本的druid好像没有实时节点的说法了-："><a href="#实时节点（RealtimeNode）-新版本的druid好像没有实时节点的说法了-：" class="headerlink" title="实时节点（RealtimeNode）(新版本的druid好像没有实时节点的说法了)："></a>实时节点（RealtimeNode）(新版本的druid好像没有实时节点的说法了)：</h4><p>实时摄入数据，对于旧的数据周期性的生成segment数据文件，上传到deepstorage中<br>为了避免单点故障，索引服务(Indexer)的主从架构已经逐渐替代了实时节点，所以现在的实时节点，其实里面包含了很多角色:<br>作用：可以通过索引服务的API，写数据导入任务，用以新增、删除、合并Segment等。是一个主从架构：</p>
<h5 id="统治节点（overlord）："><a href="#统治节点（overlord）：" class="headerlink" title="统治节点（overlord）："></a>统治节点（overlord）：</h5><p>类似于YarnResourceManager：负责集群资源的管理和分配<br>监视数据服务器上的MiddleManager进程，将提取任务分配给MiddleManager</p>
<h5 id="中间管理者（middlemanager）："><a href="#中间管理者（middlemanager）：" class="headerlink" title="中间管理者（middlemanager）："></a>中间管理者（middlemanager）：</h5><p>类似于YarnNodeManager：负责单个节点资源的管理和分配<br>新数据提取到群集中的过程。他们负责从外部数据源读取并发布新的段</p>
<h5 id="苦工（peon）："><a href="#苦工（peon）：" class="headerlink" title="苦工（peon）："></a>苦工（peon）：</h5><p>类似于Yarncontainer：负责具体任务的执行<br>Peon进程是由MiddleManagers产生的任务执行引擎。<br>每个Peon运行一个单独的JVM，并负责执行单个任务。<br>Peon总是与生成它们的MiddleManager在同一主机上运行</p>
<h5 id="Router-路由：可选-："><a href="#Router-路由：可选-：" class="headerlink" title="Router(路由：可选)："></a>Router(路由：可选)：</h5><p>可在Druid代理，统治节点和协调器之前提供统一的API网关<br> 注：统治节点和中间管理者的通信是通过zookeeper完成的</p>
<h4 id="历史节点（HistoricalNode）："><a href="#历史节点（HistoricalNode）：" class="headerlink" title="历史节点（HistoricalNode）："></a>历史节点（HistoricalNode）：</h4><p>加载已生成的segment数据文件，以供数据查询<br>启动或者受到协调节点通知的时候，通过druid_rules表去查找需要加载的数据，然后检查自身的本地缓存中已存在的Segment数据文件，<br>然后从DeepStorage中下载其他不在本地的Segment数据文件，后加载到内存！！！再提供查询。</p>
<h4 id="查询节点（BrokerNode）"><a href="#查询节点（BrokerNode）" class="headerlink" title="查询节点（BrokerNode）:"></a>查询节点（BrokerNode）:</h4><p>对外提供数据查询服务，并同时从实时节点与历史节点查询数据，合并后返回给调用方<br>缓存：外部：第三方的一些缓存系统内部：在历史节点或者查询节点做缓存</p>
<h4 id="协调节点（CoodinatorNode）"><a href="#协调节点（CoodinatorNode）" class="headerlink" title="协调节点（CoodinatorNode）:"></a>协调节点（CoodinatorNode）:</h4><p>负责历史节点的数据负载均衡，以及通过规则（Rule）管理数据的生命周期<br>① 通过从MySQL读取元数据信息，来决定深度存储上哪些数据段应该在那个历史节点中被加载，<br>② 通过ZK感知历史节点，历史节点增加，会自动分配相关的Segment，历史节点删除，会将原本在这台节点上的Segment分配给其他的历史节点<br>注：Coordinator是定期运行的，并且运行间隔可以通过配置参数配置</p>
<h3 id="三个依赖："><a href="#三个依赖：" class="headerlink" title="三个依赖："></a>三个依赖：</h3><h5 id="1）Mysql："><a href="#1）Mysql：" class="headerlink" title="1）Mysql："></a>1）Mysql：</h5><p>存储关于Druid中的metadata，规则数据，配置数据等，<br>主要包含以下几张表：<br>“druid_config”（通常是空的）,<br>“druid_rules”（协作节点使用的一些规则信息，比如哪个segment从哪个node去load）<br>“druid_segments”（存储每个segment的metadata信息）;</p>
<h5 id="2）Deepstorage："><a href="#2）Deepstorage：" class="headerlink" title="2）Deepstorage："></a>2）Deepstorage：</h5><p>存储segments，Druid目前已经支持本地磁盘，NFS挂载磁盘，HDFS，S3等。</p>
<h5 id="3）ZooKeeper："><a href="#3）ZooKeeper：" class="headerlink" title="3）ZooKeeper："></a>3）ZooKeeper：</h5><p>① 查询节点通过Zk来感知实时节点和历史节点的存在，提供查询服务。<br>② 协调节点通过ZK感知历史节点，实现负载均衡<br>③ 统治节点、协调节点的lead选举</p>
<h3 id="实时Segment数据文件的流动："><a href="#实时Segment数据文件的流动：" class="headerlink" title="实时Segment数据文件的流动："></a>实时Segment数据文件的流动：</h3><h4 id="生成"><a href="#生成" class="headerlink" title="生成:"></a>生成:</h4><p>① 实时节点（中间管理者）会周期性的将同一时间段生成的数据合并成一个Segment数据文件，并上传到DeepStorage中。<br>② Segment数据文件的相关元数据信息保存到MetaStore中（如mysql，derby等）。<br>③ 协调节点定时（默认1分钟）从MetaSotre中获取到Segment数据文件的相关元信息后，将按配置的规则分配到符合条件的历史节点中。<br>④ 协调节点会通知一个历史节点去读<br>⑤ 历史节点收到协调节点的通知后，会从DeepStorage中拉取该Segment数据文件到本地磁盘，并通过zookeeper向集群声明可以提供查询了。<br>⑥ 实时节点会丢弃该Segment数据文件，并通过zookeeper向集群声明不在提供该Sgment的查询服务。　　　　　　　　　　　　　　//其实第四步已经可以提供查询服务了<br>⑦ 而对于全局数据来说，查询节点（BrokerNode）会同时从实时节点与历史节点分别查询，对结果整合后返回用户。</p>
<h4 id="查询："><a href="#查询：" class="headerlink" title="查询："></a>查询：</h4><p>查询首先进入Broker，按照时间进行查询划分<br>确定哪些历史记录和MiddleManager正在为这些段提供服务<br>Historical/MiddleManager进程将接受查询，对其进行处理并返回结果<br>###DataSource<br><img src="/images/27.png" alt="alt"><br>每个datasource按照时间划分。每个时间范围称为一个chunk(一般都是以天分区，则一个chunk为一天)！！！//也可以按其他属性划分<br>在chunk中数据被分为一个或多个segment，每个segment都是一个单独的文件，通常包含几百万行数据<br>注：这些segment是按照时间组织成的chunk，所以在按照时间查询数据时，效率非常高。</p>
<h4 id="数据分区："><a href="#数据分区：" class="headerlink" title="数据分区："></a>数据分区：</h4><p>任何分布式存储/计算系统，都需要对数据进行合理的分区，从而实现存储和计算的均衡，以及数据并行化。<br>而Druid本身处理的是事件数据，每条数据都会带有一个时间戳，所以很自然的就可以使用时间进行分区。<br>为什么一个chunk中的数据包含多个segment！！！？？？？原因就是二级分区</p>
<h4 id="二级分区："><a href="#二级分区：" class="headerlink" title="二级分区："></a>二级分区：</h4><p>很可能每个chunk的数据量是不均衡的，而Duid为了解决这种问题，提供了“二级分区”，每一个二级分区称为一个Shard(分片)<br>其实chunk、datasource都是抽象的，实际的就是每个分区就是一个Shard，每个Shard只包含一个Segment！！！，因为Segment是Shard持久化的结果<br>Druid目前支持两种Shard策略：<br>Hash(基于维值的Hash)<br>Range(基于某个维度的取值范围)<br>譬如：<br>2000-01-01，2000-01-02中的每一个分区都是一个Shard<br>2000-01-02的数据量比较多，所以有两个Shard，分为partition0、partition1。每个分区都是一个Shard<br>Shard经过持久化之后就称为了Segment，Segment是数据存储、复制、均衡(Historical的负载均衡)和计算的基本单元了。<br>Segment具有不可变性，一个Segment一旦创建完成后(MiddleManager节点发布后)就无法被修改，<br>只能通过生成一个新的Segment来代替旧版本的Segment。</p>
<h4 id="Segment内部存储结构："><a href="#Segment内部存储结构：" class="headerlink" title="Segment内部存储结构："></a>Segment内部存储结构：</h4><p>Segment内部采用列式存储    　　　　　　　　    //并不是说每列都是一个独立的文件，而是说每列有独立的数据结构，所有列都会存储在一个文件中<br>Segment中的数据类型主要分为三种：<br>时间戳<br>维度列<br>指标列<br>对于时间戳列和指标列，实际存储是一个数组<br>对于维度列不会像指标列和时间戳这么简单，因为它需要支持filter和groupby：<br>所以Druid使用了字典编码(DictionaryEncoding)和位图索引(BitmapIndex)来存储每个维度列。每个维度列需要三个数据结构：<br>1、需要一个字典数据结构，将维值(维度列值都会被认为是字符串类型)映射成一个整数ID。<br>2、使用上面的字典编码，将该列所有维值放在一个列表中。<br>3、对于列中不同的值，使用bitmap数据结构标识哪些行包含这些值。　　　　　　//位图索引，这个需要记住<br>注：使用Bitmap位图索引可以执行快速过滤操作(找到符合条件的行号，以减少读取的数据量)<br>Druid针对维度列之所以使用这三个数据结构，是因为：<br>使用字典将字符串映射成整数ID，可以紧凑的表示结构2和结构3中的值。<br>使用Bitmap位图索引可以执行快速过滤操作(找到符合条件的行号，以减少读取的数据量)，因为Bitmap可以快速执行AND和OR操作。<br>对于groupby和TopN操作需要使用结构2中的列值列表<br>实例：<br>1.使用字典将列值映射为整数<br>{<br> “JustinBieher”:0,<br> “ke$ha”:1<br>}<br>2.使用1中的编码，将列值放到一个列表中<br>[0,0,1,1]<br>3.使用bitmap来标识不同列值<br>value=0:[1,1,0,0]//1代表该行含有该值，0标识不含有<br>value=1:[0,0,1,1]<br>因为是一个稀疏矩阵，所以比较好压缩！！<br>Druid而且运用了RoaringBitmap能够对压缩后的位图直接进行布尔运算，可以大大提高查询效率和存储效率(不需要解压缩)</p>
<h4 id="Segment命名"><a href="#Segment命名" class="headerlink" title="Segment命名:"></a>Segment命名:</h4><p>如果一个Datasource下有几百万个Segment文件，我们又如何快速找出我们所需要的文件呢？答案就是通过文件名称快速索引查找。<br>Segment的命名包含四部分：<br>数据源(Datasource)、时间间隔(包含开始时间和结束时间两部分)、版本号和分区(Segment有分片的情况下才会有)。<br>eg：wikipedia_2015-09-12T00:00:00.000Z_2015-09-13T00:00:00.000Z_2019-09-09T10:06:02.498Z<br>wikipedia：Datasource名称<br>开始时间：2015-09-12T00:00:00.000Z//该Segment所存储最早的数据，时间格式是ISO8601<br>结束时间：2015-09-13T00:00:00.000Z//该segment所存储最晚的数据，时间格式是ISO8601<br>版本号：2019-09-09T10:06:02.498Z//此Segment的启动时间，因为Druid支持批量覆盖操作，<br>//当批量摄入与之前相同数据源、相同时间间隔数据时，数据就会被覆盖，这时候版本号就会被更新<br>分片号：从0开始，如果分区号为0，可以省略//分区的表现其实就是分目录<br>注：单机形式运行Druid，这样Druid生成的Segment文件都在${DRUID_HOME}/var/druid/segments目录下<br>注：为了保证Druid的查询效率，每个Segment文件的大小建议在300MB~700MB之间<br>注：版本号的意义：<br>在druid，如果您所做的只是追加数据，那么每个时间chunk只会有一个版本。<br>但是当您覆盖数据时，因为druid通过首先加载新数据（但不允许查询）来处理这个问题，一旦新数据全部加载，<br>切换所有新查询以使用这些新数据。然后它在几分钟后掉落旧段！！！</p>
<h4 id="存储聚合"><a href="#存储聚合" class="headerlink" title="存储聚合"></a>存储聚合</h4><p>无论是实时数据消费还是批量数据处理，Druid在基于DataSource机构存储数据时即可选择对任意的指标列进行聚合操作：<br>1、基于维度列：相同的维度列数据会进行聚合<br>2、基于时间段：某一时间段的所有行会进行聚合，时间段可以通过queryGranularity参数指定<br>聚合：提前做好sum，count等操作</p>
<h4 id="Segment生命周期："><a href="#Segment生命周期：" class="headerlink" title="Segment生命周期："></a>Segment生命周期：</h4><p>在元数据存储中！每个Segment都会有一个used字段，标记该段是否能用于查询</p>
<h5 id="is-Published："><a href="#is-Published：" class="headerlink" title="is_Published："></a>is_Published：</h5><p>当Segment构建完毕，就将元数据存储在元数据存储区中，此Segment为发布状态</p>
<h5 id="is-available"><a href="#is-available" class="headerlink" title="is_available:"></a>is_available:</h5><p>如果Segment当前可用于查询（实时任务或历史进程），则为true。</p>
<h5 id="is-realtime"><a href="#is-realtime" class="headerlink" title="is_realtime:"></a>is_realtime:</h5><p>如果是由实时任务产生的，那么会为true，但是一段时间之后，也会变为false</p>
<h5 id="is-overshadowed："><a href="#is-overshadowed：" class="headerlink" title="is_overshadowed："></a>is_overshadowed：</h5><p>标记该段是否已被其他段覆盖！处于此状态的段很快就会将其used标志自动设置为false。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 是如何保证数据可靠性和一致性</title>
    <url>/2020/04/07/20/</url>
    <content><![CDATA[<h3 id="数据可靠性"><a href="#数据可靠性" class="headerlink" title="数据可靠性"></a>数据可靠性</h3><p>Kafka 作为一个商业级消息中间件，消息可靠性的重要性可想而知。本文从 Producter 往 Broker 发送消息、Topic 分区副本以及 Leader 选举几个角度介绍数据的可靠性。</p>
<h3 id="Topic-分区副本"><a href="#Topic-分区副本" class="headerlink" title="Topic 分区副本"></a>Topic 分区副本</h3><p>在 Kafka 0.8.0 之前，Kafka 是没有副本的概念的，那时候人们只会用 Kafka 存储一些不重要的数据，因为没有副本，数据很可能会丢失。但是随着业务的发展，支持副本的功能越来越强烈，所以为了保证数据的可靠性，Kafka 从 0.8.0 版本开始引入了分区副本（详情请参见 KAFKA-50）。也就是说每个分区可以人为的配置几个副本（比如创建主题的时候指定 replication-factor，也可以在 Broker 级别进行配置 default.replication.factor），一般会设置为3。</p>
<p>Kafka 可以保证单个分区里的事件是有序的，分区可以在线（可用），也可以离线（不可用）。在众多的分区副本里面有一个副本是 Leader，其余的副本是 follower，所有的读写操作都是经过 Leader 进行的，同时 follower 会定期地去 leader 上的复制数据。当 Leader 挂了的时候，其中一个 follower 会重新成为新的 Leader。通过分区副本，引入了数据冗余，同时也提供了 Kafka 的数据可靠性。</p>
<p>Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。</p>
<h3 id="Producer-往-Broker-发送消息"><a href="#Producer-往-Broker-发送消息" class="headerlink" title="Producer 往 Broker 发送消息"></a>Producer 往 Broker 发送消息</h3><p>如果我们要往 Kafka 对应的主题发送消息，我们需要通过 Producer 完成。前面我们讲过 Kafka 主题对应了多个分区，每个分区下面又对应了多个副本；为了让用户设置数据可靠性， Kafka 在 Producer 里面提供了消息确认机制。也就是说我们可以通过配置来决定消息发送到对应分区的几个副本才算消息发送成功。可以在定义 Producer 时通过 acks 参数指定（在 0.8.2.X 版本之前是通过 request.required.acks 参数设置的，详见 KAFKA-3043）。这个参数支持以下三种值：</p>
<p>acks = 0：意味着如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入 Kafka 。在这种情况下还是有可能发生错误，比如发送的对象无能被序列化或者网卡发生故障，但如果是分区离线或整个集群长时间不可用，那就不会收到任何错误。在 acks=0 模式下的运行速度是非常快的（这就是为什么很多基准测试都是基于这个模式），你可以得到惊人的吞吐量和带宽利用率，不过如果选择了这种模式， 一定会丢失一些消息。</p>
<p>acks = 1：意味若 Leader 在收到消息并把它写入到分区数据文件（不一定同步到磁盘上）时会返回确认或错误响应。在这个模式下，如果发生正常的 Leader 选举，生产者会在选举时收到一个 LeaderNotAvailableException 异常，如果生产者能恰当地处理这个错误，它会重试发送悄息，最终消息会安全到达新的 Leader 那里。不过在这个模式下仍然有可能丢失数据，比如消息已经成功写入 Leader，但在消息被复制到 follower 副本之前 Leader发生崩溃。</p>
<p>acks = all（这个和 request.required.acks = -1 含义一样）：意味着 Leader 在返回确认或错误响应之前，会等待所有同步副本都收到悄息。如果和 min.insync.replicas 参数结合起来，就可以决定在返回确认前至少有多少个副本能够收到悄息，生产者会一直重试直到消息被成功提交。不过这也是最慢的做法，因为生产者在继续发送其他消息之前需要等待所有副本都收到当前的消息。</p>
<p>根据实际的应用场景，我们设置不同的 acks，以此保证数据的可靠性。<br>另外，Producer 发送消息还可以选择同步（默认，通过 producer.type=sync 配置） 或者异步（producer.type=async）模式。如果设置成异步，虽然会极大的提高消息发送的性能，但是这样会增加丢失数据的风险。如果需要确保消息的可靠性，必须将 producer.type 设置为 sync。</p>
<h3 id="Leader-选举"><a href="#Leader-选举" class="headerlink" title="Leader 选举"></a>Leader 选举</h3><p>在介绍 Leader 选举之前，让我们先来了解一下 ISR（in-sync replicas）列表。每个分区的 leader 会维护一个 ISR 列表，ISR 列表里面就是 follower 副本的 Borker 编号，只有跟得上 Leader 的 follower 副本才能加入到 ISR 里面，这个是通过 replica.lag.time.max.ms 参数配置的，具体可以参见《图文了解 Kafka 的副本复制机制》。只有 ISR 里的成员才有被选为 leader 的可能。</p>
<p>所以当 Leader 挂掉了，而且 unclean.leader.election.enable=false 的情况下，Kafka 会从 ISR 列表中选择第一个 follower 作为新的 Leader，因为这个分区拥有最新的已经 committed 的消息。通过这个可以保证已经 committed 的消息的数据可靠性。</p>
<p>综上所述，为了保证数据的可靠性，我们最少需要配置一下几个参数：</p>
<p>producer 级别：acks=all（或者 request.required.acks=-1），同时发生模式为同步 producer.type=sync<br>topic 级别：设置 replication.factor&gt;=3，并且 min.insync.replicas&gt;=2；<br>broker 级别：关闭不完全的 Leader 选举，即 unclean.leader.election.enable=false；</p>
<h3 id="数据一致性"><a href="#数据一致性" class="headerlink" title="数据一致性"></a>数据一致性</h3><p>这里介绍的数据一致性主要是说不论是老的 Leader 还是新选举的 Leader，Consumer 都能读到一样的数据。那么 Kafka 是如何实现的呢？</p>
<p>假设分区的副本为3，其中副本0是 Leader，副本1和副本2是 follower，并且在 ISR 列表里面。虽然副本0已经写入了 Message4，但是 Consumer 只能读取到 Message2。因为所有的 ISR 都同步了 Message2，只有 High Water Mark 以上的消息才支持 Consumer 读取，而 High Water Mark 取决于 ISR 列表里面偏移量最小的分区，对应于上图的副本2，这个很类似于木桶原理。</p>
<p>这样做的原因是还没有被足够多副本复制的消息被认为是“不安全”的，如果 Leader 发生崩溃，另一个副本成为新 Leader，那么这些消息很可能丢失了。如果我们允许消费者读取这些消息，可能就会破坏一致性。试想，一个消费者从当前 Leader（副本0） 读取并处理了 Message4，这个时候 Leader 挂掉了，选举了副本1为新的 Leader，这时候另一个消费者再去从新的 Leader 读取消息，发现这个消息其实并不存在，这就导致了数据不一致性问题。</p>
<p>当然，引入了 High Water Mark 机制，会导致 Broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕）。延迟时间可以通过参数 replica.lag.time.max.ms 参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。</p>
<h3 id="Kafka-生产者分区策略"><a href="#Kafka-生产者分区策略" class="headerlink" title="Kafka 生产者分区策略"></a>Kafka 生产者分区策略</h3><p>1）分区的原因<br>  1）方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic<br>又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了；<br>（2）可以提高并发，因为可以以 Partition 为单位读写了。<br>2）分区的原则<br>我们需要将 producer 发送的数据封装成一个 ProducerRecord 对象。<br>（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；<br>（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition<br>数进行取余得到 partition 值；<br>（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后<br>面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition<br>值，也就是常说的 round-robin 算法。</p>
<h3 id="消费者分区分配策略"><a href="#消费者分区分配策略" class="headerlink" title="消费者分区分配策略"></a>消费者分区分配策略</h3><p>Range 范围分区(默认的)</p>
<p>假如有10个分区，3个消费者，把分区按照序号排列0，1，2，3，4，5，6，7，8，9；消费者为C1,C2,C3，那么用分区数除以消费者数来决定每个Consumer消费几个Partition，除不尽的前面几个消费者将会多消费一个<br>最后分配结果如下</p>
<p>C1：0，1，2，3<br>C2：4，5，6<br>C3：7，8，9</p>
<p>如果有11个分区将会是：</p>
<p>C1：0，1，2，3<br>C2：4，5，6，7<br>C3：8，9，10</p>
<p>假如我们有两个主题T1,T2，分别有10个分区，最后的分配结果将会是这样：</p>
<p>C1：T1（0，1，2，3） T2（0，1，2，3）<br>C2：T1（4，5，6） T2（4，5，6）<br>C3：T1（7，8，9） T2（7，8，9）</p>
<p>在这种情况下，C1多消费了两个分区</p>
<p>RoundRobin 轮询分区</p>
<p>把所有的partition和consumer列出来，然后轮询consumer和partition，尽可能的让把partition均匀的分配给consumer</p>
<p>假如有3个Topic T0（三个分区P0-0，P0-1,P0-2），T1(两个分区P1-0,P1-1)，T2(四个分区P2-0，P2-1，P2-2，P2-3)</p>
<p>有三个消费者：C0(订阅了T0，T1),C1（订阅了T1，T2），C2(订阅了T0,T2)</p>
<p>分区将会按照一定的顺序排列起来，消费者将会组成一个环状的结构，然后开始轮询。<br>P0-0分配给C0<br>P0-1分配给C1但是C1并没订阅T0，于是跳过C1把P0-1分配给C2,<br>P0-2分配给C0<br>P1-0分配给C1,<br>P1-1分配给C0,<br>P2-0分配给C1，<br>P2-1分配给C2,<br>P2-2分配给C1,<br>p2-3分配给C2</p>
<p>C0: P0-0，P0-2，P1-1<br>C1：P1-0，P2-0，P2-2<br>C2：P0-1，P2-1，P2-3</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据质量检测</title>
    <url>/2020/01/13/19/</url>
    <content><![CDATA[<h3 id="1-重复值检查"><a href="#1-重复值检查" class="headerlink" title="1. 重复值检查"></a>1. 重复值检查</h3><p>1.1 什么是重复值<br>重复值的检查首先要明确一点，即重复值的定义。对于一份二维表形式的数据集来说，什么是重复值？主要有两个层次：<br>① 关键字段出现重复记录，比如主索引字段出现重复；<br>② 所有字段出现重复记录。<br>第一个层次是否是重复，必须从这份数据的业务含义进行确定。比如一张表，从业务上讲，一个用户应该只会有一条记录，那么如果某个用户出现了超过一条的记录，那么这就是重复值。第二个层次，就一定是重复值了。<br>1.2 重复值产生的原因<br>重复值的产生主要有两个原因，一是上游源数据造成的，二是数据准备脚本中的数据关联造成的。从数据准备角度来看，首先检查数据准备的脚本，判断使用的源表是否有重复记录，同时检查关联语句的正确性和严谨性，比如关联条件是否合理、是否有限定数据周期等等。<br>比如：检查源表数据是否重复的SQL：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT MON_ID,COUNT(*),COUNT(DISTINCT USER_ID)</span><br><span class="line">FROM TABLE_NAME</span><br><span class="line">GROUP BY MON_ID;</span><br></pre></td></tr></table></figure>
<p>如果是上游源数据出现重复，那么应该及时反映给上游进行修正；如果是脚本关联造成的，修改脚本，重新生成数据即可。<br>还有一份情况，这份数据集是一份单独的数据集，并不是在数据仓库中开发得到的数据，既没有上游源数据，也不存在生成数据的脚本，比如公开数据集，那么如何处理其中的重复值？一般的处理方式就是直接删除重复值。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">dataset = pd.read_excel(<span class="string">&quot;/labcenter/python/dataset.xlsx&quot;</span>)</span><br><span class="line"><span class="comment">#判断重复数据</span></span><br><span class="line">dataset.duplicated()    <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.duplicated([<span class="string">&#x27;col2&#x27;</span>])    <span class="comment">#col2字段重复是重复数据</span></span><br><span class="line"><span class="comment">#删除重复数据</span></span><br><span class="line">dataset.drop_duplicates()     <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.drop_duplicates([<span class="string">&#x27;col2&#x27;</span>])   <span class="comment">#col2字段重复是重复数据</span></span><br></pre></td></tr></table></figure>
<h3 id="2-缺失值检查"><a href="#2-缺失值检查" class="headerlink" title="2. 缺失值检查"></a>2. 缺失值检查</h3><p>缺失值主要是指数据集中部分记录存在部分字段的信息缺失。</p>
<p>2.1 缺失值出现的原因<br>出现趋势值主要有三种原因：<br>① 上游源系统因为技术或者成本原因无法完全获取到这一信息，比如对用户手机APP上网记录的解析；<br>② 从业务上讲，这一信息本来就不存在，比如一个学生的收入，一个未婚者的配偶姓名；<br>③ 数据准备脚本开发中的错误造成的。<br>第一种原因，短期内无法解决；第二种原因，数据的缺失并不是错误，无法避免；第三种原因，则只需通过查证修改脚本即可。<br>缺失值的存在既代表了某一部分信息的丢失，也影响了挖掘分析结论的可靠性与稳定性，因此，必须对缺失值进行处理。<br>如果缺失值记录数超过了全部记录数的50%，则应该从数据集中直接剔除掉该字段，尝试从业务上寻找替代字段；<br>如果缺失值记录数没有超过50%，则应该首先看这个字段在业务上是否有替代字段，如果有，则直接剔除掉该字段，如果没有，则必须对其进行处理。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">##查看哪些字段有缺失值   </span></span><br><span class="line">dataset.isnull().any()    <span class="comment">#获取含有NaN的字段</span></span><br><span class="line"><span class="comment">##统计各字段的缺失值个数</span></span><br><span class="line">dataset.isnull().apply(pd.value_counts)</span><br><span class="line"><span class="comment">##删除含有缺失值的字段</span></span><br><span class="line">nan_col = dataset.isnull().any()</span><br><span class="line">dataset.drop(nan_col[nan_col].index,axis=1)</span><br></pre></td></tr></table></figure>
<p>2.2 缺失值的处理<br>缺失值的处理主要有两种方式：过滤和填充。</p>
<p>（1）缺失值的过滤<br>直接删除含有缺失值的记录，总体上会影响样本个数，如果删除样本过多或者数据集本来就是小数据集时，这种方式并不建议采用。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">##删除含有缺失值的记录</span></span><br><span class="line">dataset.dropna()</span><br></pre></td></tr></table></figure>
<p>(2)缺失值的填充<br>缺失值的填充主要三种方法：<br>① 方法一：使用特定值填充<br>使用缺失值字段的平均值、中位数、众数等统计量填充。<br>优点：简单、快速<br>缺点：容易产生数据倾斜<br>② 方法二：使用算法预测填充<br>将缺失值字段作为因变量，将没有缺失值字段作为自变量，使用决策树、随机森林、KNN、回归等预测算法进行缺失值的预测，用预测结果进行填充。<br>优点：相对精确<br>缺点：效率低，如果缺失值字段与其他字段相关性不大，预测效果差<br>③ 方法三：将缺失值单独作为一个分组，指定值进行填充<br>从业务上选择一个单独的值进行填充，使缺失值区别于其他值而作为一个分组，从而不影响算法计算。<br>优点：简单，实用<br>缺点：效率低</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">##使用Pandas进行特定值填充</span></span><br><span class="line">dataset.fillna(0)   <span class="comment">##不同字段的缺失值都用0填充</span></span><br><span class="line">dataset.fillna(&#123;<span class="string">&#x27;col2&#x27;</span>:20,<span class="string">&#x27;col5&#x27;</span>:0&#125;)    <span class="comment">##不同字段使用不同的填充值</span></span><br><span class="line">dataset.fillna(dataset.mean())   <span class="comment">##分别使用各字段的平均值填充</span></span><br><span class="line">dataset.fillna(dataset.median())     <span class="comment">##分别使用个字段的中位数填充</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">##使用sklearn中的预处理方法进行缺失值填充(只适用于连续型字段)</span></span><br><span class="line">from sklearn.preprocessing import Imputer</span><br><span class="line">dataset2 = dataset.drop([<span class="string">&#x27;col4&#x27;</span>],axis=1)</span><br><span class="line">colsets = dataset2.columns</span><br><span class="line">nan_rule1 = Imputer(missing_values=<span class="string">&#x27;NaN&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=0)    <span class="comment">##创建填充规则(平均值填充)</span></span><br><span class="line">pd.DataFrame(nan_rule1.fit_transform(dataset2),columns=colsets)    <span class="comment">##应用规则</span></span><br><span class="line">nan_rule2 = Imputer(missing_values=<span class="string">&#x27;median&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=0) <span class="comment">##创建填充规则(中位数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule2.fit_transform(dataset2),columns=colsets)    <span class="comment">##应用规则</span></span><br><span class="line">nan_rule3 = Imputer(missing_values=<span class="string">&#x27;most_frequent&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=0)  <span class="comment">##创建填充规则(众数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule3.fit_transform(dataset2),columns=colsets)    <span class="comment">##应用规则</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Oozie常用命令</title>
    <url>/2020/01/02/oozie/</url>
    <content><![CDATA[<h3 id="Oozie任务重试"><a href="#Oozie任务重试" class="headerlink" title="Oozie任务重试"></a>Oozie任务重试</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oozie job -rerun 0000000-191106143754176-oozie-oozi-W  -D oozie.wf.rerun.failnodes=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="检查xml格式"><a href="#检查xml格式" class="headerlink" title="检查xml格式"></a>检查xml格式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xmllint -noout hive-site.xml</span><br></pre></td></tr></table></figure>

<h3 id="查看coordinator状态"><a href="#查看coordinator状态" class="headerlink" title="查看coordinator状态"></a>查看coordinator状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oozie <span class="built_in">jobs</span> -jobtype coordinator  -filter status=RUNNING</span><br></pre></td></tr></table></figure>

<h3 id="查看执行错误的workflow-xml"><a href="#查看执行错误的workflow-xml" class="headerlink" title="查看执行错误的workflow.xml"></a>查看执行错误的workflow.xml</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@dlbdn3 ~]<span class="comment"># oozie job -info 0000027-181203143359779-oozie-oozi-C -oozie http://localhost:11000/oozie -order=desc -timezone=&#x27;Asia/Shanghai&#x27; -filter status=KILLED AND FAILED</span></span><br><span class="line">Job ID : 0000027-181203143359779-oozie-oozi-C</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">Job Name    : PG-LANDING-JOB-Coordinator</span><br><span class="line">App Path    : hdfs://dlbdn3:8020/user/hue/oozie/deployments/_ericsson_-oozie-13-1543825800.46</span><br><span class="line">Status      : KILLED</span><br><span class="line">Start Time  : 2018-09-13 00:00 CST</span><br><span class="line">End Time    : 2040-09-01 15:15 CST</span><br><span class="line">Pause Time  : -</span><br><span class="line">Concurrency : 1</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">ID                                         Status    Ext ID                               Err Code  Created              Nominal Time         </span><br><span class="line">0000027-181203143359779-oozie-oozi-C@30    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 02:25 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@29    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 02:20 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@28    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 02:15 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@27    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 02:10 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@26    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 02:05 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@25    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 02:00 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@24    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 01:55 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@23    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 01:50 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@22    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 01:45 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@21    KILLED    -                                    -         2018-12-03 16:39 CST 2018-09-13 01:40 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@20    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:35 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@19    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:30 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@18    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:25 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@17    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:20 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@16    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:15 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@15    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:10 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@14    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:05 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@13    KILLED    -                                    -         2018-12-03 16:34 CST 2018-09-13 01:00 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000027-181203143359779-oozie-oozi-C@12    KILLED    0000039-181203143359779-oozie-oozi-W -         2018-12-03 16:34 CST 2018-09-13 00:55 CST </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>

<h3 id="查看一个具体的workflow信息"><a href="#查看一个具体的workflow信息" class="headerlink" title="查看一个具体的workflow信息"></a>查看一个具体的workflow信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@dlbdn3 ~]<span class="comment"># oozie job --oozie http://localhost:11000/oozie -info 0000039-181203143359779-oozie-oozi-W -localtime</span></span><br><span class="line">Job ID : 0000039-181203143359779-oozie-oozi-W</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">Workflow Name : PG-LANDING-JOB</span><br><span class="line">App Path      : hdfs://dlbdn3:8020/user/hue/oozie/workspaces/hue-oozie-1535681485.06</span><br><span class="line">Status        : KILLED</span><br><span class="line">Run           : 0</span><br><span class="line">User          : ericsson</span><br><span class="line">Group         : -</span><br><span class="line">Created       : 2018-12-03 16:42 CST</span><br><span class="line">Started       : 2018-12-03 16:42 CST</span><br><span class="line">Last Modified : 2018-12-03 16:43 CST</span><br><span class="line">Ended         : 2018-12-03 16:43 CST</span><br><span class="line">CoordAction ID: 0000027-181203143359779-oozie-oozi-C@12</span><br><span class="line"></span><br><span class="line">Actions</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">ID                                                                            Status    Ext ID                 Ext Status Err Code  </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000039-181203143359779-oozie-oozi-W@:start:                                  OK        -                      OK         -         </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000039-181203143359779-oozie-oozi-W@spark-23f2                               KILLED    job_1543800485319_0062 KILLED     -         </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>

<h3 id="查看当前运行的workflow有哪些"><a href="#查看当前运行的workflow有哪些" class="headerlink" title="查看当前运行的workflow有哪些"></a>查看当前运行的workflow有哪些</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@dlbdn3 ~]<span class="comment"># oozie jobs -oozie http://localhost:11000/oozie -jobtype wf  -filter status=RUNNING</span></span><br><span class="line">No Jobs match your criteria!</span><br><span class="line">[root@dlbdn3 ~]<span class="comment"># oozie jobs -oozie http://localhost:11000/oozie -jobtype wf  -filter status=RUNNING</span></span><br><span class="line">Job ID                                   App Name     Status    User      Group     Started                 Ended                   </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000041-181203143359779-oozie-oozi-W     PG-LANDING-JOBRUNNING   ericsson  -         2018-12-03 08:50 GMT    -                       </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">[root@dlbdn3 ~]<span class="comment"># oozie jobs -oozie http://localhost:11000/oozie -jobtype wf  -filter status=RUNNING -localtime</span></span><br><span class="line">Job ID                                   App Name     Status    User      Group     Started                 Ended                   </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">0000041-181203143359779-oozie-oozi-W     PG-LANDING-JOBRUNNING   ericsson  -         2018-12-03 16:50 CST    -                       </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Oozie</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解Kafka副本机制</title>
    <url>/2020/01/01/18/</url>
    <content><![CDATA[<h3 id="一、Kafka集群"><a href="#一、Kafka集群" class="headerlink" title="一、Kafka集群"></a>一、Kafka集群</h3><p>Kafka 使用 Zookeeper 来维护集群成员 (brokers) 的信息。每个 broker 都有一个唯一标识 broker.id，用于标识自己在集群中的身份，可以在配置文件 server.properties 中进行配置，或者由程序自动生成。下面是 Kafka brokers 集群自动创建的过程：</p>
<p>每一个 broker 启动的时候，它会在 Zookeeper 的 /brokers/ids 路径下创建一个 临时节点，并将自己的 broker.id 写入，从而将自身注册到集群；</p>
<p>当有多个 broker 时，所有 broker 会竞争性地在 Zookeeper 上创建 /controller 节点，由于 Zookeeper 上的节点不会重复，所以必然只会有一个 broker 创建成功，此时该 broker 称为 controller broker。它除<br>了具备其他 broker 的功能外，还负责管理主题分区及其副本的状态。</p>
<p>当 broker 出现宕机或者主动退出从而导致其持有的 Zookeeper 会话超时时，会触发注册在 Zookeeper 上的 watcher 事件，此时 Kafka 会进行相应的容错处理；如果宕机的是 controller broker 时，还会触发新的controller 选举。</p>
<h3 id="二、副本机制"><a href="#二、副本机制" class="headerlink" title="二、副本机制"></a>二、副本机制</h3><p>为了保证高可用，kafka 的分区是多副本的，如果一个副本丢失了，那么还可以从其他副本中获取分区数据。但是这要求对应副本的数据必须是完整的，这是 Kafka 数据一致性的基础，所以才需要使用 controller broker 来进行专门的管理。下面将详解介绍 Kafka 的副本机制。</p>
<p>2.1 分区和副本<br>Kafka 的主题被分为多个分区 ，分区是 Kafka 最基本的存储单位。每个分区可以有多个副本 (可以在创建主题时使用 replication-factor 参数进行指定)。其中一个副本是首领副本 (Leader replica)，所有的事件都直接发送给首领副本；其他副本是跟随者副本 (Follower replica)，需要通过复制来保持与首领副本数据一致，当首领副本不可用时，其中一个跟随者副本将成为新首领。<br><img src="/images/19.png" alt="alt"></p>
<p>2.2 ISR机制<br>每个分区都有一个 ISR(in-sync Replica) 列表，用于维护所有同步的、可用的副本。首领副本必然是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步副本：</p>
<p>与 Zookeeper 之间有一个活跃的会话，即必须定时向 Zookeeper 发送心跳；<br>在规定的时间内从首领副本那里低延迟地获取过消息。</p>
<p>如果副本不满足上面条件的话，就会被从 ISR 列表中移除，直到满足条件才会被再次加入。</p>
<p>这里给出一个主题创建的示例：使用 –replication-factor 指定副本系数为 3，创建成功后使用 –describe 命令可以看到分区 0 的有 0,1,2 三个副本，且三个副本都在 ISR 列表中，其中 1 为首领副本。<br><img src="/images/20.png" alt="alt"></p>
<p>2.3 不完全的首领选举<br>对于副本机制，在 broker 级别有一个可选的配置参数 unclean.leader.election.enable，默认值为 fasle，代表禁止不完全的首领选举。这是针对当首领副本挂掉且 ISR 中没有其他可用副本时，是否允许某个不完全同步的副本成为首领副本，这可能会导致数据丢失或者数据不一致，在某些对数据一致性要求较高的场景 (如金融领域)，这可能无法容忍的，所以其默认值为 false，如果你能够允许部分数据不一致的话，可以配置为 true。</p>
<p>2.4 最少同步副本<br>ISR 机制的另外一个相关参数是 min.insync.replicas , 可以在 broker 或者主题级别进行配置，代表 ISR 列表中至少要有几个可用副本。这里假设设置为 2，那么当可用副本数量小于该值时，就认为整个分区处于不可用状态。此时客户端再向分区写入数据时候就会抛出异常 org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。</p>
<p>2.5 发送确认<br>Kafka 在生产者上有一个可选的参数 ack，该参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入成功：<br>acks=0 ：消息发送出去就认为已经成功了，不会等待任何来自服务器的响应；<br>acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应；<br>acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。</p>
<h3 id="三、数据请求"><a href="#三、数据请求" class="headerlink" title="三、数据请求"></a>三、数据请求</h3><p>3.1 元数据请求机制<br>在所有副本中，只有领导副本才能进行消息的读写处理。由于不同分区的领导副本可能在不同的 broker 上，如果某个 broker 收到了一个分区请求，但是该分区的领导副本并不在该 broker 上，那么它就会向客户端返回一个 Not a Leader for Partition 的错误响应。 为了解决这个问题，Kafka 提供了元数据请求机制。</p>
<p>首先集群中的每个 broker 都会缓存所有主题的分区副本信息，客户端会定期发送发送元数据请求，然后将获取的元数据进行缓存。定时刷新元数据的时间间隔可以通过为客户端配置 metadata.max.age.ms 来进行指定。有了元数据信息后，客户端就知道了领导副本所在的 broker，之后直接将读写请求发送给对应的 broker 即可。</p>
<p>如果在定时请求的时间间隔内发生的分区副本的选举，则意味着原来缓存的信息可能已经过时了，此时还有可能会收到 Not a Leader for Partition 的错误响应，这种情况下客户端会再次求发出元数据请求，然后刷新本地缓存，之后再去正确的 broker 上执行对应的操作，过程如下图：<br><img src="/images/21.png" alt="alt"></p>
<p>3.2 数据可见性<br>需要注意的是，并不是所有保存在分区首领上的数据都可以被客户端读取到，为了保证数据一致性，只有被所有同步副本 (ISR 中所有副本) 都保存了的数据才能被客户端读取到。<br><img src="/images/22.png" alt="alt"></p>
<p>3.3 零拷贝<br>Kafka 所有数据的写入和读取都是通过零拷贝来实现的。传统拷贝与零拷贝的区别如下<br>传统模式下的四次拷贝与四次上下文切换<br>以将磁盘文件通过网络发送为例。传统模式下，一般使用如下伪代码所示的方法先将文件数据读入内存，然后通过 Socket 将内存中的数据发送出去。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">buffer = File.read</span><br><span class="line">Socket.send(buffer)</span><br></pre></td></tr></table></figure>

<p>这一过程实际上发生了四次数据拷贝。首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝），然后应用程序将内存态 Buffer 数据读入到用户态 Buffer（CPU 拷贝），接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝），最后通过 DMA 拷贝将数据拷贝到 NIC Buffer。同时，还伴随着四次上下文切换，如下图所示：<br><img src="/images/23.png" alt="alt"></p>
<p>sendfile和transferTo实现零拷贝<br>Linux 2.4+ 内核通过 sendfile 系统调用，提供了零拷贝。数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝。这也是零拷贝这一说法的来源。除了减少数据拷贝外，因为整个读文件到网络发送由一个 sendfile 调用完成，整个过程只有两次上下文切换，因此大大提高了性能。零拷贝过程如下图所示：<br><img src="/images/24.png" alt="alt"></p>
<p>从具体实现来看，Kafka 的数据传输通过 TransportLayer 来完成，其子类 PlaintextTransportLayer 的 transferFrom 方法通过调用 Java NIO 中 FileChannel 的 transferTo 方法实现零拷贝，如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public long transferFrom(FileChannel fileChannel, long position, long count) throws IOException &#123;</span><br><span class="line">    <span class="built_in">return</span> fileChannel.transferTo(position, count, socketChannel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注： transferTo 和 transferFrom 并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 sendfile 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。</p>
<h3 id="四、物理存储"><a href="#四、物理存储" class="headerlink" title="四、物理存储"></a>四、物理存储</h3><p>4.1 分区分配<br>在创建主题时，Kafka 会首先决定如何在 broker 间分配分区副本，它遵循以下原则：</p>
<p>在所有 broker 上均匀地分配分区副本；<br>确保分区的每个副本分布在不同的 broker 上；<br>如果使用了 broker.rack 参数为 broker 指定了机架信息，那么会尽可能的把每个分区的副本分配到不同机架的 broker 上，以避免一个机架不可用而导致整个分区不可用。</p>
<p>基于以上原因，如果你在一个单节点上创建一个 3 副本的主题，通常会抛出下面的异常：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error <span class="keyword">while</span> executing topic <span class="built_in">command</span> : org.apache.kafka.common.errors.InvalidReplicationFactor   </span><br><span class="line">Exception: Replication factor: 3 larger than available brokers: 1.</span><br></pre></td></tr></table></figure>

<p>4.2 分区数据保留规则<br>保留数据是 Kafka 的一个基本特性， 但是 Kafka 不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息。相反， Kafka 为每个主题配置了数据保留期限，规定数据被删除之前可以保留多长时间，或者清理数据之前可以保留的数据量大小。分别对应以下四个参数：</p>
<p>log.retention.bytes ：删除数据前允许的最大数据量；默认值-1，代表没有限制；<br>log.retention.ms：保存数据文件的毫秒数，如果未设置，则使用 log.retention.minutes 中的值，默认为 null；<br>log.retention.minutes：保留数据文件的分钟数，如果未设置，则使用 log.retention.hours 中的值，默认为 null；<br>log.retention.hours：保留数据文件的小时数，默认值为 168，也就是一周。<br>因为在一个大文件里查找和删除消息是很费时的，也很容易出错，所以 Kafka 把分区分成若干个片段，当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除。如果按照默认值保留数据一周，而且每天使用一个新片段，那么你就会看到，在每天使用一个新片段的同时会删除一个最老的片段，所以大部分时间该分区会有 7 个片段存在。</p>
<p>4.3 文件格式<br>通常保存在磁盘上的数据格式与生产者发送过来消息格式是一样的。 如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送 (格式如下所示) ，然后保存到磁盘上。之后消费者读取后再自己解压这个包装消息，获取每条消息的具体信息。<br><img src="/images/25.png" alt="alt"></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>APP数据统计-用户活跃统计周活跃，月活跃(不是按照自然周计算,每天的前７天　前３０天)</title>
    <url>/2019/11/14/2/</url>
    <content><![CDATA[<h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line">import org.apache.spark.sql.types.LongType</span><br><span class="line">import org.apache.spark.sql.types.StructType</span><br><span class="line">import org.apache.spark.sql.types.StringType</span><br><span class="line">import org.apache.spark.sql.types.StructField</span><br><span class="line">import org.apache.spark.sql.types.LongType</span><br><span class="line">import com.jiatui.utils.Constants</span><br><span class="line">import com.jiatui.utils.CommonUtils</span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line"></span><br><span class="line">object App_user_active_statistics &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val Array(appName, logLevel, parallelismStr, dstTbl, start_date, end_date,enabled) = args</span><br><span class="line">    val conf = new SparkConf().setAppName(appName)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.default.parallelism&quot;</span>, parallelismStr)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, parallelismStr)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.sql.adaptive.enabled&quot;</span>, enabled)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.sql.adaptive.shuffle.targetPostShuffleInputSize&quot;</span>, <span class="string">&quot;128000000&quot;</span>)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    sc.setLogLevel(logLevel)</span><br><span class="line"></span><br><span class="line">    val hiveContext = new HiveContext(sc)</span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>)</span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.max.dynamic.partitions.pernode&quot;</span>, <span class="string">&quot;2000&quot;</span>)</span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.max.dynamic.partitions&quot;</span>, <span class="string">&quot;5000&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val ts = CommonUtils.getCurrentTime(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>)</span><br><span class="line">//   (2019-05-01 2019-07-16)</span><br><span class="line">    val allDate = CommonUtils.findDates(start_date, end_date)</span><br><span class="line">//    2019-05-01,2019-05-02,2019-05-03 </span><br><span class="line">    val allDate_bc = sc.broadcast(allDate)</span><br><span class="line">//起始日期再推３０天</span><br><span class="line">    val twenty_nine_days_ago = CommonUtils.getDateByStep(start_date, -29)</span><br><span class="line">    val all_data_sql = s<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                           SELECT  app_user_id,user_type,company_id,snapshot_date</span></span><br><span class="line"><span class="string">                              FROM </span></span><br><span class="line"><span class="string">             dwd.DWD_APP_USER_ACTIVE_RECORD </span></span><br><span class="line"><span class="string">          WHERE dt between &#x27;<span class="variable">$twenty_nine_days_ago</span>&#x27; AND  &#x27;<span class="variable">$end_date</span>&#x27;</span></span><br><span class="line"><span class="string">      &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    val all_active_data = hiveContext.sql(all_data_sql).persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class="line"></span><br><span class="line">    val day_active_data_rdd = all_active_data.flatMap(x =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      val data = new ArrayBuffer[Row]()</span><br><span class="line"></span><br><span class="line">      try &#123;</span><br><span class="line"></span><br><span class="line">        val app_user_id = x.getAs[String](<span class="string">&quot;app_user_id&quot;</span>)</span><br><span class="line">        val user_type = x.getAs[String](<span class="string">&quot;user_type&quot;</span>)</span><br><span class="line">        val company_id = x.getAs[String](<span class="string">&quot;company_id&quot;</span>)</span><br><span class="line">        val snapshot_date = x.getAs[String](<span class="string">&quot;snapshot_date&quot;</span>)</span><br><span class="line"></span><br><span class="line">        //循环日期</span><br><span class="line">        val allDatefor = allDate_bc.value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (singleDate &lt;- allDatefor) &#123;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (singleDate.equals(snapshot_date)) &#123;</span><br><span class="line"></span><br><span class="line">            data += modifyRecord(0, 1L, user_type, company_id, singleDate)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125; catch &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> t: Throwable =&gt; t.printStackTrace()</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      data</span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    //周活和月活数据</span><br><span class="line">    val week_month_active_data_rdd = all_active_data.flatMap(x =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      val data = new ArrayBuffer[(( String, String, String, String), Int)]()</span><br><span class="line">      try &#123;</span><br><span class="line"></span><br><span class="line">        val app_user_id = x.getAs[String](<span class="string">&quot;app_user_id&quot;</span>)</span><br><span class="line">        val user_type = x.getAs[String](<span class="string">&quot;user_type&quot;</span>)</span><br><span class="line">        val company_id = x.getAs[String](<span class="string">&quot;company_id&quot;</span>)</span><br><span class="line">        val snapshot_date = x.getAs[String](<span class="string">&quot;snapshot_date&quot;</span>)</span><br><span class="line"></span><br><span class="line">        val allDatefor = allDate_bc.value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (singleDate &lt;- allDatefor) &#123;</span><br><span class="line">          // 二个日期的差值</span><br><span class="line">          val date_diff = CommonUtils.getDateDiff(snapshot_date, singleDate)</span><br><span class="line">          <span class="keyword">if</span> (date_diff &gt;= 0 &amp;&amp; date_diff &lt;= 6) &#123;</span><br><span class="line">            data += (((singleDate, company_id, app_user_id, user_type), 1))</span><br><span class="line">          &#125; </span><br><span class="line">        &#125;    </span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        <span class="keyword">case</span> t: Throwable =&gt; t.printStackTrace()</span><br><span class="line">      &#125;</span><br><span class="line">      data</span><br><span class="line">    &#125;).reduceByKey &#123;</span><br><span class="line">      <span class="keyword">case</span> (x, y) =&gt;</span><br><span class="line">        x</span><br><span class="line">    &#125;.flatMap &#123;</span><br><span class="line">      x =&gt;</span><br><span class="line">          Some(modifyRecord(1, 1L, x._1._4, x._1._2, x._1._1))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //月活跃</span><br><span class="line">     val month_active_data_rdd = all_active_data.flatMap(x =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      val data = new ArrayBuffer[(( String, String, String, String), Int)]()</span><br><span class="line">      try &#123;</span><br><span class="line"></span><br><span class="line">        val app_user_id = x.getAs[String](<span class="string">&quot;app_user_id&quot;</span>)</span><br><span class="line">        val user_type = x.getAs[String](<span class="string">&quot;user_type&quot;</span>)</span><br><span class="line">        val company_id = x.getAs[String](<span class="string">&quot;company_id&quot;</span>)</span><br><span class="line">        val snapshot_date = x.getAs[String](<span class="string">&quot;snapshot_date&quot;</span>)</span><br><span class="line"></span><br><span class="line">        val allDatefor = allDate_bc.value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (singleDate &lt;- allDatefor) &#123;</span><br><span class="line">          // 二个日期的差值</span><br><span class="line">          val date_diff = CommonUtils.getDateDiff(snapshot_date, singleDate)</span><br><span class="line">           <span class="keyword">if</span> (date_diff &gt;= 0 &amp;&amp; date_diff &lt;= 29) &#123;</span><br><span class="line">            data += ((( singleDate, company_id, app_user_id, user_type),2))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;    </span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        <span class="keyword">case</span> t: Throwable =&gt; t.printStackTrace()</span><br><span class="line">      &#125;</span><br><span class="line">      data</span><br><span class="line">    &#125;).reduceByKey &#123;</span><br><span class="line">      <span class="keyword">case</span> (x, y) =&gt;</span><br><span class="line">        x</span><br><span class="line">    &#125;.flatMap &#123;</span><br><span class="line">      x =&gt;</span><br><span class="line">       Some(modifyRecord(2, 1L, x._1._4, x._1._2, x._1._1))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">          </span><br><span class="line">    val total_new_app_user_sql = s<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                           SELECT  app_user_id,user_type,company_id,snapshot_date</span></span><br><span class="line"><span class="string">                              FROM </span></span><br><span class="line"><span class="string">             dwd.dwd_app_user_increased_record </span></span><br><span class="line"><span class="string">          WHERE   dt &lt;= &#x27;<span class="variable">$end_date</span>&#x27;</span></span><br><span class="line"><span class="string">      &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">          </span><br><span class="line">    val total_new_app_user_rdd = hiveContext.sql(total_new_app_user_sql).flatMap(x =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      val data = new ArrayBuffer[Row]()</span><br><span class="line">      try &#123;</span><br><span class="line"></span><br><span class="line">        val app_user_id = x.getAs[String](<span class="string">&quot;app_user_id&quot;</span>)</span><br><span class="line">        val user_type = x.getAs[String](<span class="string">&quot;user_type&quot;</span>)</span><br><span class="line">        val company_id = x.getAs[String](<span class="string">&quot;company_id&quot;</span>)</span><br><span class="line">        val snapshot_date = x.getAs[String](<span class="string">&quot;snapshot_date&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          val allDatefor = allDate_bc.value</span><br><span class="line"></span><br><span class="line">          <span class="keyword">for</span> (singleDate &lt;- allDatefor) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (singleDate.compareTo(snapshot_date) &gt;= 0) &#123;</span><br><span class="line"></span><br><span class="line">              data += modifyRecord(4, 1L, user_type, company_id, singleDate)</span><br><span class="line"></span><br><span class="line">            &#125; </span><br><span class="line">            <span class="keyword">if</span> (singleDate.equals(snapshot_date)) &#123;</span><br><span class="line"></span><br><span class="line">              data += modifyRecord(3, 1L, user_type, company_id, singleDate)</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        <span class="keyword">case</span> t: Throwable =&gt; t.printStackTrace()</span><br><span class="line">      &#125;</span><br><span class="line">      data</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val row_rdd = day_active_data_rdd.union(week_month_active_data_rdd).union(month_active_data_rdd).union(total_new_app_user_rdd)</span><br><span class="line">    val struct = StructType(</span><br><span class="line">      Array(</span><br><span class="line">        StructField(<span class="string">&quot;day_active_app_user_cnt&quot;</span>, LongType),</span><br><span class="line">        StructField(<span class="string">&quot;week_active_app_user_cnt&quot;</span>, LongType),</span><br><span class="line">        StructField(<span class="string">&quot;month_active_app_user_cnt&quot;</span>, LongType),</span><br><span class="line">        StructField(<span class="string">&quot;new_app_user_cnt&quot;</span>, LongType),</span><br><span class="line">        StructField(<span class="string">&quot;total_app_user_cnt&quot;</span>, LongType),</span><br><span class="line">        StructField(<span class="string">&quot;user_type&quot;</span>, StringType),</span><br><span class="line">        StructField(<span class="string">&quot;company_id&quot;</span>, StringType),</span><br><span class="line">        StructField(<span class="string">&quot;snapshot_date&quot;</span>, StringType)))</span><br><span class="line"></span><br><span class="line">    val df = hiveContext.createDataFrame(row_rdd, struct)</span><br><span class="line"> </span><br><span class="line">    val tmpTable = <span class="string">&quot;tmp&quot;</span></span><br><span class="line">    df.registerTempTable(tmpTable)</span><br><span class="line"></span><br><span class="line">    val final_sql = s<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">          INSERT OVERWRITE TABLE <span class="variable">$dstTbl</span> partition(dt) </span></span><br><span class="line"><span class="string">          SELECT</span></span><br><span class="line"><span class="string">    		      day_active_app_user_cnt,</span></span><br><span class="line"><span class="string">              week_active_app_user_cnt,</span></span><br><span class="line"><span class="string">              month_active_app_user_cnt,</span></span><br><span class="line"><span class="string">              new_app_user_cnt,</span></span><br><span class="line"><span class="string">              total_app_user_cnt,</span></span><br><span class="line"><span class="string">              user_type,</span></span><br><span class="line"><span class="string">              company_id,</span></span><br><span class="line"><span class="string">              from_unixtime(unix_timestamp(),&#x27;yyyy-MM-dd HH:mm:ss&#x27;),</span></span><br><span class="line"><span class="string">              snapshot_date,</span></span><br><span class="line"><span class="string">              snapshot_date</span></span><br><span class="line"><span class="string">    	    FROM <span class="variable">$tmpTable</span></span></span><br><span class="line"><span class="string">          &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;sql = &quot;</span> + final_sql)</span><br><span class="line">    hiveContext.sql(final_sql)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def modifyRecord(index: Int, new_value: Long, user_type: String, company_id: String, snapshot_date: String) = &#123;</span><br><span class="line">    val L1 = List(0L, 0L, 0L, 0L, 0L, user_type, company_id, snapshot_date)</span><br><span class="line">    val L2 = L1.updated(index, new_value)</span><br><span class="line">    Row.fromSeq(L2)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark FastJson 解析SDK上报日期</title>
    <url>/2019/11/02/1/</url>
    <content><![CDATA[<h3 id="日志格式为"><a href="#日志格式为" class="headerlink" title="日志格式为"></a>日志格式为</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;CommonInfo&quot;</span>:&#123;<span class="string">&quot;env&quot;</span>:<span class="string">&quot;iOS&quot;</span>,<span class="string">&quot;productId&quot;</span>:<span class="string">&quot;JiaTuiAPP&quot;</span>,<span class="string">&quot;userInfo&quot;</span>:&#123;<span class="string">&quot;userId&quot;</span>:<span class="string">&quot;561239948443779072&quot;</span>&#125;,<span class="string">&quot;systemInfo&quot;</span>:&#123;<span class="string">&quot;system&quot;</span>:<span class="string">&quot;12.2&quot;</span>,<span class="string">&quot;platform&quot;</span>:<span class="string">&quot;iOS&quot;</span>,<span class="string">&quot;model&quot;</span>:<span class="string">&quot;iPhone 5s&quot;</span>,<span class="string">&quot;pixelRatio&quot;</span>:2,<span class="string">&quot;brand&quot;</span>:<span class="string">&quot;iPhone&quot;</span>,<span class="string">&quot;screenWidth&quot;</span>:320,<span class="string">&quot;screenHeight&quot;</span>:568,<span class="string">&quot;version&quot;</span>:<span class="string">&quot;1.0&quot;</span>&#125;&#125;,<span class="string">&quot;eventObject&quot;</span>:&#123;<span class="string">&quot;eventId&quot;</span>:<span class="string">&quot;2048&quot;</span>&#125;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="解析Demo"><a href="#解析Demo" class="headerlink" title="解析Demo"></a>解析Demo</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.bigdata.JsonFormat</span><br><span class="line">import com.alibaba.fastjson.JSON</span><br><span class="line">object Test &#123;</span><br><span class="line">  </span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">//    println(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    val jsonStr = <span class="string">&quot;&quot;</span><span class="string">&quot;&#123;&quot;</span>CommonInfo<span class="string">&quot;:&#123;&quot;</span>env<span class="string">&quot;:&quot;</span>iOS<span class="string">&quot;,&quot;</span>productId<span class="string">&quot;:&quot;</span>JiaTuiAPP<span class="string">&quot;,&quot;</span>userInfo<span class="string">&quot;:&#123;&quot;</span>userId<span class="string">&quot;:&quot;</span>561239948443779072<span class="string">&quot;&#125;,&quot;</span>systemInfo<span class="string">&quot;:&#123;&quot;</span>system<span class="string">&quot;:&quot;</span>12.2<span class="string">&quot;,&quot;</span>platform<span class="string">&quot;:&quot;</span>iOS<span class="string">&quot;,&quot;</span>model<span class="string">&quot;:&quot;</span>iPhone 5s<span class="string">&quot;,&quot;</span>pixelRatio<span class="string">&quot;:2,&quot;</span>brand<span class="string">&quot;:&quot;</span>iPhone<span class="string">&quot;,&quot;</span>screenWidth<span class="string">&quot;:320,&quot;</span>screenHeight<span class="string">&quot;:568,&quot;</span>version<span class="string">&quot;:&quot;</span>1.0<span class="string">&quot;&#125;&#125;,&quot;</span>eventObject<span class="string">&quot;:&#123;&quot;</span>eventId<span class="string">&quot;:&quot;</span>2048<span class="string">&quot;&#125;&#125;&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    val jsonRoot = JSON.parseObject(jsonStr)</span><br><span class="line">    val CommonInfo = JSON.parseObject(jsonRoot.get(<span class="string">&quot;CommonInfo&quot;</span>) + <span class="string">&quot;&quot;</span>)</span><br><span class="line">    val userInfo = JSON.parseObject(CommonInfo.get(<span class="string">&quot;userInfo&quot;</span>) + <span class="string">&quot;&quot;</span>)</span><br><span class="line">    val systemInfo = JSON.parseObject(CommonInfo.get(<span class="string">&quot;systemInfo&quot;</span>) + <span class="string">&quot;&quot;</span>)</span><br><span class="line">    val eventObject=JSON.parseObject(jsonRoot.get(<span class="string">&quot;eventObject&quot;</span>) + <span class="string">&quot;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">        val env=CommonInfo.get(<span class="string">&quot;env&quot;</span>)</span><br><span class="line">        val productId=CommonInfo.get(<span class="string">&quot;productId&quot;</span>)</span><br><span class="line">        val userId=userInfo.get(<span class="string">&quot;userId&quot;</span>)</span><br><span class="line">        val system=systemInfo.get(<span class="string">&quot;system&quot;</span>)</span><br><span class="line">        val platform=systemInfo.get(<span class="string">&quot;platform&quot;</span>)</span><br><span class="line">        val model=systemInfo.get(<span class="string">&quot;model&quot;</span>)</span><br><span class="line">        val pixelRatio=systemInfo.get(<span class="string">&quot;pixelRatio&quot;</span>)</span><br><span class="line">        val brand=systemInfo.get(<span class="string">&quot;brand&quot;</span>)</span><br><span class="line">        val screenWidth=systemInfo.get(<span class="string">&quot;screenWidth&quot;</span>)</span><br><span class="line">        val screenHeight=systemInfo.get(<span class="string">&quot;screenHeight&quot;</span>)</span><br><span class="line">        val version=systemInfo.get(<span class="string">&quot;version&quot;</span>)</span><br><span class="line">        val eventId=eventObject.get(<span class="string">&quot;eventId&quot;</span>)</span><br><span class="line">        println(env)</span><br><span class="line">        println(productId)</span><br><span class="line">        println(userId)</span><br><span class="line">        println(system)</span><br><span class="line">        println(platform)</span><br><span class="line">        println(model)</span><br><span class="line">        println(pixelRatio)</span><br><span class="line">        println(screenWidth)</span><br><span class="line">        println(screenHeight)</span><br><span class="line">        println(version)</span><br><span class="line">        println(eventId)</span><br><span class="line">        </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Yarn 的提交二种方式</title>
    <url>/2019/08/14/28/</url>
    <content><![CDATA[<h3 id="一、前述"><a href="#一、前述" class="headerlink" title="一、前述"></a>一、前述</h3><p> Spark可以和Yarn整合，将Application提交到Yarn上运行，和StandAlone提交模式一样，Yarn也有两种提交任务的方式。</p>
<h3 id="具体"><a href="#具体" class="headerlink" title="具体"></a>具体</h3><p>1、yarn-client提交任务方式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./spark-submit --master yarn  --class org.apache.spark.examples.SparkPi  ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br><span class="line">./spark-submit   --master yarn-lient   --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br><span class="line">./spark-submit  --master yarn --deploy-mode  client --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br></pre></td></tr></table></figure>


<p><img src="/images/44.png" alt="alt"></p>
<p>执行原理图解</p>
<p><img src="/images/45.png" alt="alt"></p>
<p>执行流程<br>1.客户端提交一个Application，在客户端启动一个Driver进程。<br>2.Driver进程会向RS(ResourceManager)发送请求，启动AM(ApplicationMaster)的资源。<br>3.RS收到请求，随机选择一台NM(NodeManager)启动AM。这里的NM相当于Standalone中的Worker节点。<br>4.AM启动后，会向RS请求一批container资源，用于启动Executor.<br>5.RS会找到一批NM返回给AM,用于启动Executor。<br>6.AM会向NM发送命令启动Executor。<br>7.Executor启动后，会反向注册给Driver，Driver发送task到Executor,执行情况和结果返回给Driver端。</p>
<p>总结<br>        1、Yarn-client模式同样是适用于测试，因为Driver运行在本地，Driver会与yarn集群中的Executor进行大量的通信，会造成客户机网卡流量的大量增加.</p>
<pre><code>    2、 ApplicationMaster的作用：

             为当前的Application申请资源

             给NodeManager发送消息启动Executor。</code></pre>
<p>注意：ApplicationMaster有launchExecutor和申请资源的功能，并没有作业调度的功能。</p>
<p>2、yarn-cluster提交任务方式</p>
<p>提交命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./spark-submit --master yarn --deploy-mode cluster  --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br><span class="line">./spark-submit   --master yarn-cluster  --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br></pre></td></tr></table></figure>
<p>结果在yarn的日志里面：</p>
<p><img src="/images/46.png" alt="alt"></p>
<p>执行原理<br><img src="/images/47.png" alt="alt"><br>执行流程<br>1.客户机提交Application应用程序，发送请求到RS(ResourceManager),请求启动AM(ApplicationMaster)。<br>2.RS收到请求后随机在一台NM(NodeManager)上启动AM（相当于Driver端）。<br>3.AM启动，AM发送请求到RS，请求一批container用于启动Executor。<br>4.RS返回一批NM节点给AM。<br>5.AM连接到NM,发送请求到NM启动Executor。<br>6.Executor反向注册到AM所在的节点的Driver。Driver发送task到Executor。</p>
<p>总结<br>        1、Yarn-Cluster主要用于生产环境中，因为Driver运行在Yarn集群中某一台nodeManager中，每次提交任务的Driver所在的机器都是随机的，不会产生某一台机器网卡流量激增的现象，缺点是任务提交后不能看到日志。只能通过yarn查看日志。</p>
<pre><code>    2.ApplicationMaster的作用：

           为当前的Application申请资源

           给nodemanager发送消息 启动Excutor。

           任务调度。(这里和client模式的区别是AM具有调度能力，因为其就是Driver端，包含Driver进程)</code></pre>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>ELK日志采集logstash output -&gt; elasticsearch 数据写入性能优化。</title>
    <url>/2019/07/14/10/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>前些时间测试线上ELK环境，发现beats组件直连elasticsearch数据灌入异常快，但是过了logstash数据明显迟缓。判定logstah的灌入存在瓶颈。以下为logstash调优细节。</p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>本次针对的优化对象是线上的日志分析平台，主要数据源为基础服务器数据和应用日志<br>ES节点：顶配3点集群，各方面负载不高，无写入瓶颈<br>logstah节点：2个汇聚端，<br>网络：内网万兆传输，无瓶颈<br>数据量（条）：1~1.5w/s</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/images/1.png" alt="alt"></p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>logstah的功能是一个管道，通过input灌入数据，filter过滤数据，output输入数据</p>
<p>input：filebeat和metricbeat总连接数为500左右，且观察日志无retry 或 timeout等输出，无明显瓶颈<br>filter和output：logstash的正则解析过程非常消耗资源，但是我们的节点资源消耗居然不高。在新版的logstash中优化了input，filter，output的线程模式，在配置文件中可以通过配置pipeline.workers来调整filter和output的线程数</p>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>查询官网手册后，最影响logstash传输效率的参数有以下几个:<br>1、pipeline.workers：决定filter和output的线程数，官方建议大于CPU数，如果logstah节点是混用服务器，建议等于或小于CPU数<br>2、pipeline.batch.size：单个线程每次调用ES bulk index API时的事件数。这些时间将被放到内存中。最好的设定值是不断地测试，测试，测试。<br>3、pipeline.batch.size：单个线程每次调用ES bulk index API时的事件数。这些时间将被放到内存中。最好的设定值是不断地测试，测试，测试。<br>优化后的logstash<br>pipeline.batch.size: 2500<br>pipeline.batch.delay: 5<br>pipeline.workers: 8<br>pipeline.batch.size: 2500<br>pipeline.batch.delay: 50</p>
<h3 id="优化结果"><a href="#优化结果" class="headerlink" title="优化结果"></a>优化结果</h3><p><img src="/images/2.png" alt="alt"></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkStructured StreamExecution：持续查询的运转引擎</title>
    <url>/2019/06/23/13/</url>
    <content><![CDATA[<h3 id="StreamExecution-的初始状态"><a href="#StreamExecution-的初始状态" class="headerlink" title="StreamExecution 的初始状态"></a>StreamExecution 的初始状态</h3><p>定义好 Dataset/DataFrame 的产生、变换和写出，再启动 StreamExection 去持续查询。这些 Dataset/DataFrame 的产生、变换和写出的信息就对应保存在 StreamExecution 非常重要的 3 个成员变量中：<br>1、sources: streaming data 的产生端（比如 kafka 等）<br>2、logicalPlan: DataFrame/Dataset 的一系列变换（即计算逻辑）<br>3、sink: 最终结果写出的接收端（比如 file system 等）</p>
<p>StreamExection 另外的重要成员变量是：<br>1、currentBatchId: 当前执行的 id<br>2、batchCommitLog: 已经成功处理过的批次有哪些<br>3、offsetLog, availableOffsets, committedOffsets: 当前执行需要处理的 source data 的 meta 信息<br>4、offsetSeqMetadata: 当前执行的 watermark 信息（event time 相关，本文暂不涉及、另文解析）等</p>
<p><img src="/images/6.png" alt="alt"></p>
<h3 id="StreamExecution-的持续查询"><a href="#StreamExecution-的持续查询" class="headerlink" title="StreamExecution 的持续查询"></a>StreamExecution 的持续查询</h3><p><img src="/images/7.png" alt="alt"><br>一次执行的过程如上图；这里有 6 个关键步骤:<br>1.StreamExecution 通过 Source.getOffset() 获取最新的 offsets，即最新的数据进度；<br>2.StreamExecution 将 offsets 等写入到 offsetLog 里<br>  这里的 offsetLog 是一个持久化的 WAL (Write-Ahead-Log)，是将来可用作故障恢复用<br>3.StreamExecution 构造本次执行的 LogicalPlan<br>  (3a) 将预先定义好的逻辑（即 StreamExecution 里的 logicalPlan 成员变量）制作一个副本出来<br>  (3b) 给定刚刚取到的 offsets，通过 Source.getBatch(offsets) 获取本执行新收到的数据的 Dataset/DataFrame 表示，并替换到 (3a) 中的副本里<br>  经过 (3a), (3b) 两步，构造完成的 LogicalPlan 就是针对本执行新收到的数据的 Dataset/DataFrame 变换（即整个处理逻辑）了<br>4.触发对本次执行的 LogicalPlan 的优化，得到 IncrementalExecution<br>  逻辑计划的优化：通过 Catalyst 优化器完成<br>  物理计划的生成与选择：结果是可以直接用于执行的 RDD DAG<br>  逻辑计划、优化的逻辑计划、物理计划、及最后结果 RDD DAG，合并起来就是 IncrementalExecution<br>5.将表示计算结果的 Dataset/DataFrame (包含 IncrementalExecution) 交给 Sink，即调用 Sink.add(ds/df)<br>6.计算完成后的 commit<br>  (6a) 通过 Source.commit() 告知 Source 数据已经完整处理结束；Source 可按需完成数据的 garbage-collection<br>  (6b) 将本次执行的批次 id 写入到 batchCommitLog 里</p>
<h3 id="StreamExecution-的持续查询（增量）"><a href="#StreamExecution-的持续查询（增量）" class="headerlink" title="StreamExecution 的持续查询（增量）"></a>StreamExecution 的持续查询（增量）</h3><p><img src="/images/8.png" alt="alt"><br>Structured Streaming 在编程模型上暴露给用户的是，每次持续查询看做面对全量数据（而不仅仅是本次执行信收到的数据），所以每次执行的结果是针对全量数据进行计算的结果。</p>
<p>但是在实际执行过程中，由于全量数据会越攒越多，那么每次对全量数据进行计算的代价和消耗会越来越大。</p>
<p>Structured Streaming 的做法是：<br>1.引入全局范围、高可用的 StateStore<br>2.转全量为增量，即在每次执行时：<br>  先从 StateStore 里 restore 出上次执行后的状态<br>  然后加入本执行的新数据，再进行计算<br>  如果有状态改变，将把改变的状态重新 save 到 StateStore 里<br>3.为了在 Dataset/DataFrame 框架里完成对 StateStore 的 restore 和 s如果在某个执行过程中发生 driver 故障，那么重新起来的 StreamExecution：<br>所以 Structured Streaming 在编程模型上暴露给用户的是，每次持续查询看做面对全量数据，但在具体实现上转换为增量的持续查询。</p>
<h3 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h3><p><img src="/images/9.png" alt="alt"><br>由于 exectutor 节点的故障可由 Spark 框架本身很好的 handle，不引起可用性问题，我们本节的故障恢复只讨论 driver 故障恢复。<br>1.如果在某个执行过程中发生 driver 故障，那么重新起来的 StreamExecution：<br>2.读取 WAL offsetlog 恢复出最新的 offsets 等；相当于取代正常流程里的 (1)(2) 步<br>3.读取 batchCommitLog 决定是否需要重做最近一个批次<br>4.如果需要，那么重做 (3a), (3b), (4), (5), (6a), (6b) 步<br>   这里第 (5) 步需要分两种情况讨论<br>   (i) 如果上次执行在 (5) 结束前即失效，那么本次执行里 sink 应该完整写出计算结果<br>   (ii) 如果上次执行在 (5) 结束后才失效，那么本次执行里 sink 可以重新写出计算结果（覆盖上次结果），也可以跳过写出计算结果（因为上次执行已经完整写出过计算结果了）<br>这样即可保证每次执行的计算结果，在 sink 这个层面，是 不重不丢 的 —— 即使中间发生过 1 次或以上的失效和恢复。</p>
<h3 id="小结：end-to-end-exactly-once-guarantees"><a href="#小结：end-to-end-exactly-once-guarantees" class="headerlink" title="小结：end-to-end exactly-once guarantees"></a>小结：end-to-end exactly-once guarantees</h3><p>所以在 Structured Streaming 里，我们总结下面的关系[4]：</p>
<p><img src="/images/10.png" alt="alt"><br>这里的 end-to-end 指的是，如果 source 选用类似 Kafka, HDFS 等，sink 选用类似 HDFS, MySQL 等，那么 Structured Streaming 将自动保证在 sink 里的计算结果是 exactly-once 的 —— Structured Streaming终于把过去需要使用者去维护的 sink 去重逻辑接盘过去了！:-)</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark Structured</tag>
      </tags>
  </entry>
  <entry>
    <title>通过BulkLoad(MR)快速将海量数据导入到Hbase</title>
    <url>/2019/06/11/5/</url>
    <content><![CDATA[<h3 id="原始文件从mysq导出来的csv文件"><a href="#原始文件从mysq导出来的csv文件" class="headerlink" title="原始文件从mysq导出来的csv文件"></a>原始文件从mysq导出来的csv文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">503003676755886086,503003161271734273,1</span><br><span class="line">503003669797548035,503003161271734273,1</span><br><span class="line">503003568609964035,503003161271734273,1</span><br><span class="line">503003700512428038,503003161271734273,1</span><br><span class="line">503003764244881414,503003161271734273,1</span><br><span class="line">503003647634841604,503003161271734273,4</span><br><span class="line">503003747857739782,503003161271734273,7</span><br><span class="line">503003582082068480,503003161271734273,5</span><br><span class="line">503003646376542208,503003161271734273,3</span><br><span class="line">503003631474180105,503003161271734273,1</span><br></pre></td></tr></table></figure>

<h3 id="使用MR-对文件进行加盐操作根据MD5-region数量"><a href="#使用MR-对文件进行加盐操作根据MD5-region数量" class="headerlink" title="使用MR 对文件进行加盐操作根据MD5/region数量"></a>使用MR 对文件进行加盐操作根据MD5/region数量</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line">import java.security.MessageDigest;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 计数系统工具类</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">public class CounterUtils &#123;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	private static final Logger logger = LoggerFactory.getLogger(CounterUtils.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	/** region数量 */</span><br><span class="line">	private static Integer region_num = 4;</span><br><span class="line">	</span><br><span class="line">	/** salt前缀  */</span><br><span class="line">	private static String salt_prefix = <span class="string">&quot;00&quot;</span>;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	/**</span><br><span class="line">	 * 为HBase rowkey产生salt值</span><br><span class="line">	 * @param key 原始计数键名</span><br><span class="line">	 * @<span class="built_in">return</span> salt值</span><br><span class="line">	 */</span><br><span class="line">	</span><br><span class="line">	public static String generateSalt(String key)&#123;</span><br><span class="line">		</span><br><span class="line">		Integer region_seq = Math.abs(generateMD5(key).hashCode()) % region_num;</span><br><span class="line">		</span><br><span class="line">		<span class="built_in">return</span> salt_prefix+String.valueOf(region_seq);</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	/**</span><br><span class="line">	 * 根据字符串产生MD5值</span><br><span class="line">	 * @param msg</span><br><span class="line">	 * @<span class="built_in">return</span>  字符串对应的MD5值</span><br><span class="line">	 */</span><br><span class="line">	</span><br><span class="line">	public static String generateMD5(String msg) &#123;</span><br><span class="line"></span><br><span class="line">		MessageDigest md5 = null;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line"></span><br><span class="line">			md5 = MessageDigest.getInstance(<span class="string">&quot;MD5&quot;</span>);</span><br><span class="line"></span><br><span class="line">		&#125; catch (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">			</span><br><span class="line">			logger.error(<span class="string">&quot;产生MD5值报错&quot;</span>,e);</span><br><span class="line"></span><br><span class="line">			<span class="built_in">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		char[] charArray = msg.toCharArray();</span><br><span class="line">		byte[] byteArray = new byte[charArray.length];</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (int i = 0; i &amp;lt; charArray.length; i++)</span><br><span class="line">			byteArray[i] = (byte) charArray[i];</span><br><span class="line"></span><br><span class="line">		byte[] md5Bytes = md5.digest(byteArray);</span><br><span class="line">		StringBuffer hexValue = new StringBuffer();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (int i = 0; i &amp;lt; md5Bytes.length; i++) &#123;</span><br><span class="line"></span><br><span class="line">			int val = ((int) md5Bytes[i]) &amp;amp; 0xff;</span><br><span class="line">			<span class="keyword">if</span> (val &amp;lt; 16)</span><br><span class="line">				hexValue.append(<span class="string">&quot;0&quot;</span>);</span><br><span class="line">			hexValue.append(Integer.toHexString(val));</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="built_in">return</span> hexValue.toString();</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="加盐生成另一种文件MR程序"><a href="#加盐生成另一种文件MR程序" class="headerlink" title="加盐生成另一种文件MR程序"></a>加盐生成另一种文件MR程序</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import java.io.DataInput;</span><br><span class="line">import java.io.DataOutput;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.net.URI;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.conf.Configured;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.io.Writable;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line">import org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line">import org.apache.hadoop.util.Tool;</span><br><span class="line">import org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line">import com.jiatui.bigdata.util.CounterUtils;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class DateConversion extends Configured implements Tool &#123;</span><br><span class="line"></span><br><span class="line">	// 构建map类</span><br><span class="line">	public static class TestMap extends Mapper&amp;lt;LongWritable, Text, Text, TestWritable&amp;gt; &#123;</span><br><span class="line"></span><br><span class="line">		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">			// 根据$开始切割字段名</span><br><span class="line">			 String[] splited = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">			// 以第一个和第二个为rowkey值</span><br><span class="line">			String rowkey1=<span class="string">&quot;LOOK_TIMES_&quot;</span> + splited[1] + <span class="string">&quot;_&quot;</span> + splited[0];</span><br><span class="line">			</span><br><span class="line">		    String rowkey = CounterUtils.generateSalt(rowkey1)+rowkey1;</span><br><span class="line"></span><br><span class="line">			 Text k2 = new Text(rowkey);</span><br><span class="line">			// 第三个为value值</span><br><span class="line">			 TestWritable v2 = new TestWritable(splited[2]);</span><br><span class="line"></span><br><span class="line">			context.write(k2, v2);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	// 构建reduce类</span><br><span class="line">	public static class TestReduce extends Reducer&amp;lt;Text, TestWritable, Text, TestWritable&amp;gt; &#123;</span><br><span class="line"></span><br><span class="line">		public void reduce(Text k2, Iterable&amp;lt;TestWritable&amp;gt; v2s, Context context)</span><br><span class="line">				throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">			String value;</span><br><span class="line"></span><br><span class="line">			// 循环所有的key值和values值</span><br><span class="line">			<span class="keyword">for</span> (TestWritable testWritable : v2s) &#123;</span><br><span class="line"></span><br><span class="line">				value = testWritable.value;</span><br><span class="line"></span><br><span class="line">				TestWritable v3 = new TestWritable(value);</span><br><span class="line">				context.write(k2, v3);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	// main方法启动</span><br><span class="line">	public static void main(String[] args) throws IOException, Exception &#123;</span><br><span class="line">		ToolRunner.run(new DateConversion(), args);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public int run(String[] args) throws Exception &#123;</span><br><span class="line">		Configuration conf = new Configuration();</span><br><span class="line">		conf.set(<span class="string">&quot;mapred.textoutputformat.separator&quot;</span>, <span class="string">&quot;,&quot;</span>);</span><br><span class="line">	</span><br><span class="line">		String[] argArray = new GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line"></span><br><span class="line">		Job job = Job.getInstance(conf, <span class="string">&quot;Test&quot;</span>);</span><br><span class="line">		FileSystem fs = FileSystem.get(new URI(args[1]), conf);</span><br><span class="line">		fs.delete(new Path(args[1]));</span><br><span class="line">		job.setJarByClass(DateConversion.class);</span><br><span class="line">		job.setMapperClass(TestMap.class);</span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(TestWritable.class);</span><br><span class="line">		job.setReducerClass(TestReduce.class);</span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(TestWritable.class);</span><br><span class="line">		job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">		job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line">		job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">		<span class="built_in">return</span> 0;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	static class TestWritable implements Writable &#123;</span><br><span class="line"></span><br><span class="line">		String value;</span><br><span class="line"></span><br><span class="line">		public TestWritable(String value) &#123;</span><br><span class="line">			this.value = value;</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// 无参构造方法public class UserBean implements Writable</span><br><span class="line">		// 这个应该是在自定义writable的时候需要注意，反射过程中需要调用无参构造。</span><br><span class="line">		public <span class="function"><span class="title">TestWritable</span></span>() &#123;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		public void readFields(DataInput <span class="keyword">in</span>) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">			this.value = in.readUTF();</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		public void write(DataOutput out) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">			out.writeUTF(value);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		public String <span class="function"><span class="title">toString</span></span>() &#123;</span><br><span class="line">			<span class="built_in">return</span> value;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="生成文件的形式为"><a href="#生成文件的形式为" class="headerlink" title="生成文件的形式为"></a>生成文件的形式为</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">000LOOK_TIMES_503003049082486784_503003525400236039,3</span><br><span class="line">000LOOK_TIMES_503003049082486784_503003558279393283,1</span><br><span class="line">001LOOK_TIMES_503003049099264000_503003517552689161,6</span><br><span class="line">002LOOK_TIMES_503003049099264000_503003586590937094,2</span><br><span class="line">002LOOK_TIMES_503003049099264000_503003611433799685,1</span><br><span class="line">002LOOK_TIMES_503003049099264000_503003638604505093,1</span><br><span class="line">003LOOK_TIMES_503003049099264000_503003646833725450,2</span><br><span class="line">003LOOK_TIMES_503003049099264000_503254267948171264,2</span><br><span class="line">003LOOK_TIMES_503003049099264000_504261308741332992,1</span><br><span class="line">003LOOK_TIMES_503003049099264000_505510528722927616,1 </span><br></pre></td></tr></table></figure>

<h3 id="在进行另一个MR操作将生成的文件导入到Hbase表中"><a href="#在进行另一个MR操作将生成的文件导入到Hbase表中" class="headerlink" title="在进行另一个MR操作将生成的文件导入到Hbase表中"></a>在进行另一个MR操作将生成的文件导入到Hbase表中</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * </span><br><span class="line">* &amp;lt;p&amp;gt;Title: GeneratorHFile&amp;lt;/p&amp;gt;  </span><br><span class="line">* &amp;lt;p&amp;gt;Description: 先把文件rowkey加盐成文件，根据rowkey ASC 排序 上传HDFS(排序在hive总进行)</span><br><span class="line">* insert overwrite  directory <span class="string">&#x27;/dmp_operator1/test/&#x27;</span></span><br><span class="line">  row format delimited</span><br><span class="line">  fields terminated by <span class="string">&#x27;,&#x27;</span></span><br><span class="line">  select * from <span class="built_in">test</span>  order by id asc;</span><br><span class="line">* &amp;lt;/p&amp;gt;  </span><br><span class="line">* @author zhangshuai  </span><br><span class="line">* @date 2018年12月19日</span><br><span class="line"> */</span><br><span class="line">public class GeneratorHFile extends Mapper&amp;lt;LongWritable, Text, ImmutableBytesWritable, Put&amp;gt; &#123;</span><br><span class="line">	</span><br><span class="line">	private static Logger logger = Logger.getLogger(GeneratorHFile.class); </span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">	</span><br><span class="line">	protected void map(LongWritable Key, Text Value,</span><br><span class="line">			Mapper&amp;lt;LongWritable, Text, ImmutableBytesWritable, Put&amp;gt;.Context context)</span><br><span class="line">			throws IOException, InterruptedException &#123;</span><br><span class="line">		</span><br><span class="line">		//切分导入的数据</span><br><span class="line">		String Values=Value.toString();</span><br><span class="line">		String[] Lines=Values.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		</span><br><span class="line">		String Rowkey=Lines[0];</span><br><span class="line">		</span><br><span class="line">		Long ColValue=Long.valueOf(Lines[1]);</span><br><span class="line">		//拼装rowkey和put;</span><br><span class="line">		ImmutableBytesWritable PutRowkey=new ImmutableBytesWritable(Bytes.toBytes(Rowkey));</span><br><span class="line">		Put put=new Put(Bytes.toBytes(Rowkey));</span><br><span class="line">		put.addColumn(Bytes.toBytes(<span class="string">&quot;cf1&quot;</span>), Bytes.toBytes(<span class="string">&quot;cnt&quot;</span>), Bytes.toBytes(ColValue));</span><br><span class="line">		</span><br><span class="line">		context.write(PutRowkey,put);</span><br><span class="line">		</span><br><span class="line">		</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.jiatui.bigdata;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.Admin;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.hbase.client.HTable;</span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.client.Table;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line">import com.jiatui.bigdata.util.ScheduleKerBeros;</span><br><span class="line"></span><br><span class="line">public class GenerateHFileDriver &#123;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">		ScheduleKerBeros scheduleKerBeros = new ScheduleKerBeros();</span><br><span class="line">		scheduleKerBeros.scheduled();</span><br><span class="line"></span><br><span class="line">		/**</span><br><span class="line">		 * 获取Hbase配置，创建连接到目标表，表在Shell中已经创建好，建表语句create</span><br><span class="line">		 * <span class="string">&#x27;counter_sys.counter_tbl&#x27;</span>,<span class="string">&#x27;cf1&#x27;</span>，这里注意HBase对大小写很敏感</span><br><span class="line">		 */</span><br><span class="line"></span><br><span class="line">		Configuration conf = HBaseConfiguration.create();</span><br><span class="line">		conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;192.168.103.3,192.168.103.4,192.168.103.5&quot;</span>);</span><br><span class="line">		conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">		conf.addResource(<span class="string">&quot;/home/dmp_operator1/tickets/hbase-site.xml&quot;</span>);</span><br><span class="line">		conf.addResource(<span class="string">&quot;/home/dmp_operator1/tickets/core-site.xml&quot;</span>);</span><br><span class="line">		conf.addResource(<span class="string">&quot;/home/dmp_operator1/tickets/hdfs-site.xml&quot;</span>);</span><br><span class="line">		conf.set(<span class="string">&quot;zookeeper.znode.parent&quot;</span>, <span class="string">&quot;/hbase-secure&quot;</span>);</span><br><span class="line"></span><br><span class="line">		conf.set(<span class="string">&quot;mapreduce.input.fileinputformat.split.maxsize&quot;</span>, String.valueOf(64 * 1024 * 1024));</span><br><span class="line">		conf.set(<span class="string">&quot;mapred.min.split.size&quot;</span>, String.valueOf(64 * 1024 * 1024));</span><br><span class="line">		conf.set(<span class="string">&quot;mapreduce.input.fileinputformat.split.minsize.per.node&quot;</span>, String.valueOf(64 * 1024 * 1024));</span><br><span class="line">		conf.set(<span class="string">&quot;mapreduce.input.fileinputformat.split.minsize.per.rack&quot;</span>, String.valueOf(64 * 1024 * 1024));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		Connection conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">		Table table = conn.getTable(TableName.valueOf(<span class="string">&quot;counter_sys:counter_tbl&quot;</span>));</span><br><span class="line">		Admin admin = conn.getAdmin();</span><br><span class="line"></span><br><span class="line">		final String InputFile = <span class="string">&quot;hdfs://DATASEA/user/hbase/test/input&quot;</span>;</span><br><span class="line">		final String OutputFile = <span class="string">&quot;hdfs://DATASEA/user/hbase/test/output&quot;</span>;</span><br><span class="line">		final Path OutputPath = new Path(OutputFile);</span><br><span class="line"></span><br><span class="line">		// 设置相关类名</span><br><span class="line">		Job job = Job.getInstance(conf, <span class="string">&quot;counter_sys:counter_tbl&quot;</span>);</span><br><span class="line">		job.setJarByClass(GenerateHFileDriver.class);</span><br><span class="line">		job.setMapperClass(GeneratorHFile.class);</span><br><span class="line">		job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">		job.setMapOutputValueClass(Put.class);</span><br><span class="line"></span><br><span class="line">		// 设置文件的输入路径和输出路径</span><br><span class="line">		job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">		job.setOutputFormatClass(HFileOutputFormat2.class);</span><br><span class="line">		FileInputFormat.setInputPaths(job, InputFile);</span><br><span class="line">		FileOutputFormat.setOutputPath(job, OutputPath);</span><br><span class="line"></span><br><span class="line">		// 配置MapReduce作业，以执行增量加载到给定表中。</span><br><span class="line">		HFileOutputFormat2.configureIncrementalLoad(job, table,</span><br><span class="line">				conn.getRegionLocator(TableName.valueOf(<span class="string">&quot;counter_sys:counter_tbl&quot;</span>)));</span><br><span class="line"></span><br><span class="line">		// MapReduce作业完成，告知RegionServers在哪里找到这些文件,将文件加载到HBase中</span><br><span class="line">		<span class="keyword">if</span> (job.waitForCompletion(<span class="literal">true</span>)) &#123;</span><br><span class="line">			LoadIncrementalHFiles Loader = new LoadIncrementalHFiles(conf);</span><br><span class="line">			Loader.doBulkLoad(OutputPath, admin, table,</span><br><span class="line">					conn.getRegionLocator(TableName.valueOf(<span class="string">&quot;counter_sys:counter_tbl&quot;</span>)));</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import com.sun.tools.extcheck.Main;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.HColumnDescriptor;</span><br><span class="line">import org.apache.hadoop.hbase.HTableDescriptor;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.hbase.client.HBaseAdmin;</span><br><span class="line">import org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import java.time.LocalTime;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * </span><br><span class="line"> * &amp;lt;p&amp;gt;</span><br><span class="line"> * Title: ScheduleTest</span><br><span class="line"> * &amp;lt;/p&amp;gt;</span><br><span class="line"> * &amp;lt;p&amp;gt;</span><br><span class="line"> * Description:</span><br><span class="line"> * &amp;lt;/p&amp;gt;</span><br><span class="line"> * </span><br><span class="line"> * @author zhangshuai</span><br><span class="line"> * @date 2018年12月6日</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class ScheduleKerBeros &#123;</span><br><span class="line">	private static final Logger logger = LoggerFactory.getLogger(ScheduleKerBeros.class);</span><br><span class="line"></span><br><span class="line">	/**</span><br><span class="line">	 * </span><br><span class="line">	 * &amp;lt;p&amp;gt;Title: scheduled&amp;lt;/p&amp;gt;  </span><br><span class="line">	 * &amp;lt;p&amp;gt;Description: &amp;lt;/p&amp;gt;</span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	public void <span class="function"><span class="title">scheduled</span></span>() &#123;</span><br><span class="line"></span><br><span class="line">		logger.info(<span class="string">&quot;开始授权...&quot;</span>);</span><br><span class="line">	    //System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, this.getClass().getResource(<span class="string">&quot;/krb5.conf&quot;</span>).getPath());</span><br><span class="line">	    System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;/etc/krb5.conf&quot;</span>);</span><br><span class="line">		try &#123;</span><br><span class="line">			logger.info(<span class="string">&quot;授权中1...&quot;</span>);</span><br><span class="line">			UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;hbase/dev-bg-m01@DATASEA.COM&quot;</span>,<span class="string">&quot;/etc/security/keytabs/hbase.service.keytab&quot;</span>);</span><br><span class="line">		//UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;dmp_operator1@DATASEA.COM&quot;</span>,this.getClass().getResource(<span class="string">&quot;/dmp_operator1.keytab&quot;</span>).getPath());</span><br><span class="line">			</span><br><span class="line">			</span><br><span class="line">			logger.info(<span class="string">&quot;授权成功1...&quot;</span>);</span><br><span class="line"></span><br><span class="line">		&#125; catch (Exception e) &#123;</span><br><span class="line">			</span><br><span class="line">		&#125;</span><br><span class="line">	        </span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring mvc 框架定时刷新kerberos认证票据</title>
    <url>/2019/05/02/7/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.XXX.counter.listener;</span><br><span class="line"></span><br><span class="line">import javax.servlet.ServletContextEvent;</span><br><span class="line">import javax.servlet.ServletContextListener;</span><br><span class="line">import java.util.Timer;  </span><br><span class="line">public class TicketScanerListener implements ServletContextListener &#123;</span><br><span class="line">	private Object lock = new Object();  </span><br><span class="line">	public void contextDestroyed(ServletContextEvent arg0) &#123;</span><br><span class="line">		System.out.println(<span class="string">&quot;web应用关闭...&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public void contextInitialized(ServletContextEvent arg0) &#123;</span><br><span class="line">		System.out.println(<span class="string">&quot;web应用初始化...&quot;</span>);</span><br><span class="line">		// 创建定时器</span><br><span class="line">		Timer timer = new Timer();</span><br><span class="line">		// 每隔30秒就定时执行任务</span><br><span class="line">		timer.schedule(new TicketScanerTask(lock), 0, 1000 * 1000);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.xxx.counter.listener;</span><br><span class="line">  </span><br><span class="line">import java.util.TimerTask;</span><br><span class="line"></span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.security.UserGroupInformation;  </span><br><span class="line">/** </span><br><span class="line"> * 定时器，定义定时任务的具体内容 </span><br><span class="line"> */  </span><br><span class="line">public class TicketScanerTask extends TimerTask&#123;  </span><br><span class="line">	private static Logger logger = Logger.getLogger(TicketScanerTask.class); </span><br><span class="line">    // 存储传递过来的锁  </span><br><span class="line">    private Object lock;  </span><br><span class="line">    // 构造方法  </span><br><span class="line">    TicketScanerTask( Object lock)&#123;  </span><br><span class="line">        this.lock = lock;  </span><br><span class="line">    &#125;  </span><br><span class="line">    </span><br><span class="line">    private static Configuration conf = null;</span><br><span class="line">    public static String principal = <span class="string">&quot;dmp_operator1@DATASEA.COM&quot;</span>;</span><br><span class="line">    public static String keytabPath = <span class="string">&quot;dmp_operator1.keytab&quot;</span>;</span><br><span class="line">    @Override  </span><br><span class="line">    public void  <span class="function"><span class="title">run</span></span>() &#123;  </span><br><span class="line">        // 考虑到多线程的情况，这里必须要同步  </span><br><span class="line">        synchronized (lock)&#123;  </span><br><span class="line"></span><br><span class="line">        	logger.info(<span class="string">&quot;TicketScanerTask......&quot;</span>);</span><br><span class="line"></span><br><span class="line">            	System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>,this.getClass().getResource(<span class="string">&quot;/krb5.conf&quot;</span>).getPath());</span><br><span class="line">            </span><br><span class="line">                try &#123;</span><br><span class="line">                    UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;dmp_operator1@DATASEA.COM&quot;</span>, this.getClass().getResource(<span class="string">&quot;/dmp_operator1.keytab&quot;</span>).getPath());</span><br><span class="line"></span><br><span class="line">                &#125; catch (Exception e) &#123;</span><br><span class="line">    			// TODO Auto-generated catch block</span><br><span class="line">    			e.printStackTrace();</span><br><span class="line">    			logger.error(<span class="string">&quot;授权失败...&quot;</span>);</span><br><span class="line">    		&#125;</span><br><span class="line">              </span><br><span class="line">        	</span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;listener&gt;</span><br><span class="line">    &lt;listener-class&gt;com.xxx.counter.listener.TicketScanerListener&lt;/listener-class&gt;</span><br><span class="line">  &lt;/listener&gt; </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.xxx.counter.util;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line">public class HbaseConnection &#123;</span><br><span class="line">	private static final Logger logger = LoggerFactory.getLogger(HbaseConnection.class);</span><br><span class="line"></span><br><span class="line">	public static Configuration conf = null;</span><br><span class="line">	public static Connection connection = null;</span><br><span class="line">	</span><br><span class="line">	public Connection getConnection () throws IOException &#123;</span><br><span class="line">		conf = HBaseConfiguration.create();</span><br><span class="line"></span><br><span class="line">	    conf.addResource(this.getClass().getResource(<span class="string">&quot;/hbase-site.xml&quot;</span>).getPath());</span><br><span class="line">	    conf.addResource(this.getClass().getResource(<span class="string">&quot;/core-site.xml&quot;</span>).getPath());</span><br><span class="line">	    conf.addResource(this.getClass().getResource(<span class="string">&quot;/hdfs-site.xml&quot;</span>).getPath());</span><br><span class="line">	    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;192.168.103.3,192.168.103.4,192.168.103.5&quot;</span>);</span><br><span class="line">	    conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">	    conf.set(<span class="string">&quot;zookeeper.znode.parent&quot;</span>, <span class="string">&quot;/hbase-secure&quot;</span>);</span><br><span class="line">	    conf.setLong(<span class="string">&quot;hbase.rpc.timeout&quot;</span>, 30000);</span><br><span class="line">	    connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">	    <span class="built_in">return</span> connection;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Scala 获取自然日 属于每周的第一天和每周的最后一天，每月的第一天和每月的最后一天</title>
    <url>/2019/04/02/3/</url>
    <content><![CDATA[<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.xxxx.bigdata</span><br><span class="line">import java.util.Calendar</span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext</span><br><span class="line">import org.apache.spark.sql.types.StructType</span><br><span class="line">import org.apache.spark.sql.types.StructField</span><br><span class="line">import org.apache.spark.sql.types.StringType</span><br><span class="line"></span><br><span class="line">object Date_week_month &#123;</span><br><span class="line">  def getNowWeekStart(logdate: String) = &#123;</span><br><span class="line">    var period: String = <span class="string">&quot;&quot;</span></span><br><span class="line">    var cal: Calendar = Calendar.getInstance();</span><br><span class="line">    var df: SimpleDateFormat = new SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd&quot;</span>);</span><br><span class="line"></span><br><span class="line">    cal.setTime(df.parse(logdate))</span><br><span class="line"></span><br><span class="line">    var d = 0</span><br><span class="line">    <span class="keyword">if</span> (cal.get(Calendar.DAY_OF_WEEK) == 1) &#123;</span><br><span class="line">      d = -6</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      d = 2 - cal.get(Calendar.DAY_OF_WEEK)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cal.add(Calendar.DAY_OF_WEEK, d)</span><br><span class="line"></span><br><span class="line">    val startdate = new SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd&quot;</span>).format(cal.getTime())</span><br><span class="line"></span><br><span class="line">    cal.add(Calendar.DAY_OF_WEEK, 6)</span><br><span class="line"></span><br><span class="line">    val enddate = new SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd&quot;</span>).format(cal.getTime())</span><br><span class="line"></span><br><span class="line">    (startdate, enddate)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getNowMonthStart(logdate: String) = &#123;</span><br><span class="line">    var cal: Calendar = Calendar.getInstance();</span><br><span class="line">    var df: SimpleDateFormat = new SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd&quot;</span>);</span><br><span class="line">    cal.setTime(df.parse(logdate))</span><br><span class="line">    cal.set(Calendar.DATE, 1)</span><br><span class="line">    val monthstart = df.format(cal.getTime())</span><br><span class="line"></span><br><span class="line">    cal.set(Calendar.DATE, 1)</span><br><span class="line">    cal.roll(Calendar.DATE, -1)</span><br><span class="line">    val monthend = df.format(cal.getTime())</span><br><span class="line"></span><br><span class="line">    (monthstart, monthend)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val conf = new SparkConf().setAppName(<span class="string">&quot;test&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.default.parallelism&quot;</span>, <span class="string">&quot;200&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="string">&quot;200&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;zookeeper.znode.parent&quot;</span>, <span class="string">&quot;/hbase-secure&quot;</span>)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val hiveContext = new HiveContext(sc)</span><br><span class="line"></span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>)</span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.max.dynamic.partitions.pernode&quot;</span>, <span class="string">&quot;2000&quot;</span>)</span><br><span class="line">    hiveContext.setConf(<span class="string">&quot;hive.exec.max.dynamic.partitions&quot;</span>, <span class="string">&quot;5000&quot;</span>)</span><br><span class="line">    sc.setLogLevel(<span class="string">&quot;ERROR&quot;</span>)</span><br><span class="line">    val startTime = <span class="string">&quot;2018-01-01&quot;</span></span><br><span class="line"></span><br><span class="line">    val endTime = <span class="string">&quot;2021-12-31&quot;</span></span><br><span class="line"></span><br><span class="line">    val dateFormat = new SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val dateFiled = Calendar.DAY_OF_MONTH</span><br><span class="line"></span><br><span class="line">    var beginDate = dateFormat.parse(startTime)</span><br><span class="line"></span><br><span class="line">    val endDate = dateFormat.parse(endTime)</span><br><span class="line"></span><br><span class="line">    val calendar = Calendar.getInstance()</span><br><span class="line"></span><br><span class="line">    calendar.setTime(beginDate)</span><br><span class="line"></span><br><span class="line">    val dateArray: ArrayBuffer[String] = ArrayBuffer()</span><br><span class="line"></span><br><span class="line">    val data = new ArrayBuffer[Row]</span><br><span class="line">    <span class="keyword">while</span> (beginDate.compareTo(endDate) &lt;= 0) &#123;</span><br><span class="line"></span><br><span class="line">      val login_time = dateFormat.format(beginDate)</span><br><span class="line"></span><br><span class="line">      val starttime = getNowWeekStart(login_time)._1</span><br><span class="line">      val endtime = getNowWeekStart(login_time)._2</span><br><span class="line"></span><br><span class="line">      val monthstart = getNowMonthStart(login_time)._1</span><br><span class="line">      val monthend = getNowMonthStart(login_time)._2</span><br><span class="line"></span><br><span class="line">      data += Row(login_time, starttime, endtime, monthstart, monthend)</span><br><span class="line"></span><br><span class="line">      calendar.add(dateFiled, 1)</span><br><span class="line">      beginDate = calendar.getTime</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val rdd = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line">    val stuct = StructType(</span><br><span class="line">      Array(</span><br><span class="line">        StructField(<span class="string">&quot;login_time&quot;</span>, StringType),</span><br><span class="line">        StructField(<span class="string">&quot;starttime&quot;</span>, StringType),</span><br><span class="line">        StructField(<span class="string">&quot;endtime&quot;</span>, StringType),</span><br><span class="line">        StructField(<span class="string">&quot;monthstart&quot;</span>, StringType),</span><br><span class="line">        StructField(<span class="string">&quot;monthend&quot;</span>, StringType)))</span><br><span class="line"></span><br><span class="line">    val test1 = hiveContext.createDataFrame(rdd, stuct)</span><br><span class="line"></span><br><span class="line">    val tmpTable = <span class="string">&quot;dateTable&quot;</span></span><br><span class="line"></span><br><span class="line">    test1.registerTempTable(tmpTable)</span><br><span class="line"></span><br><span class="line">    val sql = s<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">      select count(*) from </span></span><br><span class="line"><span class="string">		      <span class="variable">$tmpTable</span> </span></span><br><span class="line"><span class="string">      &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;sql = &quot;</span> + sql)</span><br><span class="line"></span><br><span class="line">    hiveContext.sql(sql).show();</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Hive-SQL-实现以上方法先循环取出，日期之间所有的日期"><a href="#Hive-SQL-实现以上方法先循环取出，日期之间所有的日期" class="headerlink" title="Hive SQL 实现以上方法先循环取出，日期之间所有的日期"></a>Hive SQL 实现以上方法先循环取出，日期之间所有的日期</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">START_DAY=<span class="string">&#x27;2018-01-01&#x27;</span></span><br><span class="line">END_DAY=<span class="string">&#x27;2021-12-31&#x27;</span></span><br><span class="line"> </span><br><span class="line">initSec=`date -d <span class="variable">$START_DAY</span> +%s`</span><br><span class="line">finiSec=`date -d <span class="variable">$END_DAY</span> +%s`</span><br><span class="line"><span class="keyword">for</span> ((i=<span class="variable">$initSec</span>; i &lt;=<span class="variable">$finiSec</span>; i+=24 * 3600 ));<span class="keyword">do</span></span><br><span class="line">        cur_day=`date -d <span class="string">&quot;1970-1-1 UTC <span class="variable">$i</span> seconds&quot;</span> +%F`</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$cur_day</span>,,,,&quot;</span>&gt;&gt; testa</span><br><span class="line"> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="生成了-这样的文件"><a href="#生成了-这样的文件" class="headerlink" title="生成了 这样的文件"></a>生成了 这样的文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2018-01-01,,,,</span><br><span class="line">2018-01-02,,,,</span><br><span class="line">2018-01-03,,,,</span><br><span class="line">2018-01-04,,,,</span><br><span class="line">2018-01-05,,,,</span><br><span class="line">2018-01-06,,,,</span><br><span class="line">2018-01-07,,,,</span><br><span class="line">.............</span><br></pre></td></tr></table></figure>
<h3 id="然后load到Hive表中"><a href="#然后load到Hive表中" class="headerlink" title="然后load到Hive表中"></a>然后load到Hive表中</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE <span class="built_in">test</span> (dt String,WEEK_START string,WEEK_END string,MONTH_START string,MONTH_END string)row format delimited fields TERMINATED BY <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="执行SQL"><a href="#执行SQL" class="headerlink" title="执行SQL"></a>执行SQL</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">insert overwrite table <span class="built_in">test</span> select  dt,date_sub(dt,cast(date_format(dt,<span class="string">&#x27;u&#x27;</span>) as int)-1) as start_time,</span><br><span class="line">date_sub(dt,cast(date_format(dt,<span class="string">&#x27;u&#x27;</span>) as int)-7) as end_time,trunc(dt,<span class="string">&#x27;MM&#x27;</span>),last_day(dt) from <span class="built_in">test</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">| 2021-02-24  | 2021-02-22  | 2021-02-28  | 2021-02-01  | 2021-02-28  |</span><br><span class="line">| 2021-02-25  | 2021-02-22  | 2021-02-28  | 2021-02-01  | 2021-02-28  |</span><br><span class="line">| 2021-02-26  | 2021-02-22  | 2021-02-28  | 2021-02-01  | 2021-02-28  |</span><br><span class="line">| 2021-02-27  | 2021-02-22  | 2021-02-28  | 2021-02-01  | 2021-02-28  |</span><br><span class="line">| 2021-02-28  | 2021-02-22  | 2021-02-28  | 2021-02-01  | 2021-02-28  |</span><br><span class="line">| 2021-03-01  | 2021-03-01  | 2021-03-07  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-02  | 2021-03-01  | 2021-03-07  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-03  | 2021-03-01  | 2021-03-07  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-04  | 2021-03-01  | 2021-03-07  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-05  | 2021-03-01  | 2021-03-07  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-06  | 2021-03-01  | 2021-03-07  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-07  | 2021-03-01  | 2021-03-07  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-08  | 2021-03-08  | 2021-03-14  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-09  | 2021-03-08  | 2021-03-14  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-10  | 2021-03-08  | 2021-03-14  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-11  | 2021-03-08  | 2021-03-14  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-12  | 2021-03-08  | 2021-03-14  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-13  | 2021-03-08  | 2021-03-14  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-14  | 2021-03-08  | 2021-03-14  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-15  | 2021-03-15  | 2021-03-21  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-16  | 2021-03-15  | 2021-03-21  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-17  | 2021-03-15  | 2021-03-21  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-18  | 2021-03-15  | 2021-03-21  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-19  | 2021-03-15  | 2021-03-21  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-20  | 2021-03-15  | 2021-03-21  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-21  | 2021-03-15  | 2021-03-21  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-22  | 2021-03-22  | 2021-03-28  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-23  | 2021-03-22  | 2021-03-28  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-24  | 2021-03-22  | 2021-03-28  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-25  | 2021-03-22  | 2021-03-28  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-26  | 2021-03-22  | 2021-03-28  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-27  | 2021-03-22  | 2021-03-28  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-28  | 2021-03-22  | 2021-03-28  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-29  | 2021-03-29  | 2021-04-04  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-30  | 2021-03-29  | 2021-04-04  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-03-31  | 2021-03-29  | 2021-04-04  | 2021-03-01  | 2021-03-31  |</span><br><span class="line">| 2021-04-01  | 2021-03-29  | 2021-04-04  | 2021-04-01  | 2021-04-30  |</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark1.X操作DataFrame示例</title>
    <url>/2019/04/02/8/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;id&quot;</span>:1, <span class="string">&quot;name&quot;</span>:<span class="string">&quot;Ganymede&quot;</span>, <span class="string">&quot;age&quot;</span>:32&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&quot;id&quot;</span>:2, <span class="string">&quot;name&quot;</span>:<span class="string">&quot;Lilei&quot;</span>, <span class="string">&quot;age&quot;</span>:19&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&quot;id&quot;</span>:3, <span class="string">&quot;name&quot;</span>:<span class="string">&quot;Lily&quot;</span>, <span class="string">&quot;age&quot;</span>:25&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&quot;id&quot;</span>:4, <span class="string">&quot;name&quot;</span>:<span class="string">&quot;Hanmeimei&quot;</span>, <span class="string">&quot;age&quot;</span>:25&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&quot;id&quot;</span>:5, <span class="string">&quot;name&quot;</span>:<span class="string">&quot;Lucy&quot;</span>, <span class="string">&quot;age&quot;</span>:37&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&quot;id&quot;</span>:6, <span class="string">&quot;name&quot;</span>:<span class="string">&quot;Tom&quot;</span>, <span class="string">&quot;age&quot;</span>:27&#125;</span><br><span class="line"></span><br><span class="line">1,Ganymede,32</span><br><span class="line"></span><br><span class="line">2, Lilei, 19</span><br><span class="line"></span><br><span class="line">3, Lily, 25</span><br><span class="line"></span><br><span class="line">4, Hanmeimei, 25</span><br><span class="line"></span><br><span class="line">5, Lucy, 37</span><br><span class="line"></span><br><span class="line">6, wcc, 4</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package DataCleaning</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.&#123;Row, SQLContext&#125;</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object DataFrameTest &#123;</span><br><span class="line">  <span class="keyword">case</span> class People(id: Int, name: String, age: Int)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName(<span class="string">&quot;DataFrameTest&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val sqlContxt = new SQLContext(sc)</span><br><span class="line"></span><br><span class="line">    val dfsql = sqlContxt.read.json(<span class="string">&quot;people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val dftxt = sc.textFile(<span class="string">&quot;people.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //映射表</span><br><span class="line">    dfsql.registerTempTable(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line">    dfsql.show()</span><br><span class="line">    dfsql.printSchema()</span><br><span class="line"></span><br><span class="line">    dfsql.select(dfsql.col(<span class="string">&quot;id&quot;</span>),dfsql.col(<span class="string">&quot;name&quot;</span>)).foreach( x =&amp;gt; &#123;</span><br><span class="line"></span><br><span class="line">      println(x.get(0),x.get(1))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    dfsql.select(dfsql.col(<span class="string">&quot;id&quot;</span>),dfsql.col(<span class="string">&quot;name&quot;</span>)).foreachPartition( Iterator =&amp;gt;</span><br><span class="line">    Iterator.foreach(x =&amp;gt; &#123;</span><br><span class="line">      println(x.getAs(<span class="string">&quot;id&quot;</span>),x.getAs(<span class="string">&quot;name&quot;</span>))</span><br><span class="line">    &#125;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sqlContxt.sql(<span class="string">&quot;select id,name from people&quot;</span>).foreach( x =&amp;gt; &#123;</span><br><span class="line">      println(<span class="string">&quot;SQL打印出来的````&quot;</span>+x.get(0),x.get(1))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    sqlContxt.sql(<span class="string">&quot;select id,name from people&quot;</span>).foreachPartition(Iterable =&amp;gt; &#123;</span><br><span class="line">      Iterable.foreach( x =&amp;gt; &#123;</span><br><span class="line">        println(<span class="string">&quot;SQL打印出来的````&quot;</span>+x.getAs(<span class="string">&quot;id&quot;</span>),x.getAs(<span class="string">&quot;name&quot;</span>))</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val peopleRowRdd = dftxt.map(x =&amp;gt; x.split(<span class="string">&quot;,&quot;</span>)).map(data =&amp;gt;&#123;</span><br><span class="line">      val id = data(0).trim().toInt</span><br><span class="line">      val name = data(1).trim()</span><br><span class="line">      val age = data(2).trim().toInt</span><br><span class="line">      Row(id,name,age)</span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line">    val structType = StructType(Array(</span><br><span class="line">      StructField(<span class="string">&quot;id&quot;</span>, IntegerType, <span class="literal">true</span>),</span><br><span class="line">      StructField(<span class="string">&quot;name&quot;</span>,StringType,<span class="literal">true</span> ),</span><br><span class="line">      StructField(<span class="string">&quot;age&quot;</span>, IntegerType, <span class="literal">true</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    val df = sqlContxt.createDataFrame(peopleRowRdd, structType)</span><br><span class="line"></span><br><span class="line">    df.registerTempTable(<span class="string">&quot;people1&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;-------------------&quot;</span>)</span><br><span class="line">    df.show()</span><br><span class="line">    df.printSchema()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val people = sc.textFile(<span class="string">&quot;people.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val peopleRDD = people.map &#123; x =&amp;gt; x.split(<span class="string">&quot;,&quot;</span>) &#125;.map ( data =&amp;gt;</span><br><span class="line">    &#123;</span><br><span class="line">      People(data(0).trim().toInt, data(1).trim(), data(2).trim().toInt)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    //这里需要隐式转换一把</span><br><span class="line">    import sqlContxt.implicits._</span><br><span class="line">    val dfDF = peopleRDD.toDF()</span><br><span class="line">    dfDF.registerTempTable(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;-------------case class反射来映射注册临时表-----------------------&quot;</span>)</span><br><span class="line">    dfDF.show()</span><br><span class="line">    dfDF.printSchema()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka HA Kafka一致性重要机制之ISR(kafka replica)</title>
    <url>/2019/04/02/9/</url>
    <content><![CDATA[<h3 id="一、kafka-replica"><a href="#一、kafka-replica" class="headerlink" title="一、kafka replica"></a>一、kafka replica</h3><p> 当某个topic的replication-factor为N且N大于1时，每个Partition都会有N个副本(Replica)。kafka的replica包含leader与follower。<br> Replica的个数小于等于Broker的个数，也就是说，对于每个Partition而言，每个Broker上最多只会有一个Replica，因此可以使用Broker id 指定Partition的Replica。<br> 所有Partition的Replica默认情况会均匀分布到所有Broker上。</p>
<h3 id="二、Data-Replication如何Propagate-扩散出去-消息？"><a href="#二、Data-Replication如何Propagate-扩散出去-消息？" class="headerlink" title="二、Data Replication如何Propagate(扩散出去)消息？"></a>二、Data Replication如何Propagate(扩散出去)消息？</h3><p>每个Partition有一个leader与多个follower，producer往某个Partition中写入数据是，只会往leader中写入数据，然后数据才会被复制进其他的Replica中。<br>数据是由leader push过去还是有flower pull过来？<br>kafka是由follower周期性或者尝试去pull(拉)过来(其实这个过程与consumer消费过程非常相似)，写是都往leader上写，但是读并不是任意flower上读都行，读也只在leader上读，flower只是数据的一个备份，保证leader被挂掉后顶上来，并不往外提供服务。</p>
<h3 id="三、Data-Replication何时Commit？"><a href="#三、Data-Replication何时Commit？" class="headerlink" title="三、Data Replication何时Commit？"></a>三、Data Replication何时Commit？</h3><p>同步复制： 只有所有的follower把数据拿过去后才commit，一致性好，可用性不高。<br>异步复制： 只要leader拿到数据立即commit，等follower慢慢去复制，可用性高，立即返回，一致性差一些。<br>Commit：是指leader告诉客户端，这条数据写成功了。kafka尽量保证commit后立即leader挂掉，其他flower都有该条数据</p>
<h3 id="kafka不是完全同步，也不是完全异步，是一种ISR机制"><a href="#kafka不是完全同步，也不是完全异步，是一种ISR机制" class="headerlink" title="kafka不是完全同步，也不是完全异步，是一种ISR机制"></a>kafka不是完全同步，也不是完全异步，是一种ISR机制</h3><p> leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护<br> 如果一个flower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除<br> 当ISR中所有Replica都向Leader发送ACK时，leader才commit既然所有Replica都向Leader发送ACK时，leader才commit，那么flower怎么会leader落后太多？<br>producer往kafka中发送数据，不仅可以一次发送一条数据，还可以发送message的数组；批量发送，同步的时候批量发送，异步的时候本身就是就是批量；底层会有队列缓存起来，批量发送，对应broker而言，就会收到很多数据(假设1000)，这时候leader发现自己有1000条数据，flower只有500条数据，落后了500条数据，就把它从ISR中移除出去，这时候发现其他的flower与他的差距都很小，就等待；如果因为内存等原因，差距很大，就把它从ISR中移除出去。</p>
<h3 id="server配置"><a href="#server配置" class="headerlink" title="server配置"></a>server配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rerplica.lag.time.max.ms=10000</span><br><span class="line"> <span class="comment"># 如果leader发现flower超过10秒没有向它发起fech请求，那么leader考虑这个flower是不是程序出了点问题</span></span><br><span class="line"> <span class="comment"># 或者资源紧张调度不过来，它太慢了，不希望它拖慢后面的进度，就把它从ISR中移除。</span></span><br><span class="line"></span><br><span class="line"> rerplica.lag.max.messages=4000 <span class="comment"># 相差4000条就移除</span></span><br><span class="line"> <span class="comment"># flower慢的时候，保证高可用性，同时满足这两个条件后又加入ISR中，</span></span><br><span class="line"> <span class="comment"># 在可用性与一致性做了动态平衡   亮点</span></span><br></pre></td></tr></table></figure>
<h3 id="topic配置"><a href="#topic配置" class="headerlink" title="topic配置"></a>topic配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">min.insync.replicas=1 <span class="comment"># 需要保证ISR中至少有多少个replica</span></span><br></pre></td></tr></table></figure>

<h3 id="Producer配置"><a href="#Producer配置" class="headerlink" title="Producer配置"></a>Producer配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">request.required.asks=0</span><br><span class="line"> <span class="comment"># 0:相当于异步的，不需要leader给予回复，producer立即返回，发送就是成功,</span></span><br><span class="line">     那么发送消息网络超时或broker crash(1.Partition的Leader还没有commit消息 2.Leader与Follower数据不同步)，</span><br><span class="line">     既有可能丢失也可能会重发</span><br><span class="line"> <span class="comment"># 1：当leader接收到消息之后发送ack，丢会重发，丢的概率很小</span></span><br><span class="line"> <span class="comment"># -1：当所有的follower都同步消息成功后发送ack.  丢失消息可能性比较低</span></span><br></pre></td></tr></table></figure>

<h3 id="Data-Replication如何处理Replica恢复"><a href="#Data-Replication如何处理Replica恢复" class="headerlink" title="Data Replication如何处理Replica恢复"></a>Data Replication如何处理Replica恢复</h3><p>leader挂掉了，从它的follower中选举一个作为leader，并把挂掉的leader从ISR中移除，继续处理数据。一段时间后该leader重新启动了，它知道它之前的数据到哪里了，尝试获取它挂掉后leader处理的数据，获取完成后它就加入了ISR。</p>
<h3 id="Data-Replication如何处理Replica全部宕机"><a href="#Data-Replication如何处理Replica全部宕机" class="headerlink" title="Data Replication如何处理Replica全部宕机"></a>Data Replication如何处理Replica全部宕机</h3><p> 等待ISR中任一Replica恢复,并选它为Leader 等待时间较长,降低可用性或ISR中的所有Replica都无法恢复或者数据丢失,则该Partition将永不可用<br> 选择第一个恢复的Replica为新的Leader,无论它是否在ISR中并未包含所有已被之前Leader Commit过的消息,因此会造成数据丢失可用性较高</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark scala 抽取mysql数据 导入Hive</title>
    <url>/2019/03/02/4/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">driverClass=com.mysql.jdbc.Driver</span><br><span class="line">feeds.jdbcUrl=jdbc:mysql://172.16.XX.X:3306/feeds?useUnicode=<span class="literal">true</span>&amp;characterEncoding=utf8&amp;tinyInt1isBit=<span class="literal">false</span>&amp;autoReconnect=<span class="literal">true</span>&amp;useSSL=<span class="literal">false</span></span><br><span class="line">feeds.user=root</span><br><span class="line">feeds.password=Xingrui@jiatuimysql</span><br><span class="line"></span><br><span class="line">initialPoolSize=3</span><br><span class="line">minPoolSize=3</span><br><span class="line">acquireIncrement=3</span><br><span class="line">maxPoolSize=15</span><br><span class="line">maxIdleTime=10</span><br><span class="line">acquireRetryAttempts=30</span><br><span class="line">acquireRetryDelay=1000</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;project xmlns=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> xmlns:xsi=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span> xsi:schemaLocation=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;</span>&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line">  &lt;groupId&gt;bigdata_cardevent_etl&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;bigdata_cardevent_etl&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</span><br><span class="line">  &lt;name&gt;<span class="variable">$&#123;project.artifactId&#125;</span>&lt;/name&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">		&lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">		&lt;spark.version&gt;1.6.3&lt;/spark.version&gt;</span><br><span class="line">		&lt;mysql.version&gt;5.1.47&lt;/mysql.version&gt;</span><br><span class="line">		&lt;c3p0.version&gt;0.9.5-pre4&lt;/c3p0.version&gt;</span><br><span class="line">    	&lt;fastjson.version&gt;1.2.47&lt;/fastjson.version&gt;</span><br><span class="line">    	</span><br><span class="line">	&lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">	&lt;dependencies&gt;    </span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;<span class="variable">$&#123;spark.version&#125;</span>&lt;/version&gt;</span><br><span class="line">			&lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;<span class="variable">$&#123;spark.version&#125;</span>&lt;/version&gt;</span><br><span class="line">			&lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">		    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">		    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">		    &lt;version&gt;<span class="variable">$&#123;mysql.version&#125;</span>&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">      		&lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">      		&lt;artifactId&gt;fastjson&lt;/artifactId&gt;</span><br><span class="line">      		&lt;version&gt;<span class="variable">$&#123;fastjson.version&#125;</span>&lt;/version&gt;</span><br><span class="line">    	&lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		    &lt;groupId&gt;com.mchange&lt;/groupId&gt;</span><br><span class="line">		    &lt;artifactId&gt;c3p0&lt;/artifactId&gt;</span><br><span class="line">		    &lt;version&gt;<span class="variable">$&#123;c3p0.version&#125;</span>&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">	&lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">	&lt;build&gt;</span><br><span class="line">		&lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;</span><br><span class="line">		&lt;testSourceDirectory&gt;src/<span class="built_in">test</span>/scala&lt;/testSourceDirectory&gt;</span><br><span class="line">		&lt;plugins&gt;</span><br><span class="line">			&lt;!-- compiler插件, 设定JDK版本 --&gt;</span><br><span class="line">			&lt;plugin&gt;</span><br><span class="line">				&lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class="line">				&lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class="line">				&lt;version&gt;2.15.2&lt;/version&gt;</span><br><span class="line">				&lt;executions&gt;</span><br><span class="line">					&lt;execution&gt;</span><br><span class="line">						&lt;goals&gt;</span><br><span class="line">							&lt;goal&gt;compile&lt;/goal&gt;</span><br><span class="line">						&lt;/goals&gt;</span><br><span class="line">					&lt;/execution&gt;</span><br><span class="line">				&lt;/executions&gt;</span><br><span class="line">			&lt;/plugin&gt;</span><br><span class="line">			&lt;plugin&gt;</span><br><span class="line">				&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">				&lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">				&lt;version&gt;1.6&lt;/version&gt;</span><br><span class="line">				&lt;configuration&gt;</span><br><span class="line">					&lt;createDependencyReducedPom&gt;<span class="literal">true</span>&lt;/createDependencyReducedPom&gt;</span><br><span class="line">					&lt;<span class="built_in">source</span>&gt;1.8&lt;/<span class="built_in">source</span>&gt;</span><br><span class="line">					&lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">					&lt;filters&gt;</span><br><span class="line">						&lt;filter&gt;</span><br><span class="line">							&lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class="line">							&lt;excludes&gt;</span><br><span class="line">								&lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</span><br><span class="line">								&lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span><br><span class="line">								&lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span><br><span class="line">							&lt;/excludes&gt;</span><br><span class="line">						&lt;/filter&gt;</span><br><span class="line">					&lt;/filters&gt;</span><br><span class="line">				&lt;/configuration&gt;</span><br><span class="line">				&lt;executions&gt;</span><br><span class="line">					&lt;execution&gt;</span><br><span class="line">						&lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">						&lt;goals&gt;</span><br><span class="line">							&lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">						&lt;/goals&gt;</span><br><span class="line">						&lt;configuration&gt;</span><br><span class="line">							&lt;transformers&gt;</span><br><span class="line">								&lt;transformer</span><br><span class="line">									implementation=<span class="string">&quot;org.apache.maven.plugins.shade.resource.AppendingTransformer&quot;</span>&gt;</span><br><span class="line">									&lt;resource&gt;reference.conf&lt;/resource&gt;</span><br><span class="line">								&lt;/transformer&gt;</span><br><span class="line">								&lt;transformer</span><br><span class="line">									implementation=<span class="string">&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;</span> /&gt;</span><br><span class="line">								&lt;transformer</span><br><span class="line">									implementation=<span class="string">&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;</span>&gt;</span><br><span class="line">									&lt;mainClass&gt;com.jiatui.bigdata.format.sdklog.main.FormatSdkLogApp&lt;/mainClass&gt;</span><br><span class="line">								&lt;/transformer&gt;</span><br><span class="line">							&lt;/transformers&gt;</span><br><span class="line">						&lt;/configuration&gt;</span><br><span class="line">					&lt;/execution&gt;</span><br><span class="line">				&lt;/executions&gt;</span><br><span class="line">			&lt;/plugin&gt;</span><br><span class="line">		&lt;/plugins&gt;</span><br><span class="line">	&lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.jiatui.bigdata.util</span><br><span class="line"></span><br><span class="line">import com.mchange.v2.c3p0.ComboPooledDataSource</span><br><span class="line">import java.util.Properties</span><br><span class="line">import java.io.File</span><br><span class="line">import java.io.FileInputStream</span><br><span class="line">import java.sql.Connection</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line">import java.sql.ResultSet</span><br><span class="line">/**</span><br><span class="line"> * c3p0连接</span><br><span class="line"> */</span><br><span class="line">class DBUtilC3p0() extends Serializable&#123;</span><br><span class="line">      private val cpds: ComboPooledDataSource =new ComboPooledDataSource(<span class="literal">true</span>)</span><br><span class="line">      </span><br><span class="line">      private val prop=new Properties</span><br><span class="line">      </span><br><span class="line">&#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      val file: File = new File(<span class="string">&quot;c3p0.properties&quot;</span>)</span><br><span class="line">      val <span class="keyword">in</span> = new FileInputStream(file)</span><br><span class="line">      prop.load(<span class="keyword">in</span>)</span><br><span class="line">      println(<span class="string">&quot;prop:&quot;</span> + prop)</span><br><span class="line">      cpds.setJdbcUrl(prop.getProperty(<span class="string">&quot;feeds.jdbcUrl&quot;</span>))</span><br><span class="line">      cpds.setDriverClass(prop.getProperty(<span class="string">&quot;driverClass&quot;</span>))</span><br><span class="line">      cpds.setUser(prop.getProperty(<span class="string">&quot;feeds.user&quot;</span>))</span><br><span class="line">      cpds.setPassword(prop.getProperty(<span class="string">&quot;feeds.password&quot;</span>))</span><br><span class="line">      cpds.setMaxPoolSize(Integer.valueOf(prop.getProperty(<span class="string">&quot;maxPoolSize&quot;</span>)))</span><br><span class="line">      cpds.setMinPoolSize(Integer.valueOf(prop.getProperty(<span class="string">&quot;minPoolSize&quot;</span>)))</span><br><span class="line">      cpds.setAcquireIncrement(Integer.valueOf(prop.getProperty(<span class="string">&quot;acquireIncrement&quot;</span>)))</span><br><span class="line">      cpds.setInitialPoolSize(Integer.valueOf(prop.getProperty(<span class="string">&quot;initialPoolSize&quot;</span>)))</span><br><span class="line">      cpds.setMaxIdleTime(Integer.valueOf(prop.getProperty(<span class="string">&quot;maxIdleTime&quot;</span>)))</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      <span class="keyword">case</span> ex: Exception =&gt; ex.printStackTrace()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">      </span><br><span class="line">   </span><br><span class="line">      </span><br><span class="line">def getConnection: Connection = &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      cpds.getConnection()</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      <span class="keyword">case</span> ex: Exception =&gt;</span><br><span class="line">        ex.printStackTrace()</span><br><span class="line">        null</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">object DBUtilC3p0 &#123;</span><br><span class="line">  var dbUtilC3p0: DBUtilC3p0 = _</span><br><span class="line">  def getDBUtilC3p0(): DBUtilC3p0 = &#123;</span><br><span class="line">    synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (dbUtilC3p0 == null) &#123;</span><br><span class="line">        dbUtilC3p0 = new DBUtilC3p0()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    dbUtilC3p0</span><br><span class="line">  &#125;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.jiatui.bigdata.util</span><br><span class="line"></span><br><span class="line">import java.sql.ResultSet</span><br><span class="line">import java.sql.Connection</span><br><span class="line">import java.sql.PreparedStatement</span><br><span class="line">import java.util.concurrent.BlockingQueue</span><br><span class="line">import java.util.concurrent.LinkedBlockingQueue</span><br><span class="line">import java.sql.DriverManager</span><br><span class="line">import jodd.util.PropertiesUtil</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * &lt;p&gt;Title: CardEventEtl&lt;/p&gt;</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;Description:获取feeds所有数据表名 &lt;/p&gt;</span><br><span class="line"> *</span><br><span class="line"> * @author zhangshuai</span><br><span class="line"> *</span><br><span class="line"> * @date 2019年3月1日</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">object DBUtil &#123;</span><br><span class="line"></span><br><span class="line">def getFeedsTableNames() = &#123;</span><br><span class="line">    var data = new ArrayBuffer[String]()</span><br><span class="line">    val conn = DBUtilC3p0.getDBUtilC3p0().getConnection</span><br><span class="line">    try &#123;</span><br><span class="line">      val <span class="built_in">stat</span> = conn.createStatement()</span><br><span class="line">      val rs: ResultSet = stat.executeQuery(<span class="string">&quot;show tables like &#x27;%_card_event&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">        val tableName = rs.getString(1)</span><br><span class="line"></span><br><span class="line">          data += tableName</span><br><span class="line">       </span><br><span class="line">      &#125;</span><br><span class="line">      rs.close()</span><br><span class="line">      stat.close()</span><br><span class="line">      data</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">        t.printStackTrace()</span><br><span class="line">        println(<span class="string">&quot;获取feeds表表名时候出错.&quot;</span>)</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        <span class="keyword">if</span> (conn != null) &#123;</span><br><span class="line">          conn.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">          t.printStackTrace()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    data</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">//  private var connection: Connection = _</span><br><span class="line">//</span><br><span class="line">//  private var preparedStatement: PreparedStatement = _</span><br><span class="line">//</span><br><span class="line">//  private var resultSet: ResultSet = _</span><br><span class="line">//</span><br><span class="line">//  def getCardEventTableNames = &#123;</span><br><span class="line">//    var data = new ArrayBuffer[String]()</span><br><span class="line">//    val conn = DBUtil.getConnection</span><br><span class="line">//    try &#123;</span><br><span class="line">//      val <span class="built_in">stat</span> = conn.createStatement()</span><br><span class="line">//      val rs: ResultSet = stat.executeQuery(<span class="string">&quot;show tables like &#x27;%_card_event&#x27;&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//      <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">//        val tableName = rs.getString(1)</span><br><span class="line">//</span><br><span class="line">//        data += tableName</span><br><span class="line">//</span><br><span class="line">//      &#125;</span><br><span class="line">//      rs.close()</span><br><span class="line">//      stat.close()</span><br><span class="line">//      data</span><br><span class="line">//    &#125; catch &#123;</span><br><span class="line">//      <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">//        t.printStackTrace()</span><br><span class="line">//        println(<span class="string">&quot;获取统计表表名时候出错.&quot;</span>)</span><br><span class="line">//    &#125; finally &#123;</span><br><span class="line">//</span><br><span class="line">//      try &#123;</span><br><span class="line">//        <span class="keyword">if</span> (conn != null) &#123;</span><br><span class="line">//          conn.close()</span><br><span class="line">//</span><br><span class="line">//        &#125;</span><br><span class="line">//</span><br><span class="line">//      &#125; catch &#123;</span><br><span class="line">//        <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">//          t.printStackTrace()</span><br><span class="line">//      &#125;</span><br><span class="line">//</span><br><span class="line">//    &#125;</span><br><span class="line">//    data</span><br><span class="line">//</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  </span><br><span class="line">// def getFeedsConnection: Connection =&#123;</span><br><span class="line">//    DBUtil.getConnection</span><br><span class="line">// &#125;</span><br><span class="line">//  </span><br><span class="line">//  </span><br><span class="line">//  </span><br><span class="line">//  // 数据库驱动类</span><br><span class="line">//</span><br><span class="line">//  private val driverClass: String = ConfigurationManager.getProperty(<span class="string">&quot;driverClass&quot;</span>)</span><br><span class="line">//  // 数据库连接地址</span><br><span class="line">//  private val url: String = ConfigurationManager.getProperty(<span class="string">&quot;feeds.jdbcUrl&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//  // 数据库连接用户名</span><br><span class="line">//  private val username: String = ConfigurationManager.getProperty(<span class="string">&quot;feeds.user&quot;</span>)</span><br><span class="line">//  // 数据库连接密码</span><br><span class="line">//  private val password: String = ConfigurationManager.getProperty(<span class="string">&quot;feeds.password&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//  // 加载数据库驱动</span><br><span class="line">//  //  Class.forName(driverClass)</span><br><span class="line">//  Class.forName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//  // 连接池大小</span><br><span class="line">//  val poolSize: Int = ConfigurationManager.getProperty(<span class="string">&quot;poolSize&quot;</span>).toInt</span><br><span class="line">//</span><br><span class="line">//  // 连接池 - 同步队列</span><br><span class="line">//  private val pool: BlockingQueue[Connection] = new LinkedBlockingQueue[Connection]()</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 初始化连接池</span><br><span class="line">//   */</span><br><span class="line">//  <span class="keyword">for</span> (i &lt;- 1 to poolSize) &#123;</span><br><span class="line">//    DBUtil.pool.put(DriverManager.getConnection(url, username, password))</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 从连接池中获取一个Connection</span><br><span class="line">//   * @<span class="built_in">return</span></span><br><span class="line">//   */</span><br><span class="line">//  private def getConnection: Connection = &#123;</span><br><span class="line">//    pool.take()</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 向连接池归还一个Connection</span><br><span class="line">//   * @param conn</span><br><span class="line">//   */</span><br><span class="line">//  private def returnConnection(conn: Connection): Unit = &#123;</span><br><span class="line">//    DBUtil.pool.put(conn)</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 启动守护线程释放资源</span><br><span class="line">//   */</span><br><span class="line">//  def releaseResource() = &#123;</span><br><span class="line">//    val thread = new Thread(new CloseRunnable)</span><br><span class="line">//    thread.setDaemon(<span class="literal">true</span>)</span><br><span class="line">//    thread.start()</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 关闭连接池连接资源类</span><br><span class="line">//   */</span><br><span class="line">//  class CloseRunnable extends Runnable &#123;</span><br><span class="line">//    override def run(): Unit = &#123;</span><br><span class="line">//      <span class="keyword">while</span> (DBUtil.pool.size &gt; 0) &#123;</span><br><span class="line">//        try &#123;</span><br><span class="line">//          //          println(s<span class="string">&quot;当前连接池大小: <span class="variable">$&#123;DBUtil.pool.size&#125;</span>&quot;</span>)</span><br><span class="line">//          DBUtil.pool.take().close()</span><br><span class="line">//        &#125; catch &#123;</span><br><span class="line">//          <span class="keyword">case</span> e: Exception =&gt; e.printStackTrace()</span><br><span class="line">//        &#125;</span><br><span class="line">//      &#125;</span><br><span class="line">//    &#125;</span><br><span class="line">//  &#125;</span><br><span class="line"></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.jiatui.bigdata.util</span><br><span class="line"></span><br><span class="line">import java.sql.ResultSet</span><br><span class="line">import java.sql.Connection</span><br><span class="line">import java.sql.PreparedStatement</span><br><span class="line">import java.util.concurrent.BlockingQueue</span><br><span class="line">import java.util.concurrent.LinkedBlockingQueue</span><br><span class="line">import java.sql.DriverManager</span><br><span class="line">import jodd.util.PropertiesUtil</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * &lt;p&gt;Title: CardEventEtl&lt;/p&gt;</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;Description:获取feeds所有数据表名 &lt;/p&gt;</span><br><span class="line"> *</span><br><span class="line"> * @author zhangshuai</span><br><span class="line"> *</span><br><span class="line"> * @date 2019年3月1日</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">object DBUtil &#123;</span><br><span class="line"></span><br><span class="line">def getFeedsTableNames() = &#123;</span><br><span class="line">    var data = new ArrayBuffer[String]()</span><br><span class="line">    val conn = DBUtilC3p0.getDBUtilC3p0().getConnection</span><br><span class="line">    try &#123;</span><br><span class="line">      val <span class="built_in">stat</span> = conn.createStatement()</span><br><span class="line">      val rs: ResultSet = stat.executeQuery(<span class="string">&quot;show tables like &#x27;%_card_event&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">        val tableName = rs.getString(1)</span><br><span class="line"></span><br><span class="line">          data += tableName</span><br><span class="line">       </span><br><span class="line">      &#125;</span><br><span class="line">      rs.close()</span><br><span class="line">      stat.close()</span><br><span class="line">      data</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">        t.printStackTrace()</span><br><span class="line">        println(<span class="string">&quot;获取feeds表表名时候出错.&quot;</span>)</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        <span class="keyword">if</span> (conn != null) &#123;</span><br><span class="line">          conn.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">          t.printStackTrace()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    data</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">//  private var connection: Connection = _</span><br><span class="line">//</span><br><span class="line">//  private var preparedStatement: PreparedStatement = _</span><br><span class="line">//</span><br><span class="line">//  private var resultSet: ResultSet = _</span><br><span class="line">//</span><br><span class="line">//  def getCardEventTableNames = &#123;</span><br><span class="line">//    var data = new ArrayBuffer[String]()</span><br><span class="line">//    val conn = DBUtil.getConnection</span><br><span class="line">//    try &#123;</span><br><span class="line">//      val <span class="built_in">stat</span> = conn.createStatement()</span><br><span class="line">//      val rs: ResultSet = stat.executeQuery(<span class="string">&quot;show tables like &#x27;%_card_event&#x27;&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//      <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">//        val tableName = rs.getString(1)</span><br><span class="line">//</span><br><span class="line">//        data += tableName</span><br><span class="line">//</span><br><span class="line">//      &#125;</span><br><span class="line">//      rs.close()</span><br><span class="line">//      stat.close()</span><br><span class="line">//      data</span><br><span class="line">//    &#125; catch &#123;</span><br><span class="line">//      <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">//        t.printStackTrace()</span><br><span class="line">//        println(<span class="string">&quot;获取统计表表名时候出错.&quot;</span>)</span><br><span class="line">//    &#125; finally &#123;</span><br><span class="line">//</span><br><span class="line">//      try &#123;</span><br><span class="line">//        <span class="keyword">if</span> (conn != null) &#123;</span><br><span class="line">//          conn.close()</span><br><span class="line">//</span><br><span class="line">//        &#125;</span><br><span class="line">//</span><br><span class="line">//      &#125; catch &#123;</span><br><span class="line">//        <span class="keyword">case</span> t: Exception =&gt;</span><br><span class="line">//          t.printStackTrace()</span><br><span class="line">//      &#125;</span><br><span class="line">//</span><br><span class="line">//    &#125;</span><br><span class="line">//    data</span><br><span class="line">//</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  </span><br><span class="line">// def getFeedsConnection: Connection =&#123;</span><br><span class="line">//    DBUtil.getConnection</span><br><span class="line">// &#125;</span><br><span class="line">//  </span><br><span class="line">//  </span><br><span class="line">//  </span><br><span class="line">//  // 数据库驱动类</span><br><span class="line">//</span><br><span class="line">//  private val driverClass: String = ConfigurationManager.getProperty(<span class="string">&quot;driverClass&quot;</span>)</span><br><span class="line">//  // 数据库连接地址</span><br><span class="line">//  private val url: String = ConfigurationManager.getProperty(<span class="string">&quot;feeds.jdbcUrl&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//  // 数据库连接用户名</span><br><span class="line">//  private val username: String = ConfigurationManager.getProperty(<span class="string">&quot;feeds.user&quot;</span>)</span><br><span class="line">//  // 数据库连接密码</span><br><span class="line">//  private val password: String = ConfigurationManager.getProperty(<span class="string">&quot;feeds.password&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//  // 加载数据库驱动</span><br><span class="line">//  //  Class.forName(driverClass)</span><br><span class="line">//  Class.forName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">//</span><br><span class="line">//  // 连接池大小</span><br><span class="line">//  val poolSize: Int = ConfigurationManager.getProperty(<span class="string">&quot;poolSize&quot;</span>).toInt</span><br><span class="line">//</span><br><span class="line">//  // 连接池 - 同步队列</span><br><span class="line">//  private val pool: BlockingQueue[Connection] = new LinkedBlockingQueue[Connection]()</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 初始化连接池</span><br><span class="line">//   */</span><br><span class="line">//  <span class="keyword">for</span> (i &lt;- 1 to poolSize) &#123;</span><br><span class="line">//    DBUtil.pool.put(DriverManager.getConnection(url, username, password))</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 从连接池中获取一个Connection</span><br><span class="line">//   * @<span class="built_in">return</span></span><br><span class="line">//   */</span><br><span class="line">//  private def getConnection: Connection = &#123;</span><br><span class="line">//    pool.take()</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 向连接池归还一个Connection</span><br><span class="line">//   * @param conn</span><br><span class="line">//   */</span><br><span class="line">//  private def returnConnection(conn: Connection): Unit = &#123;</span><br><span class="line">//    DBUtil.pool.put(conn)</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 启动守护线程释放资源</span><br><span class="line">//   */</span><br><span class="line">//  def releaseResource() = &#123;</span><br><span class="line">//    val thread = new Thread(new CloseRunnable)</span><br><span class="line">//    thread.setDaemon(<span class="literal">true</span>)</span><br><span class="line">//    thread.start()</span><br><span class="line">//  &#125;</span><br><span class="line">//</span><br><span class="line">//  /**</span><br><span class="line">//   * 关闭连接池连接资源类</span><br><span class="line">//   */</span><br><span class="line">//  class CloseRunnable extends Runnable &#123;</span><br><span class="line">//    override def run(): Unit = &#123;</span><br><span class="line">//      <span class="keyword">while</span> (DBUtil.pool.size &gt; 0) &#123;</span><br><span class="line">//        try &#123;</span><br><span class="line">//          //          println(s<span class="string">&quot;当前连接池大小: <span class="variable">$&#123;DBUtil.pool.size&#125;</span>&quot;</span>)</span><br><span class="line">//          DBUtil.pool.take().close()</span><br><span class="line">//        &#125; catch &#123;</span><br><span class="line">//          <span class="keyword">case</span> e: Exception =&gt; e.printStackTrace()</span><br><span class="line">//        &#125;</span><br><span class="line">//      &#125;</span><br><span class="line">//    &#125;</span><br><span class="line">//  &#125;</span><br><span class="line"></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka生产者详解</title>
    <url>/2018/12/02/16/</url>
    <content><![CDATA[<h3 id="一、生产者发送消息的过程"><a href="#一、生产者发送消息的过程" class="headerlink" title="一、生产者发送消息的过程"></a>一、生产者发送消息的过程</h3><p>首先介绍一下 Kafka 生产者发送消息的过程：</p>
<p>Kafka 会将发送消息包装为 ProducerRecord 对象， ProducerRecord 对象包含了目标主题和要发送的内容，同时还可以指定键和分区。在发送 ProducerRecord 对象前，生产者会先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。</p>
<p>接下来，数据被传给分区器。如果之前已经在 ProducerRecord 对象里指定了分区，那么分区器就不会再做任何事情。如果没有指定分区 ，那么分区器会根据 ProducerRecord 对象的键来选择一个分区，紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的 broker 上。</p>
<p>服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka，就返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败，则会返回一个错误。生产者在收到错误之后会尝试重新发送消息，如果达到指定的重试次数后还没有成功，则直接抛出异常，不再重试。</p>
<p><img src="/images/15.png" alt="alt"></p>
<h3 id="二、创建生产者"><a href="#二、创建生产者" class="headerlink" title="二、创建生产者"></a>二、创建生产者</h3><p>2.1 项目依赖<br>本项目采用 Maven 构建，想要调用 Kafka 生产者 API，需要导入 kafka-clients 依赖，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.2.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>2.2 创建生产者<br>创建 Kafka 生产者时，以下三个属性是必须指定的：</p>
<p>bootstrap.servers ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错；<br>key.serializer ：指定键的序列化器；<br>value.serializer ：指定值的序列化器。</p>
<p>创建的示例代码如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">public class SimpleProducer &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        String topicName = <span class="string">&quot;Hello-Kafka&quot;</span>;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        /*创建生产者*/</span><br><span class="line">        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (int i = 0; i &lt; 10; i++) &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, <span class="string">&quot;hello&quot;</span> + i, </span><br><span class="line">                                                                         <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">            /* 发送消息*/</span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125;</span><br><span class="line">        /*关闭生产者*/</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2.3 测试</p>
<ol>
<li>启动Kakfa<br>Kafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># zookeeper启动命令</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 内置zookeeper启动命令</span></span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>

<p>启动单节点 kafka 用于测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建topic<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建用于测试主题</span></span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                     --replication-factor 1 --partitions 1 \</span><br><span class="line">                     --topic Hello-Kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure></li>
<li>运行项目<br>此时可以看到消费者控制台，输出如下，这里 kafka-console-consumer 只会打印出值信息，不会打印出键信息。<br><img src="/images/16.png" alt="alt"></li>
</ol>
<p>2.4 可能出现的问题<br>在这里可能出现的一个问题是：生产者程序在启动后，一直处于等待状态。这通常出现在你使用默认配置启动 Kafka 的情况下，此时需要对 server.properties 文件中的 listeners 配置进行更改</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop001 为我启动kafka服务的主机名，你可以换成自己的主机名或者ip地址</span></span><br><span class="line">listeners=PLAINTEXT://hadoop001:9092</span><br></pre></td></tr></table></figure>

<h3 id="二、发送消息"><a href="#二、发送消息" class="headerlink" title="二、发送消息"></a>二、发送消息</h3><p>上面的示例程序调用了 send 方法发送消息后没有做任何操作，在这种情况下，我们没有办法知道消息发送的结果。想要知道消息发送的结果，可以使用同步发送或者异步发送来实现。</p>
<p>2.1 同步发送<br>在调用 send 方法后可以接着调用 get() 方法，send 方法的返回值是一个 Future<RecordMetadata>对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (int i = 0; i &lt; 10; i++) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">        /*同步发送消息*/</span><br><span class="line">        RecordMetadata metadata = producer.send(record).get();</span><br><span class="line">        System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">    &#125; catch (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 Hello-Kafka 主题时候，使用 –partitions 指定其分区数为 1，即只有一个分区。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">topic=Hello-Kafka, partition=0, offset=40 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=41 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=42 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=43 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=44 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=45 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=46 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=47 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=48 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=49</span><br></pre></td></tr></table></figure>
<p>2.2 异步发送<br>通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (int i = 0; i &lt; 10; i++) &#123;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">    /*异步发送消息，并监听回调*/</span><br><span class="line">    producer.send(record, new <span class="function"><span class="title">Callback</span></span>() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != null) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;进行异常处理&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                        metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="三、自定义分区器"><a href="#三、自定义分区器" class="headerlink" title="三、自定义分区器"></a>三、自定义分区器</h3><p>Kafka 有着默认的分区机制：</p>
<p>如果键值为 null， 则使用轮询 (Round Robin) 算法将消息均衡地分布到各个分区上；<br>如果键值不为 null，那么 Kafka 会使用内置的散列算法对键进行散列，然后分布到各个分区上。<br>某些情况下，你可能有着自己的分区需求，这时候可以采用自定义分区器实现。这里给出一个自定义分区器的示例</p>
<p>3.1 自定义分区器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 自定义分区器</span><br><span class="line"> */</span><br><span class="line">public class CustomPartitioner implements Partitioner &#123;</span><br><span class="line"></span><br><span class="line">    private int passLine;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line">        /*从生产者配置中获取分数线*/</span><br><span class="line">        passLine = (Integer) configs.get(<span class="string">&quot;pass.line&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int partition(String topic, Object key, byte[] keyBytes, Object value, </span><br><span class="line">                         byte[] valueBytes, Cluster cluster) &#123;</span><br><span class="line">        /*key 值为分数，当分数大于分数线时候，分配到 1 分区，否则分配到 0 分区*/</span><br><span class="line">        <span class="built_in">return</span> (Integer) key &gt;= passLine ? 1 : 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void <span class="function"><span class="title">close</span></span>() &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;分区器关闭&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要在创建生产者时指定分区器，和分区器所需要的配置参数：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">public class ProducerWithPartitioner &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        String topicName = <span class="string">&quot;Kafka-Partitioner-Test&quot;</span>;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        /*传递自定义分区器*/</span><br><span class="line">        props.put(<span class="string">&quot;partitioner.class&quot;</span>, <span class="string">&quot;com.heibaiying.producers.partitioners.CustomPartitioner&quot;</span>);</span><br><span class="line">        /*传递分区器所需的参数*/</span><br><span class="line">        props.put(<span class="string">&quot;pass.line&quot;</span>, 6);</span><br><span class="line"></span><br><span class="line">        Producer&lt;Integer, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (int i = 0; i &lt;= 10; i++) &#123;</span><br><span class="line">            String score = <span class="string">&quot;score:&quot;</span> + i;</span><br><span class="line">            ProducerRecord&lt;Integer, String&gt; record = new ProducerRecord&lt;&gt;(topicName, i, score);</span><br><span class="line">            /*异步发送消息*/</span><br><span class="line">            producer.send(record, (metadata, exception) -&gt;</span><br><span class="line">                    System.out.printf(<span class="string">&quot;%s, partition=%d, \n&quot;</span>, score, metadata.partition()));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.2 测试<br>需要创建一个至少有两个分区的主题：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                   --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 --partitions 2 \</span><br><span class="line">                    --topic Kafka-Partitioner-Test</span><br></pre></td></tr></table></figure>
<p>此时输入如下，可以看到分数大于等于 6 分的都被分到 1 分区，而小于 6 分的都被分到了 0 分区。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">score:6, partition=1, </span><br><span class="line">score:7, partition=1, </span><br><span class="line">score:8, partition=1, </span><br><span class="line">score:9, partition=1, </span><br><span class="line">score:10, partition=1, </span><br><span class="line">score:0, partition=0, </span><br><span class="line">score:1, partition=0, </span><br><span class="line">score:2, partition=0, </span><br><span class="line">score:3, partition=0, </span><br><span class="line">score:4, partition=0, </span><br><span class="line">score:5, partition=0, </span><br><span class="line">分区器关闭</span><br></pre></td></tr></table></figure>

<h3 id="四、生产者其他属性"><a href="#四、生产者其他属性" class="headerlink" title="四、生产者其他属性"></a>四、生产者其他属性</h3><p>上面生产者的创建都仅指定了服务地址，键序列化器、值序列化器，实际上 Kafka 的生产者还有很多可配置属性，如下：</p>
<ol>
<li>acks<br>acks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的：</li>
</ol>
<p>acks=0 ： 消息发送出去就认为已经成功了，不会等待任何来自服务器的响应；<br>acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应；<br>acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。<br>2. buffer.memory<br>设置生产者内存缓冲区的大小。</p>
<ol start="3">
<li><p>compression.type<br>默认情况下，发送的消息不会被压缩。如果想要进行压缩，可以配置此参数，可选值有 snappy，gzip，lz4。</p>
</li>
<li><p>retries<br>发生错误后，消息重发的次数。如果达到设定值，生产者就会放弃重试并返回错误。</p>
</li>
<li><p>batch.size<br>当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。</p>
</li>
<li><p>linger.ms<br>该参数制定了生产者在发送批次之前等待更多消息加入批次的时间。</p>
</li>
<li><p>clent.id<br>客户端 id,服务器用来识别消息的来源。</p>
</li>
<li><p>max.in.flight.requests.per.connection<br>指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量，把它设置为 1 可以保证消息是按照发送的顺序写入服务器，即使发生了重试。</p>
</li>
<li><p>timeout.ms, request.timeout.ms &amp; metadata.fetch.timeout.ms<br>timeout.ms 指定了 borker 等待同步副本返回消息的确认时间；<br>request.timeout.ms 指定了生产者在发送数据时等待服务器返回响应的时间；<br>metadata.fetch.timeout.ms 指定了生产者在获取元数据（比如分区首领是谁）时等待服务器返回响应的时间。</p>
</li>
<li><p>max.block.ms<br>指定了在调用 send() 方法或使用 partitionsFor() 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常。</p>
</li>
<li><p>max.request.size<br>该参数用于控制生产者发送的请求大小。它可以指发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。例如，假设这个值为 1000K ，那么可以发送的单个最大消息为 1000K ，或者生产者可以在单个请求里发送一个批次，该批次包含了 1000 个消息，每个消息大小为 1K。</p>
</li>
<li><p>receive.buffer.bytes &amp; send.buffer.byte<br>这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka消费者详解</title>
    <url>/2018/12/02/17/</url>
    <content><![CDATA[<h3 id="一、消费者和消费者群组"><a href="#一、消费者和消费者群组" class="headerlink" title="一、消费者和消费者群组"></a>一、消费者和消费者群组</h3><p>在 Kafka 中，消费者通常是消费者群组的一部分，多个消费者群组共同读取同一个主题时，彼此之间互不影响。Kafka 之所以要引入消费者群组这个概念是因为 Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS ，或者进行耗时的计算，在这些情况下，单个消费者无法跟上数据生成的速度。此时可以增加更多的消费者，让它们分担负载，分别处理部分分区的消息，这就是 Kafka 实现横向伸缩的主要手段。<br><img src="/images/17.png" alt="alt"><br>需要注意的是：同一个分区只能被同一个消费者群组里面的一个消费者读取，不可能存在同一个分区被同一个消费者群里多个消费者共同读取的情况，如图：<br><img src="/images/18.png" alt="alt"></p>
<p>可以看到即便消费者 Consumer5 空闲了，但是也不会去读取任何一个分区的数据，这同时也提醒我们在使用时应该合理设置消费者的数量，以免造成闲置和额外开销。</p>
<h3 id="二、分区再均衡"><a href="#二、分区再均衡" class="headerlink" title="二、分区再均衡"></a>二、分区再均衡</h3><p>因为群组里的消费者共同读取主题的分区，所以当一个消费者被关闭或发生崩溃时，它就离开了群组，原本由它读取的分区将由群组里的其他消费者来读取。同时在主题发生变化时 ， 比如添加了新的分区，也会发生分区与消费者的重新分配，分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。正是因为再均衡，所以消费费者群组才能保证高可用性和伸缩性。</p>
<p>消费者通过向群组协调器所在的 broker 发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发再均衡。</p>
<h3 id="三、创建Kafka消费者"><a href="#三、创建Kafka消费者" class="headerlink" title="三、创建Kafka消费者"></a>三、创建Kafka消费者</h3><p>在创建消费者的时候以下以下三个选项是必选的：</p>
<p>bootstrap.servers ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错；<br>key.deserializer ：指定键的反序列化器；<br>value.deserializer ：指定值的反序列化器。</p>
<p>除此之外你还需要指明你需要想订阅的主题，可以使用如下两个 API :</p>
<p>consumer.subscribe(Collection<String> topics) ：指明需要订阅的主题的集合；<br>consumer.subscribe(Pattern pattern) ：使用正则来匹配需要订阅的集合。</p>
<p>最后只需要通过轮询 API(poll) 向服务器定时请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，这使得开发者只需要关注从分区返回的数据，然后进行业务处理。 示例如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">String topic = <span class="string">&quot;Hello-Kafka&quot;</span>;</span><br><span class="line">String group = <span class="string">&quot;group1&quot;</span>;</span><br><span class="line">Properties props = new Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line">/*指定分组 ID*/</span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, group);</span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">/*订阅主题 (s)*/</span><br><span class="line">consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line"></span><br><span class="line">try &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        /*轮询获取数据*/</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.printf(<span class="string">&quot;topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n&quot;</span>,</span><br><span class="line">           record.topic(), record.partition(), record.key(), record.value(), record.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="三、-自动提交偏移量"><a href="#三、-自动提交偏移量" class="headerlink" title="三、 自动提交偏移量"></a>三、 自动提交偏移量</h3><p>3.1 偏移量的重要性<br>Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。消费者通过往一个叫作 ＿consumer_offset 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有 什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费，比如下面情况：</p>
<p>如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复消费；<br>如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。</p>
<p>3.2 自动提交偏移量<br>Kafka 支持自动提交和手动提交偏移量两种方式。这里先介绍比较简单的自动提交：</p>
<p>只需要将消费者的 enable.auto.commit 属性配置为 true 即可完成自动提交的配置。 此时每隔固定的时间，消费者就会把 poll() 方法接收到的最大偏移量进行提交，提交间隔由 auto.commit.interval.ms 属性进行配置，默认值是 5s。</p>
<p>使用自动提交是存在隐患的，假设我们使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 3s 内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减小可能出现重复消息的时间窗，不过这种情况是无法完全避免的。基于这个原因，Kafka 也提供了手动提交偏移量的 API，使得用户可以更为灵活的提交偏移量。</p>
<h3 id="四、手动提交偏移量"><a href="#四、手动提交偏移量" class="headerlink" title="四、手动提交偏移量"></a>四、手动提交偏移量</h3><p>用户可以通过将 enable.auto.commit 设为 false，然后手动提交偏移量。基于用户需求手动提交偏移量可以分为两大类：<br>手动提交当前偏移量：即手动提交当前轮询的最大偏移量；<br>手动提交固定偏移量：即按照业务需求，提交某一个固定的偏移量。</p>
<p>而按照 Kafka API，手动提交偏移量又可以分为同步提交和异步提交。<br>4.1 同步提交<br>通过调用 consumer.commitSync() 来进行同步提交，不传递任何参数时提交的是当前轮询的最大偏移量。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record);</span><br><span class="line">    &#125;</span><br><span class="line">    /*同步提交*/</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果某个提交失败，同步提交还会进行重试，这可以保证数据能够最大限度提交成功，但是同时也会降低程序的吞吐量。基于这个原因，Kafka 还提供了异步提交的 API。</p>
<p>4.2 异步提交<br>异步提交可以提高程序的吞吐量，因为此时你可以尽管请求数据，而不用等待 Broker 的响应。代码如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record);</span><br><span class="line">    &#125;</span><br><span class="line">    /*异步提交并定义回调*/</span><br><span class="line">    consumer.commitAsync(new <span class="function"><span class="title">OffsetCommitCallback</span></span>() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123;</span><br><span class="line">          <span class="keyword">if</span> (exception != null) &#123;</span><br><span class="line">             System.out.println(<span class="string">&quot;错误处理&quot;</span>);</span><br><span class="line">             offsets.forEach((x, y) -&gt; System.out.printf(<span class="string">&quot;topic = %s,partition = %d, offset = %s \n&quot;</span>,</span><br><span class="line">                                                            x.topic(), x.partition(), y.offset()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>异步提交存在的问题是，在提交失败的时候不会进行自动重试，实际上也不能进行自动重试。假设程序同时提交了 200 和 300 的偏移量，此时 200 的偏移量失败的，但是紧随其后的 300 的偏移量成功了，此时如果重试就会存在 200 覆盖 300 偏移量的可能。同步提交就不存在这个问题，因为在同步提交的情况下，300 的提交请求必须等待服务器返回 200 提交请求的成功反馈后才会发出。基于这个原因，某些情况下，需要同时组合同步和异步两种提交方式。</p>
<p>注：虽然程序不能在失败时候进行自动重试，但是我们是可以手动进行重试的，你可以通过一个 Map&lt;TopicPartition, Integer&gt; offsets 来维护你提交的每个分区的偏移量，然后当失败时候，你可以判断失败的偏移量是否小于你维护的同主题同分区的最后提交的偏移量，如果小于则代表你已经提交了更大的偏移量请求，此时不需要重试，否则就可以进行手动重试。</p>
<p>4.3 同步加异步提交<br>下面这种情况，在正常的轮询中使用异步提交来保证吞吐量，但是因为在最后即将要关闭消费者了，所以此时需要用同步提交来保证最大限度的提交成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">        &#125;</span><br><span class="line">        // 异步提交</span><br><span class="line">        consumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125; catch (Exception e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        // 因为即将要关闭消费者，所以要用同步提交保证提交成功</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        consumer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4.4 提交特定偏移量<br>在上面同步和异步提交的 API 中，实际上我们都没有对 commit 方法传递参数，此时默认提交的是当前轮询的最大偏移量，如果你需要提交特定的偏移量，可以调用它们的重载方法。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/*同步提交特定偏移量*/</span><br><span class="line">commitSync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) </span><br><span class="line">/*异步提交特定偏移量*/    </span><br><span class="line">commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)</span><br></pre></td></tr></table></figure>

<p>需要注意的是，因为你可以订阅多个主题，所以 offsets 中必须要包含所有主题的每个分区的偏移量，示例代码如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">            /*记录每个主题的每个分区的偏移量*/</span><br><span class="line">            TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition());</span><br><span class="line">            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset()+1, <span class="string">&quot;no metaData&quot;</span>);</span><br><span class="line">            /*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/</span><br><span class="line">            offsets.put(topicPartition, offsetAndMetadata);</span><br><span class="line">        &#125;</span><br><span class="line">        /*提交特定偏移量*/</span><br><span class="line">        consumer.commitAsync(offsets, null);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="五、监听分区再均衡"><a href="#五、监听分区再均衡" class="headerlink" title="五、监听分区再均衡"></a>五、监听分区再均衡</h3><p>因为分区再均衡会导致分区与消费者的重新划分，有时候你可能希望在再均衡前执行一些操作：比如提交已经处理但是尚未提交的偏移量，关闭数据库连接等。此时可以在订阅主题时候，调用 subscribe 的重载方法传入自定义的分区再均衡监听器。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> /*订阅指定集合内的所有主题*/</span><br><span class="line">subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span><br><span class="line"> /*使用正则匹配需要订阅的主题*/    </span><br><span class="line">subscribe(Pattern pattern, ConsumerRebalanceListener listener)   </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">consumer.subscribe(Collections.singletonList(topic), new <span class="function"><span class="title">ConsumerRebalanceListener</span></span>() &#123;</span><br><span class="line">    /*该方法会在消费者停止读取消息之后，再均衡开始之前就调用*/</span><br><span class="line">    @Override</span><br><span class="line">    public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;再均衡即将触发&quot;</span>);</span><br><span class="line">        // 提交已经处理的偏移量</span><br><span class="line">        consumer.commitSync(offsets);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /*该方法会在重新分配分区之后，消费者开始读取消息之前被调用*/</span><br><span class="line">    @Override</span><br><span class="line">    public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">try &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">            TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition());</span><br><span class="line">            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset() + 1, <span class="string">&quot;no metaData&quot;</span>);</span><br><span class="line">            /*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/</span><br><span class="line">            offsets.put(topicPartition, offsetAndMetadata);</span><br><span class="line">        &#125;</span><br><span class="line">        consumer.commitAsync(offsets, null);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="六-、退出轮询"><a href="#六-、退出轮询" class="headerlink" title="六 、退出轮询"></a>六 、退出轮询</h3><p>Kafka 提供了 consumer.wakeup() 方法用于退出轮询，它通过抛出 WakeupException 异常来跳出循环。需要注意的是，在退出线程时最好显示的调用 consumer.close() , 此时消费者会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡 ，而不需要等待会话超时。</p>
<p>下面的示例代码为监听控制台输出，当输入 exit 时结束轮询，关闭消费者并退出程序：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/*调用 wakeup 优雅的退出*/</span><br><span class="line">final Thread mainThread = Thread.currentThread();</span><br><span class="line">new Thread(() -&gt; &#123;</span><br><span class="line">    Scanner sc = new Scanner(System.in);</span><br><span class="line">    <span class="keyword">while</span> (sc.hasNext()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;exit&quot;</span>.equals(sc.next())) &#123;</span><br><span class="line">            consumer.wakeup();</span><br><span class="line">            try &#123;</span><br><span class="line">                /*等待主线程完成提交偏移量、关闭消费者等操作*/</span><br><span class="line">                mainThread.join();</span><br><span class="line">                <span class="built_in">break</span>;</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"></span><br><span class="line">try &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; rd : records) &#123;</span><br><span class="line">            System.out.printf(<span class="string">&quot;topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n&quot;</span>,</span><br><span class="line">                              rd.topic(), rd.partition(), rd.key(), rd.value(), rd.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; catch (WakeupException e) &#123;</span><br><span class="line">    //对于 wakeup() 调用引起的 WakeupException 异常可以不必处理</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">    System.out.println(<span class="string">&quot;consumer 关闭&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="七、独立的消费者"><a href="#七、独立的消费者" class="headerlink" title="七、独立的消费者"></a>七、独立的消费者</h3><p>因为 Kafka 的设计目标是高吞吐和低延迟，所以在 Kafka 中，消费者通常都是从属于某个群组的，这是因为单个消费者的处理能力是有限的。但是某些时候你的需求可能很简单，比如可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据，这个时候就不需要消费者群组和再均衡了， 只需要把主题或者分区分配给消费者，然后开始读取消息井提交偏移量即可。</p>
<p>在这种情况下，就不需要订阅主题， 取而代之的是消费者为自己分配分区。 一个消费者可以订阅主题（井加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。 分配分区的示例代码如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">List&lt;TopicPartition&gt; partitions = new ArrayList&lt;&gt;();</span><br><span class="line">List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);</span><br><span class="line"></span><br><span class="line">/*可以指定读取哪些分区 如这里假设只读取主题的 0 分区*/</span><br><span class="line"><span class="keyword">for</span> (PartitionInfo partition : partitionInfos) &#123;</span><br><span class="line">    <span class="keyword">if</span> (partition.partition()==0)&#123;</span><br><span class="line">        partitions.add(new TopicPartition(partition.topic(), partition.partition()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 为消费者指定分区</span><br><span class="line">consumer.assign(partitions);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;Integer, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;Integer, String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(<span class="string">&quot;partition = %s, key = %d, value = %s\n&quot;</span>,</span><br><span class="line">                          record.partition(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="附录-Kafka消费者可选属性"><a href="#附录-Kafka消费者可选属性" class="headerlink" title="附录 : Kafka消费者可选属性"></a>附录 : Kafka消费者可选属性</h3><ol>
<li><p>fetch.min.byte<br>消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</p>
</li>
<li><p>fetch.max.wait.ms<br>broker 返回给消费者数据的等待时间，默认是 500ms。</p>
</li>
<li><p>max.partition.fetch.bytes<br>该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。</p>
</li>
<li><p>session.timeout.ms<br>消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。</p>
</li>
<li><p>auto.offset.reset<br>该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</p>
</li>
</ol>
<p>latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;<br>earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。<br>6. enable.auto.commit<br>是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。</p>
<ol start="7">
<li><p>client.id<br>客户端 id，服务器用来识别消息的来源。</p>
</li>
<li><p>max.poll.records<br>单次调用 poll() 方法能够返回的记录数量。</p>
</li>
<li><p>receive.buffer.bytes &amp; send.buffer.byte<br>这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Oozie任务调度使用详细代码</title>
    <url>/2018/12/02/6/</url>
    <content><![CDATA[<h3 id="job-properties"><a href="#job-properties" class="headerlink" title="job.properties"></a>job.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nameNode=hdfs://dev-bg-m01:8020</span><br><span class="line">jobTracker=dev-bg-m01:8050</span><br><span class="line">queueName=default</span><br><span class="line">oozie.use.system.libpath=<span class="literal">true</span></span><br><span class="line"><span class="comment">#oozie.libpath=/user/dmp_operator1/share/libs</span></span><br><span class="line">jdbcURL=jdbc:hive2://dev-bg-m01:10000/default</span><br><span class="line">jdbcPrincipal=hive/dev-bg-m01@DATASEA.COM</span><br><span class="line">appsDir=user/dmp_operator1/OozieApps</span><br><span class="line">dataDir=user/dmp_operator1/data</span><br><span class="line">jdbcPrincipal=hive/dev-bg-m01@DATASEA.COM</span><br><span class="line">jdbcURL=jdbc:hive2://dev-bg-m01:10000/default</span><br><span class="line">oozie.coord.application.path=<span class="variable">$&#123;nameNode&#125;</span>/<span class="variable">$&#123;appsDir&#125;</span>/shop/t_order</span><br><span class="line"><span class="comment">#oozie.wf.application.path=$&#123;nameNode&#125;/$&#123;appsDir&#125;/shop/t_order</span></span><br><span class="line">workflowAppUri=<span class="variable">$&#123;nameNode&#125;</span>/<span class="variable">$&#123;appsDir&#125;</span>/shop/t_order</span><br><span class="line">start=2018-12-14T01:00+0800</span><br><span class="line">end=2018-12-30T01:00+0800 </span><br></pre></td></tr></table></figure>

<h3 id="getDate-sh"><a href="#getDate-sh" class="headerlink" title="getDate.sh"></a>getDate.sh</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">last_day=`date -d last-day +%Y-%m-%d`</span><br><span class="line">current_day=`date -d today +%Y-%m-%d`</span><br><span class="line"><span class="built_in">echo</span> start_day=<span class="variable">$last_day</span></span><br><span class="line"><span class="built_in">echo</span> end_day=<span class="variable">$current_day</span></span><br></pre></td></tr></table></figure>

<h3 id="coordinator-xml"><a href="#coordinator-xml" class="headerlink" title="coordinator.xml"></a>coordinator.xml</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;!--</span><br><span class="line">       Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">  distributed with this work <span class="keyword">for</span> additional information</span><br><span class="line">  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">  to you under the Apache License, Version 2.0 (the</span><br><span class="line">  <span class="string">&quot;License&quot;</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">  with the License.  You may obtain a copy of the License at</span><br><span class="line">  </span><br><span class="line">       http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">  </span><br><span class="line">  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">  distributed under the License is distributed on an <span class="string">&quot;AS IS&quot;</span> BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License <span class="keyword">for</span> the specific language governing permissions and</span><br><span class="line">  limitations under the License.</span><br><span class="line">--&gt;</span><br><span class="line">&lt;coordinator-app name=<span class="string">&quot;t_order_sqoop&quot;</span> frequency=<span class="string">&quot;40 11 * * *&quot;</span> start=<span class="string">&quot;<span class="variable">$&#123;start&#125;</span>&quot;</span> end=<span class="string">&quot;<span class="variable">$&#123;end&#125;</span>&quot;</span> timezone=<span class="string">&quot;GMT+0800&quot;</span></span><br><span class="line">                 xmlns=<span class="string">&quot;uri:oozie:coordinator:0.4&quot;</span>&gt;</span><br><span class="line">        &lt;action&gt;</span><br><span class="line">        &lt;workflow&gt;</span><br><span class="line">            &lt;app-path&gt;<span class="variable">$&#123;workflowAppUri&#125;</span>&lt;/app-path&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;jobTracker&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;<span class="variable">$&#123;jobTracker&#125;</span>&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;nameNode&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;<span class="variable">$&#123;nameNode&#125;</span>&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;queueName&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;<span class="variable">$&#123;queueName&#125;</span>&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/workflow&gt;</span><br><span class="line">    &lt;/action&gt;</span><br><span class="line">&lt;/coordinator-app&gt;</span><br></pre></td></tr></table></figure>

<h3 id="workflow-xml"><a href="#workflow-xml" class="headerlink" title="workflow.xml"></a>workflow.xml</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;&lt;!-- Licensed to the Apache Software </span><br><span class="line">        Foundation (ASF) under one or more contributor license agreements. See the </span><br><span class="line">        NOTICE file distributed with this work <span class="keyword">for</span> additional information regarding </span><br><span class="line">        copyright ownership. The ASF licenses this file to you under the Apache License, </span><br><span class="line">        Version 2.0 (the <span class="string">&quot;License&quot;</span>); you may not use this file except <span class="keyword">in</span> compliance </span><br><span class="line">        with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 </span><br><span class="line">        Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software distributed </span><br><span class="line">        under the License is distributed on an <span class="string">&quot;AS IS&quot;</span> BASIS, WITHOUT WARRANTIES </span><br><span class="line">        OR CONDITIONS OF ANY KIND, either express or implied. See the License <span class="keyword">for</span> </span><br><span class="line">        the specific language governing permissions and limitations under the License. --&gt;</span><br><span class="line">&lt;workflow-app xmlns=<span class="string">&quot;uri:oozie:workflow:0.5&quot;</span> name=<span class="string">&quot;dmp-base-jobs&quot;</span>&gt;</span><br><span class="line"></span><br><span class="line">        &lt;credentials&gt;</span><br><span class="line">                &lt;credential name=<span class="string">&quot;hs2-creds&quot;</span> <span class="built_in">type</span>=<span class="string">&quot;hive2&quot;</span>&gt;</span><br><span class="line">                        &lt;property&gt;</span><br><span class="line">                                &lt;name&gt;hive2.server.principal&lt;/name&gt;</span><br><span class="line">                                &lt;value&gt;<span class="variable">$&#123;jdbcPrincipal&#125;</span>&lt;/value&gt;</span><br><span class="line">                        &lt;/property&gt;</span><br><span class="line">                        &lt;property&gt;</span><br><span class="line">                                &lt;name&gt;hive2.jdbc.url&lt;/name&gt;</span><br><span class="line">                                &lt;value&gt;<span class="variable">$&#123;jdbcURL&#125;</span>&lt;/value&gt;</span><br><span class="line">                        &lt;/property&gt;</span><br><span class="line">                &lt;/credential&gt;</span><br><span class="line">        &lt;/credentials&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &lt;start to=<span class="string">&quot;shell-date&quot;</span> /&gt;</span><br><span class="line">        &lt;action name=<span class="string">&quot;shell-date&quot;</span>&gt;</span><br><span class="line">                &lt;shell xmlns=<span class="string">&quot;uri:oozie:shell-action:0.2&quot;</span>&gt;</span><br><span class="line">                        &lt;job-tracker&gt;<span class="variable">$&#123;jobTracker&#125;</span>&lt;/job-tracker&gt;</span><br><span class="line">                        &lt;name-node&gt;<span class="variable">$&#123;nameNode&#125;</span>&lt;/name-node&gt;</span><br><span class="line">                        &lt;configuration&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;<span class="variable">$&#123;queueName&#125;</span>&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;-Xmx1638m&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.yarn.app.mapreduce.am.resource.mb&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.yarn.app.mapreduce.am.command-opts&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;-Xmx1638m&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                        &lt;/configuration&gt;</span><br><span class="line">                        &lt;<span class="built_in">exec</span>&gt;getDate.sh&lt;/<span class="built_in">exec</span>&gt;</span><br><span class="line">                        &lt;file&gt;getDate.sh&lt;/file&gt;</span><br><span class="line">                        &lt;capture-output /&gt;</span><br><span class="line">                &lt;/shell&gt;</span><br><span class="line">                &lt;ok to=<span class="string">&quot;appForkStart&quot;</span> /&gt;</span><br><span class="line">                &lt;error to=<span class="string">&quot;fail&quot;</span> /&gt;</span><br><span class="line">        &lt;/action&gt;</span><br><span class="line">        &lt;fork name=<span class="string">&quot;appForkStart&quot;</span>&gt;</span><br><span class="line">                &lt;path start=<span class="string">&quot;t_dynamic_info_import&quot;</span> /&gt;</span><br><span class="line">        &lt;/fork&gt;</span><br><span class="line">         &lt;action name=<span class="string">&quot;t_dynamic_info_import&quot;</span>&gt;</span><br><span class="line">                &lt;sqoop xmlns=<span class="string">&quot;uri:oozie:sqoop-action:0.2&quot;</span>&gt;</span><br><span class="line">                        &lt;job-tracker&gt;<span class="variable">$&#123;jobTracker&#125;</span>&lt;/job-tracker&gt;</span><br><span class="line">                        &lt;name-node&gt;<span class="variable">$&#123;nameNode&#125;</span>&lt;/name-node&gt;</span><br><span class="line">                        &lt;configuration&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;<span class="variable">$&#123;queueName&#125;</span>&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;5120&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;-Xmx4096m&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.yarn.app.mapreduce.am.resource.mb&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;5120&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.yarn.app.mapreduce.am.command-opts&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;-Xmx4096m&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                        &lt;/configuration&gt;</span><br><span class="line">                        &lt;arg&gt;import&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--connect&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;jdbc:mysql://192.168.3.119/jt_ai_forum?tinyInt1isBit=<span class="literal">false</span>&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--username&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;root&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--password&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;Xingrui@DCDB123&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--driver&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;com.mysql.jdbc.Driver&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--query&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;select</span><br><span class="line">                                dynamic_id,company_id,card_id,dynamic_type,title,content,images,video_style,video_url,is_pub,is_top,pub_time,gmt_create,gmt_modified,old_db_id,programa_id</span><br><span class="line">                                from t_dynamic_info WHERE 1=1 and <span class="variable">$CONDITIONS</span></span><br><span class="line">                        &lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--hive-drop-import-delims&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--null-string&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;\\N&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--null-non-string&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;\\N&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--delete-target-dir&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--target-dir&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;/apps/hive/warehouse/ods.db/ods_t_dynamic_info/dt=<span class="variable">$&#123;(wf:actionData(&#x27;shell-date&#x27;)[&#x27;start_date&#x27;])&#125;</span></span><br><span class="line">                        &lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;--fields-terminated-by&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;\001&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;-m&lt;/arg&gt;</span><br><span class="line">                        &lt;arg&gt;1&lt;/arg&gt;</span><br><span class="line">                &lt;/sqoop&gt;</span><br><span class="line">                &lt;ok to=<span class="string">&quot;appForkEnd&quot;</span> /&gt;</span><br><span class="line">                &lt;error to=<span class="string">&quot;fail&quot;</span> /&gt;</span><br><span class="line">        &lt;/action&gt;</span><br><span class="line"></span><br><span class="line"> &lt;join name=<span class="string">&quot;appForkEnd&quot;</span> to=<span class="string">&quot;DWD_APP_USER_INCREASED_RECORD_ETL&quot;</span> /&gt;</span><br><span class="line"></span><br><span class="line"> &lt;action name=<span class="string">&quot;DWD_APP_USER_INCREASED_RECORD_ETL&quot;</span>&gt;</span><br><span class="line">                &lt;shell xmlns=<span class="string">&quot;uri:oozie:shell-action:0.2&quot;</span>&gt;</span><br><span class="line">                        &lt;job-tracker&gt;<span class="variable">$&#123;jobTracker&#125;</span>&lt;/job-tracker&gt;</span><br><span class="line">                        &lt;name-node&gt;<span class="variable">$&#123;nameNode&#125;</span>&lt;/name-node&gt;</span><br><span class="line">                        &lt;configuration&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;<span class="variable">$&#123;queueName&#125;</span>&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;5120&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;-Xmx4096m&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.yarn.app.mapreduce.am.resource.mb&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;5120&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                                &lt;property&gt;</span><br><span class="line">                                        &lt;name&gt;oozie.launcher.yarn.app.mapreduce.am.command-opts&lt;/name&gt;</span><br><span class="line">                                        &lt;value&gt;-Xmx4096m&lt;/value&gt;</span><br><span class="line">                                &lt;/property&gt;</span><br><span class="line">                        &lt;/configuration&gt;</span><br><span class="line">                        &lt;<span class="built_in">exec</span>&gt;DWD/DWD_APP_USER_INCREASED_RECORD/DWD_APP_USER_INCREASED_RECORD_ETL.sh</span><br><span class="line">                        &lt;/<span class="built_in">exec</span>&gt;</span><br><span class="line">                        &lt;argument&gt;<span class="variable">$&#123;(wf:actionData(&#x27;shell-date&#x27;)[&#x27;start_date&#x27;])&#125;</span>&lt;/argument&gt;</span><br><span class="line">                        &lt;argument&gt;<span class="variable">$&#123;(wf:actionData(&#x27;shell-date&#x27;)[&#x27;end_date&#x27;])&#125;</span>&lt;/argument&gt;</span><br><span class="line">                        &lt;file&gt;DWD/DWD_APP_USER_INCREASED_RECORD/DWD_APP_USER_INCREASED_RECORD_ETL.sh</span><br><span class="line">                        &lt;/file&gt;</span><br><span class="line">                        &lt;capture-output /&gt;</span><br><span class="line">                &lt;/shell&gt;</span><br><span class="line">                &lt;ok to=<span class="string">&quot;bdl_company_info_etl&quot;</span> /&gt;</span><br><span class="line">                &lt;error to=<span class="string">&quot;fail&quot;</span> /&gt;</span><br><span class="line">        &lt;/action&gt;</span><br><span class="line">        &lt;<span class="built_in">kill</span> name=<span class="string">&quot;fail&quot;</span>&gt;</span><br><span class="line">                &lt;message&gt;Shell action failed, error</span><br><span class="line">                        message[<span class="variable">$&#123;wf:errorMessage(wf:lastErrorNode())&#125;</span>]&lt;/message&gt;</span><br><span class="line">        &lt;/<span class="built_in">kill</span>&gt;</span><br><span class="line">        &lt;end name=<span class="string">&quot;end&quot;</span> /&gt;</span><br><span class="line">&lt;/workflow-app&gt;</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Oozie</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Kylin</title>
    <url>/2018/03/18/12/</url>
    <content><![CDATA[<h3 id="一、kylin解决了什么关键问题？"><a href="#一、kylin解决了什么关键问题？" class="headerlink" title="一、kylin解决了什么关键问题？"></a>一、kylin解决了什么关键问题？</h3><p>Apache Kylin的初衷就是解决千亿、万亿条记录的秒级查询问题，其中的关键就是打破查询时间随着数据量呈线性增长的这一规律。<br>大数据OLAP，我们可以注意到两个事实：<br>• 大数据查询要的一般是统计结果，是多条记录经过聚合函数计算后的统计值。原始的记录则不是必需的，或者被访问的频率和概率极低。<br>• 聚合是按维度进行的，而维度的聚合可能性是有限的，一般不随数据的膨胀而线性增长。<br>基于以上两点，我们得到一个新的思路——“预计算”。应尽量多地预先计算聚合结果，在查询时刻也尽量使用预计算的结果得出查询结果，从而避免直接扫描可能无限增长的原始记录。<br>举例来说，要用下面的SQL来查询10月1日那天销量最高的商品。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT item, SUM(sell_amount) FROM sell_details WHERE sell_date=<span class="string">&#x27;2016-10-01&#x27;</span> GROUP BY item ORDER BY SUM(sell_amount) DESC</span><br></pre></td></tr></table></figure>

<p>传统的方法需要扫描所有的记录，找到10月1日的销售记录，然后按商品聚合销售额，最后排序返回。</p>
<p>假如10月1日有1亿条交易，那么查询必需读取并累计至少1亿条记录，且查询速度会随将来销量的增加而逐步下降，如果日交易量提高至2亿条，那查询执行的时间可能会增加一倍</p>
<p>而预计算的方法则会事先按维度[sell_date，item]计算SUM(sell_amount)并将其存储下来，在查询时找到10月1日的销售商品就可以直接排序返回了。</p>
<p>读取的记录数最大不超过维度[sell_date，item]的组合数。显然这个数字将远远小于实际的销售记录，比如10月1日的1亿条交易包含了100万种商品，那么预计算后就只有100万条记录了，是原来的百分之一。并且这些记录是已经按商品聚合的结果，省去了运行时的聚合运算。从未来的发展来看，查询速度只会随日期和商品数目的增长而变化，与销售记录总数不再有直接联系。假如日交易量提高一倍到2亿，但只要商品总数不变，那么预计算的结果记录总数就不会变，查询的速度也不会变。</p>
<p>“预计算”就是Kylin在“大规模并行处理”和“列式存储”之外，提供给大数据分析的第三个关键技术。</p>
<h3 id="二、kylin工作原理"><a href="#二、kylin工作原理" class="headerlink" title="二、kylin工作原理"></a>二、kylin工作原理</h3><p>2.1 维度和度量简介<br>在说明MOLAP Cube之前，需要先介绍一下维度（dimension）和度量（measure）这两个概念。</p>
<p>简单来讲，维度就是观察数据的角度。比如电商的销售数据，可以从时间的维度来观察（如图1-2的左图所示），也可以进一步细化从时间和地区的维度来观察（如图1-2的右图所示）。</p>
<p>维度一般是一组离散的值，比如时间维度上的每一个独立的日期，或者商品维度上的每一件独立的商品。因此，统计时可以把维度值相同的记录聚合起来，应用聚合函数做累加、平均、去重复计数等聚合计算。</p>
<p><img src="/images/3.png" alt="alt"></p>
<p>度量就是被聚合的统计值，也是聚合运算的结果，它一般是连续值，如图1-2中的销售额，抑或是销售商品的总件数。通过比较和测算度量，分析师可以对数据进行评估，比如今年的销售额相比去年有多大的增长、增长的速度是否达到预期、不同商品类别的增长比例是否合理等。</p>
<p>2.2 Cube和Cuboid<br>了解了维度和度量，就可以对数据表或者数据模型上的所有字段进行分类了，它们要么是维度，要么是度量（可以被聚合）。于是就有了根据维度、度量做预计算的Cube理论。</p>
<p>给定一个数据模型，我们可以对其上所有维度进行组合。对于N个维度来说，所有组合的可能性有2N种。对每一种维度的组合，将度量做聚合运算，运算的结果保存为一个物化视图，称为Cuboid。将所有维度组合的Cuboid作为一个整体，被称为Cube。所以简单来说，一个Cube就是许多按维度聚合的物化视图的集合。</p>
<p>举一个具体的例子。假定有一个电商的销售数据集，其中维度有时间（Time）、商品（Item）、地点（Location）和供应商（Supplier），度量有销售额（GMV）。那么，所有维度的组合就有24=16种（如图），比如一维度（1D）的组合有[Time][Item][Location][Supplier]四种；二维度（2D）的组合有[Time，Item][Time，Location][Time、Supplier][Item，Location][Item，Supplier][Location，Supplier]六种；三维度（3D）的组合也有四种；最后，零维度（0D）和四维度（4D）的组合各有一种，共计16种组合。</p>
<p>计算Cuboid，就是按维度来聚合销售额（GMV）。如果用SQL来表达计算Cuboid[Time，Location]，那就是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">select Time, Location, Sum(GMV) as GMV from Sales group by Time, Location </span><br></pre></td></tr></table></figure>

<p><img src="/images/4.png" alt="alt"></p>
<p>将计算的结果保存为物化视图，所有Cuboid物化视图的总称就是Cube了。</p>
<p>2.3 工作原理<br>Apache Kylin的工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询。过程如下：</p>
<p>（1）指定数据模型，定义维度和度量。</p>
<p>（2）预计算Cube，计算所有Cuboid并将其保存为物化视图。</p>
<p>（3）执行查询时，读取Cuboid，进行加工运算产生查询结果。</p>
<p>由于Kylin的查询过程不会扫描原始记录，而是通过预计算预先完成表的关联、聚合等复杂运算，并利用预计算的结果来执行查询，因此其速度相比非预计算的查询技术一般要快一个到两个数量级。并且在超大数据集上其优势更明显。当数据集达到千亿乃至万亿级别时，Kylin的速度甚至可以超越其他非预计算技术1000倍以上。</p>
<h3 id="三、kylin技术架构"><a href="#三、kylin技术架构" class="headerlink" title="三、kylin技术架构"></a>三、kylin技术架构</h3><p>Apache Kylin系统可以分为在线查询和离线构建两部分，其技术架构如图所示。在线查询主要由上半区组成，离线构建在下半区。</p>
<p>先看离线构建的部分。从图1-4中可以看到，数据源在左侧，目前主要是Hadoop、Hive、Kafka和RDBMS，其中保存着待分析的用户数据。</p>
<p>根据元数据定义，下方构建引擎从数据源中抽取数据，并构建Cube。</p>
<p>数据以关系表的形式输入，且必须符合星形模型（Star Schema）或雪花模型（Snowflake Schema）。</p>
<p>用户可以选择使用MapReduce或Spark进行构建。</p>
<p>构建后的Cube保存在右侧的存储引擎中，目前HBase是默认的存储引擎。</p>
<p><img src="/images/5.png" alt="alt"><br>完成离线构建后，用户可以从上方查询系统发送SQL来进行查询分析。Kylin提供了多样的REST API、JDBC/ODBC接口。无论从哪个接口进入，最终SQL都会来到REST服务层，再转交给查询引擎进行处理。这里需要注意的是，SQL语句是基于数据源的关系模型书写的，而不是Cube。Kylin在设计时刻意对查询用户屏蔽了Cube的概念，分析师只需要理解简单的关系模型就可以使用Kylin，没有额外的学习门槛，传统的SQL应用也更容易迁移。查询引擎解析SQL，生成基于关系表的逻辑执行计划，然后将其转译为基于Cube的物理执行计划，最后查询预计算生成的Cube产生结果。整个过程不访问原始数据源。</p>
<p>　　注意　对于查询引擎下方的路由选择，在最初设计时考虑过将Kylin不能执行的查询引导到Hive中继续执行。但在实践后发现Hive与Kylin的执行速度差异过大，导致用户无法对查询的速度有一致的期望，大多语句很可能查询几秒就返回了，而有些要等几分钟到几十分钟，用户体验非常糟糕。最后这个路由功能在发行版中默认被关闭。</p>
<p>Apache Kylin v1.5版本引入了“可扩展架构”的概念。图1-4所示为Rest Server、Cube Build Engine和数据源表示的抽象层。可扩展是指Kylin可以对其三个主要依赖模块——数据源、构建引擎和存储引擎，做任意的扩展和替换。在设计之初，作为Hadoop家族的一员，这三者分别是Hive、MapReduce和HBase。但随着Apache Kylin的推广和使用的深入，用户发现它们存在不足之处。</p>
<p>比如，实时分析可能会希望从Kafka导入数据而不是从Hive；而Spark的迅速崛起，又使我们不得不考虑将MapReduce替换为Spark以提高Cube的构建速度；至于HBase，它的读性能可能不如Cassandra等。可见，是否可以将某种技术替换为另一种技术已成为一个常见的问题。于是，我们对Apache Kylin v1.5版本的系统架构进行了重构，将数据源、构建引擎、存储引擎三大主要依赖模块抽象为接口，而Hive、MapReduce、HBase只是默认实现。其他实现还有：数据源还可以是Kafka、Hadoop或RDBMS；构建引擎还可以是Spark、Flink。资深用户可以根据自己的需要做二次开发，将其中的一个或者多个技术替换为更适合自身需要的技术。</p>
<p>这也为Kylin技术的与时俱进奠定了基础。如果将来有更先进的分布式计算技术可以取代MapReduce，或者有更高效的存储系统全面超越了HBase，Kylin可以用较小的代价将一个子系统替换掉，从而保证Kylin紧跟技术发展的最新潮流，保持最高的技术水平。</p>
<p>可扩展架构也带来了额外的灵活性，比如，它可以允许多个引擎并存。例如，Kylin可以同时对接Hive、Kafka和其他第三方数据源；抑或用户可以为不同的Cube指定不同的构建引擎或存储引擎，以期达到极致的性能和功能定制。</p>
<h3 id="四、kylin特点"><a href="#四、kylin特点" class="headerlink" title="四、kylin特点"></a>四、kylin特点</h3><p>Apache Kylin的主要特点包括：<br>支持SQL接口<br>支持超大数据集<br>秒级响应<br>可伸缩性<br>高吞吐率<br>BI及可视化工具集成</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>kylin olap</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 累加器与广播变量</title>
    <url>/2018/03/18/15/</url>
    <content><![CDATA[<h3 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h3><p>在 Spark 中，提供了两种类型的共享变量：累加器 (accumulator) 与广播变量 (broadcast variable)：</p>
<p>累加器：用来对信息进行聚合，主要用于累计计数等场景；<br>广播变量：主要用于在节点间高效分发大对象。</p>
<h3 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h3><p>这里先看一个具体的场景，对于正常的累计求和，如果在集群模式中使用下面的代码进行计算，会发现执行结果并非预期</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">var counter = 0</span><br><span class="line">val data = Array(1, 2, 3, 4, 5)</span><br><span class="line">sc.parallelize(data).foreach(x =&gt; counter += x)</span><br><span class="line"> println(counter)</span><br></pre></td></tr></table></figure>
<p>counter 最后的结果是 0，导致这个问题的主要原因是闭包。<br><img src="/images/11.png" alt="alt"></p>
<p> 2.1 理解闭包</p>
<ol>
<li>Scala 中闭包的概念</li>
</ol>
<p>这里先介绍一下 Scala 中关于闭包的概念：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">var more = 10</span><br><span class="line">val addMore = (x: Int) =&gt; x + more</span><br></pre></td></tr></table></figure>

<p>如上函数 addMore 中有两个变量 x 和 more:</p>
<p>x : 是一个绑定变量 (bound variable)，因为其是该函数的入参，在函数的上下文中有明确的定义；<br>more : 是一个自由变量 (free variable)，因为函数字面量本生并没有给 more 赋予任何含义。<br>按照定义：在创建函数时，如果需要捕获自由变量，那么包含指向被捕获变量的引用的函数就被称为闭包函数。</p>
<ol start="2">
<li>Spark 中的闭包</li>
</ol>
<p>在实际计算时，Spark 会将对 RDD 操作分解为 Task，Task 运行在 Worker Node 上。在执行之前，Spark 会对任务进行闭包，如果闭包内涉及到自由变量，则程序会进行拷贝，并将副本变量放在闭包中，之后闭包被序列化并发送给每个执行者。因此，当在 foreach 函数中引用 counter 时，它将不再是 Driver 节点上的 counter，而是闭包中的副本 counter，默认情况下，副本 counter 更新后的值不会回传到 Driver，所以 counter 的最终值仍然为零。</p>
<p>需要注意的是：在 Local 模式下，有可能执行 foreach 的 Worker Node 与 Diver 处在相同的 JVM，并引用相同的原始 counter，这时候更新可能是正确的，但是在集群模式下一定不正确。所以在遇到此类问题时应优先使用累加器。</p>
<p>累加器的原理实际上很简单：就是将每个副本变量的最终值传回 Driver，由 Driver 聚合后得到最终值，并更新原始变量。<br><img src="/images/12.png" alt="alt"></p>
<p>2.2 使用累加器<br>SparkContext 中定义了所有创建累加器的方法，需要注意的是：被中横线划掉的累加器方法在 Spark 2.0.0 之后被标识为废弃。</p>
<p><img src="/images/13.png" alt="alt"></p>
<p>使用示例和执行结果分别如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">val data = Array(1, 2, 3, 4, 5)</span><br><span class="line">// 定义累加器</span><br><span class="line">val accum = sc.longAccumulator(<span class="string">&quot;My Accumulator&quot;</span>)</span><br><span class="line">sc.parallelize(data).foreach(x =&gt; accum.add(x))</span><br><span class="line">// 获取累加器的值</span><br><span class="line">accum.value</span><br></pre></td></tr></table></figure>
<p><img src="/images/14.png" alt="alt"></p>
<h3 id="三、广播变量"><a href="#三、广播变量" class="headerlink" title="三、广播变量"></a>三、广播变量</h3><p>在上面介绍中闭包的过程中我们说道每个 Task 任务的闭包都会持有自由变量的副本，如果变量很大且 Task 任务很多的情况下，这必然会对网络 IO 造成压力，为了解决这个情况，Spark 提供了广播变量。</p>
<p>广播变量的做法很简单：就是不把副本变量分发到每个 Task 中，而是将其分发到每个 Executor，Executor 中的所有 Task 共享一个副本变量。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">// 把一个数组定义为一个广播变量</span><br><span class="line">val broadcastVar = sc.broadcast(Array(1, 2, 3, 4, 5))</span><br><span class="line">// 之后用到该数组时应优先使用广播变量，而不是原值</span><br><span class="line">sc.parallelize(broadcastVar.value).map(_ * 10).collect()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>python用map-reduce(IP地址库匹配省份和城市)</title>
    <url>/2018/03/16/11/</url>
    <content><![CDATA[<h3 id="IP地址库文件为city-txt大致内容如下"><a href="#IP地址库文件为city-txt大致内容如下" class="headerlink" title="IP地址库文件为city.txt大致内容如下"></a>IP地址库文件为city.txt大致内容如下</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">708104192|708112383|辽宁|葫芦岛</span><br><span class="line">708112384|708116479|辽宁|朝阳</span><br><span class="line">708116480|708124671|辽宁|营口</span><br><span class="line">708124672|708132863|辽宁|阜新</span><br><span class="line">708132864|708141055|辽宁|阜新</span><br><span class="line">708141056|708145151|辽宁|锦州</span><br><span class="line">708145152|708149247|辽宁|葫芦岛</span><br><span class="line">708149248|708153343|辽宁|葫芦岛</span><br><span class="line">708153344|708157439|辽宁|盘锦</span><br><span class="line">708157440|708161535|辽宁|盘锦</span><br><span class="line">708161536|708165631|辽宁|朝阳</span><br><span class="line">708165632|708182015|辽宁|沈阳</span><br></pre></td></tr></table></figure>

<h3 id="经过数据清洗后得出真实的手机号码并插入当前hive表中"><a href="#经过数据清洗后得出真实的手机号码并插入当前hive表中" class="headerlink" title="经过数据清洗后得出真实的手机号码并插入当前hive表中"></a>经过数据清洗后得出真实的手机号码并插入当前hive表中</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">INSERT overwrite table true_flase_phone_month select loginname,ipcount,user_ip,loginname regexp <span class="string">&#x27;^((13[0-9])|(14[5|7])|(15([0-3]|[5-9]))|(18[0,5-9]))\\d&#123;8&#125;$&#x27;</span>,user_ip regexp <span class="string">&#x27;^(25[0-5]|2[0-4][0-9]|[0-1]&#123;1&#125;[0-9]&#123;2&#125;|[1-9]&#123;1&#125;[0-9]&#123;1&#125;|[1-9])\.(25[0-5]|2[0-4][0-9]|[0-1]&#123;1&#125;[0-9]&#123;2&#125;|[1-9]&#123;1&#125;[0-9]&#123;1&#125;|[1-9]|0)\.(25[0-5]|2[0-4][0-9]|[0-1]&#123;1&#125;[0-9]&#123;2&#125;|[1-9]&#123;1&#125;[0-9]&#123;1&#125;|[1-9]|0)\.(25[0-5]|2[0-4][0-9]|[0-1]&#123;1&#125;[0-9]&#123;2&#125;|[1-9]&#123;1&#125;[0-9]&#123;1&#125;|[0-9])$&#x27;</span> from log_login_info_desc_month;</span><br><span class="line">CREATE TABLE true_phone_month</span><br><span class="line">( </span><br><span class="line">     loginname STRING, </span><br><span class="line">     ipcount   BIGINT, </span><br><span class="line">     user_ip   STRING</span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line">INSERT overwrite table true_phone_month             </span><br><span class="line">select  loginname,ipcount,user_ip  from true_flase_phone_month <span class="built_in">where</span>  login_true_false=<span class="string">&#x27;true&#x27;</span> and user_ip_true_false=<span class="string">&#x27;true&#x27;</span>; </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python中map和reduce代码如下</span><br><span class="line">---map</span><br><span class="line"><span class="meta">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">import re</span><br><span class="line">import socket</span><br><span class="line">import struct</span><br><span class="line">import os  </span><br><span class="line">import gc</span><br><span class="line"></span><br><span class="line">i=1</span><br><span class="line">filepath = os.environ[<span class="string">&#x27;mapreduce_map_input_file&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">filename = os.path.split(filepath)[-1]</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="keyword">if</span> line.strip()==<span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="built_in">continue</span>    </span><br><span class="line">    <span class="keyword">if</span> filename == <span class="string">&#x27;city.txt&#x27;</span>:</span><br><span class="line">        fields = line[:-1].split(<span class="string">&quot;|&quot;</span>)    </span><br><span class="line">        ip_1 = fields[0]  </span><br><span class="line">        ip_2 = fields[1]  </span><br><span class="line">        prov = fields[2]  </span><br><span class="line">        area = fields[3]       </span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;%s|%s|%s|%s|%s&quot;</span> % (ip_1,1,i,prov,area))</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;%s|%s|%s|%s|%s&quot;</span> % (ip_2,1,i,prov,area))</span><br><span class="line">        i+=1</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fields = line[:-1].split(<span class="string">&quot;,&quot;</span>)    </span><br><span class="line">        try:            </span><br><span class="line">            logon_no=fields[0]</span><br><span class="line">            logon_cnt=fields[1]</span><br><span class="line">            logon_ip=fields[2]</span><br><span class="line">            <span class="keyword">if</span> logon_cnt and logon_no and logon_ip:</span><br><span class="line">                ip_s=socket.ntohl(struct.unpack(<span class="string">&quot;I&quot;</span>,socket.inet_aton(str(logon_ip)))[0])                  </span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&quot;%s|%s|%s|%s&quot;</span> % (ip_s,0,logon_no,logon_cnt))</span><br><span class="line">        except:</span><br><span class="line">            <span class="built_in">continue</span></span><br><span class="line">---reduce</span><br><span class="line"><span class="meta">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">prov=0</span><br><span class="line">area=0</span><br><span class="line">ip_no1=0</span><br><span class="line">ip_no2=0</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="keyword">if</span> line.strip()==<span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="built_in">continue</span></span><br><span class="line">    a=line[:-1].split(<span class="string">&quot;|&quot;</span>)  </span><br><span class="line">    try:</span><br><span class="line">        <span class="keyword">if</span> int(a[1])==1:            </span><br><span class="line">            ip_no1=a[2]</span><br><span class="line">            <span class="keyword">if</span> ip_no1==ip_no2:</span><br><span class="line">                prov=0</span><br><span class="line">                area=0</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prov=a[3].strip()</span><br><span class="line">                area=a[4].strip()</span><br><span class="line">            ip_no2=ip_no1</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> int(a[1])==0:</span><br><span class="line">            logon_time=a[3].strip()</span><br><span class="line">            logon_no=a[2].strip()</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;%s|%s|%s|%s|%s&quot;</span> % (logon_time,logon_no,prov,area,a[0].strip()))</span><br><span class="line">    except:</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/hdp/2.5.0.0-1245/hadoop-mapreduce/hadoop-streaming.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=<span class="string">&quot;-n&quot;</span> -D mapred.map.tasks=10 -D mapred.reduce.tasks=1 -input /data139/ods/ip_address/city.txt -input /apps/hive/warehouse/dw_ods.db/true_phone_month/* -output /data139/bill_tmp/ip_change/<span class="variable">$&#123;month_start&#125;</span> -file /home/hdfs/ip_change_script/my_map.py -file /home/hdfs/ip_change_script/my_reduce.py -mapper <span class="string">&quot;python my_map.py&quot;</span> -reducer <span class="string">&quot;python my_reduce.py&quot;</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS 常用 shell 命令</title>
    <url>/2018/03/14/14/</url>
    <content><![CDATA[<h3 id="HDFS-常用-shell-命令"><a href="#HDFS-常用-shell-命令" class="headerlink" title="HDFS 常用 shell 命令"></a>HDFS 常用 shell 命令</h3><ol>
<li><p>显示当前目录结构</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  &lt;path&gt;</span><br><span class="line"><span class="comment"># 递归显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  -R  &lt;path&gt;</span><br><span class="line"><span class="comment"># 显示根目录下内容</span></span><br><span class="line">hadoop fs -ls  /</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">hadoop fs -mkdir  &lt;path&gt; </span><br><span class="line"><span class="comment"># 递归创建目录</span></span><br><span class="line">hadoop fs -mkdir -p  &lt;path&gt;  </span><br></pre></td></tr></table></figure>
</li>
<li><p>删除操作</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">hadoop fs -rm  &lt;path&gt;</span><br><span class="line"><span class="comment"># 递归删除目录和文件</span></span><br><span class="line">hadoop fs -rm -R  &lt;path&gt; </span><br></pre></td></tr></table></figure>
</li>
<li><p>从本地加载文件到 HDFS</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 二选一执行即可</span></span><br><span class="line">hadoop fs -put  [localsrc] [dst] </span><br><span class="line">hadoop fs - copyFromLocal [localsrc] [dst] </span><br></pre></td></tr></table></figure>
</li>
<li><p>从 HDFS 导出文件到本地</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 二选一执行即可</span></span><br><span class="line">hadoop fs -get  [dst] [localsrc] </span><br><span class="line">hadoop fs -copyToLocal [dst] [localsrc] </span><br></pre></td></tr></table></figure>
</li>
<li><p>查看文件内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 二选一执行即可</span></span><br><span class="line">hadoop fs -text  &lt;path&gt; </span><br><span class="line">hadoop fs -cat  &lt;path&gt;  </span><br></pre></td></tr></table></figure>
</li>
<li><p>显示文件的最后一千字节</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -tail  &lt;path&gt; </span><br><span class="line"><span class="comment"># 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节</span></span><br><span class="line">hadoop fs -tail -f  &lt;path&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>拷贝文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -cp [src] [dst]</span><br></pre></td></tr></table></figure>
</li>
<li><p>移动文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -mv [src] [dst] </span><br></pre></td></tr></table></figure>
</li>
<li><p>统计当前目录下各文件大小<br>默认单位字节</p>
</li>
</ol>
<p>-s : 显示所有文件大小总和，<br>-h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -du  &lt;path&gt;  </span><br></pre></td></tr></table></figure>

<ol start="11">
<li>合并下载多个文件</li>
</ol>
<p>-nl 在每个文件的末尾添加换行符（LF）<br>-skip-empty-file 跳过空文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -getmerge</span><br><span class="line"><span class="comment"># 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml</span></span><br><span class="line">hadoop fs -getmerge -nl  /<span class="built_in">test</span>/hbase-policy.xml /<span class="built_in">test</span>/hbase-site.xml /usr/test.xml</span><br></pre></td></tr></table></figure>

<ol start="12">
<li>统计文件系统的可用空间信息<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -df -h /</span><br></pre></td></tr></table></figure>


</li>
</ol>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>shell</tag>
      </tags>
  </entry>
</search>
